# Руководство по оптимизации Ollama

Этот документ описывает исправления, применённые для решения проблем с производительностью Ollama, выявленных в логах.

## Проблемы и решения

### 1. Раннер выгружается между вызовами (500 на `/v1/embeddings`)

**Проблема:** Ollama выгружает модель между запросами, что приводит к 500 ошибкам и задержкам.

**Решение:**
- Настроены переменные окружения `OLLAMA_KEEP_ALIVE=300` и `OLLAMA_USE_MMAP=1`
- Добавлены ретраи с экспоненциальной задержкой в `EmbeddingsClient`
- Добавлен health-check для автоматического восстановления соединения

### 2. Разные контексты для generate/embeddings

**Проблема:** Использование одной модели для разных задач с разными `num_ctx` вызывает перезагрузку модели.

**Решение:**
- Разделены модели: LLM (`gemma3n:e4b-it-q8_0`) и Embeddings (`mxbai-embed-large:latest`)
- Зафиксирован `num_ctx=32000` для максимального контекста
- Обновлён `VectorIndex` для использования специализированной embed-модели

### 3. Холодный старт из-за тяжёлой квантизации

**Проблема:** Q8_0 квантизация требует много VRAM и времени загрузки.

**Решение:**
- Сохранена `q8_0` квантизация для максимальной точности
- Увеличен контекст до 32k токенов для полного использования модели
- Включение mmap для экономии памяти
- Добавлен прогрев модели на старте для ускорения последующих запросов

### 4. Векторный индекс на LLM-теге

**Проблема:** Использование LLM модели для эмбеддингов вызывает ненужные загрузки.

**Решение:**
- Переключение на специализированную embed-модель `mxbai-embed-large:latest`
- Обновление размерности эмбеддингов с 384 до 1024

### 5. Стартовый статус "llm_client: ❌ неактивен"

**Проблема:** Клиент помечается как неактивный до прогрева.

**Решение:**
- Добавлен асинхронный прогрев LLM клиента на старте
- Добавлены методы `warmup()` и `health_check()` в `OllamaClient`

### 6. Ошибки Telethon засоряют цикл агента

**Проблема:** Ретраи Telethon влияют на основной цикл агента.

**Решение:**
- Изолированы ретраи подключения в `TelethonIndexer`
- Добавлены экспоненциальные задержки и ограничение попыток
- Добавлены методы проверки здоровья соединения

## Применение исправлений

### 1. Настройка переменных окружения

```bash
# Запустите скрипт настройки
./scripts/setup_ollama_env.sh

# Или вручную добавьте в ~/.zshrc или ~/.bashrc:
export OLLAMA_KEEP_ALIVE=300
export OLLAMA_USE_MMAP=1
```

### 2. Установка моделей

```bash
# Установите специализированную модель для эмбеддингов
ollama pull mxbai-embed-large:latest

# Установите основную LLM модель
ollama pull gemma3n:e4b-it-q8_0
```

### 3. Перезапуск приложения

```bash
# Активируйте виртуальное окружение
source venv/bin/activate

# Запустите приложение
python main.py
```

## Ожидаемые улучшения

- **Скорость:** Устранение 500 ошибок и задержек загрузки модели
- **Стабильность:** Изолированные ретраи и health-check
- **Память:** Оптимизированное использование VRAM через mmap
- **Надёжность:** Автоматическое восстановление соединений

## Мониторинг

Проверьте логи на наличие:
- `LLM warmup completed` - успешный прогрев
- `Embeddings attempt X/3` - ретраи эмбеддингов
- `Telethon: Connection established successfully` - успешное подключение
- Отсутствие `500` ошибок в эмбеддингах

## Откат изменений

Если возникнут проблемы, можно откатить изменения:

1. Вернуть `config/settings.yaml` к исходному состоянию
2. Удалить переменные окружения Ollama
3. Перезапустить Ollama и приложение
