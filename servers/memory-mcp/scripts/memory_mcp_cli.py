#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Telegram –¥–∞–º–ø–∞–º–∏
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é, —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –∏ –ø–æ–∏—Å–∫
"""

import argparse
import asyncio
import hashlib
import json
import logging
import os
import signal
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ PYTHONPATH –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥—É–ª–∏
try:
    from memory_mcp.core.ollama_client import OllamaEmbeddingClient

    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("‚ö†Ô∏è OllamaClient –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

try:
    from memory_mcp.analysis.instruction_manager import InstructionManager

    INSTRUCTION_MANAGER_AVAILABLE = True
except ImportError:
    INSTRUCTION_MANAGER_AVAILABLE = False
    print("‚ö†Ô∏è InstructionManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º MCP —Å–µ—Ä–≤–µ—Ä –∏–∑ scripts
try:
    from mcp_server import TelegramDumpMCP

    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False
    print("‚ö†Ô∏è TelegramDumpMCP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MessageExtractor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é."""

    def __init__(self, input_dir: str = "input", chats_dir: str = "chats"):
        self.input_dir = Path(input_dir)
        self.chats_dir = Path(chats_dir)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—É—â–µ–≥–æ –≥–æ–¥–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        current_year = datetime.now().year
        self.cutoff_date = datetime(current_year, 1, 1, tzinfo=timezone.utc)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stats = {
            "total_chats": 0,
            "processed_chats": 0,
            "skipped_chats": 0,
            "total_messages_input": 0,
            "total_messages_output": 0,
            "messages_copied": 0,
            "messages_filtered_by_date": 0,
            "duplicates_skipped": 0,
            "errors": 0,
            "files_processed": 0,
            "files_skipped": 0,
        }

        # –ö—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        self.existing_messages_cache = {}

    def parse_date(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö."""
        if not date_str:
            return None

        try:
            # –ó–∞–º–µ–Ω—è–µ–º Z –Ω–∞ +00:00 –¥–ª—è UTC
            if date_str.endswith("Z"):
                date_str = date_str.replace("Z", "+00:00")
            return datetime.fromisoformat(date_str)
        except ValueError:
            # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã
            try:
                return datetime.fromisoformat(date_str.replace("Z", ""))
            except ValueError:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str}")
                return None

    def get_message_hash(self, message: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ö—ç—à —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ID, –¥–∞—Ç—É –∏ –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
        text_preview = str(message.get("text", ""))[:100]
        hash_input = (
            f"{message.get('id', '')}_{message.get('date_utc', '')}_{text_preview}"
        )
        return hashlib.md5(hash_input.encode("utf-8")).hexdigest()

    def load_existing_messages(self, chat_dir: Path) -> Tuple[Set[str], Set[str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –∏ —Ö—ç—à–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —Ä–∞–±–æ—á–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        existing_ids = set()
        existing_hashes = set()

        if chat_dir.name in self.existing_messages_cache:
            return self.existing_messages_cache[chat_dir.name]

        # –ò—â–µ–º JSON —Ñ–∞–π–ª—ã –≤ —á–∞—Ç–µ
        json_files = []
        for pattern in ["unknown.json", "result.json", "messages.json"]:
            json_file = chat_dir / pattern
            if json_file.exists():
                json_files.append(json_file)

        for json_file in json_files:
            try:
                with open(json_file, encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if not line:
                            continue

                        try:
                            message = json.loads(line)
                            if "id" in message:
                                existing_ids.add(str(message["id"]))
                                existing_hashes.add(self.get_message_hash(message))
                        except json.JSONDecodeError as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ JSON –≤ {json_file}:{line_num}: {e}")
                            continue
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {json_file}: {e}")

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.existing_messages_cache[chat_dir.name] = (existing_ids, existing_hashes)
        return existing_ids, existing_hashes

    def filter_messages(
        self,
        messages: List[Dict],
        existing_ids: Set[str],
        existing_hashes: Set[str],
        filter_by_date: bool = True,
    ) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –¥–∞—Ç–µ –∏ –¥—É–±–ª–∏–∫–∞—Ç–∞–º."""
        filtered_messages = []

        for message in messages:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            if "id" not in message:
                logger.warning("–°–æ–æ–±—â–µ–Ω–∏–µ –±–µ–∑ ID –ø—Ä–æ–ø—É—â–µ–Ω–æ")
                continue

            message_id = str(message["id"])
            message_hash = self.get_message_hash(message)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
            if message_id in existing_ids:
                self.stats["duplicates_skipped"] += 1
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö—ç—à—É (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞)
            if message_hash in existing_hashes:
                self.stats["duplicates_skipped"] += 1
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if filter_by_date and "date_utc" in message:
                msg_date = self.parse_date(message["date_utc"])
                if msg_date and msg_date < self.cutoff_date:
                    self.stats["messages_filtered_by_date"] += 1
                    continue

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            filtered_messages.append(message)
            existing_ids.add(message_id)
            existing_hashes.add(message_hash)

        return filtered_messages

    def extract_chat_messages(
        self,
        input_chat_dir: Path,
        chats_chat_dir: Path,
        dry_run: bool = False,
        filter_by_date: bool = True,
    ) -> Dict[str, int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –æ–¥–Ω–æ–≥–æ —á–∞—Ç–∞."""
        chat_stats = {
            "messages_input": 0,
            "messages_output": 0,
            "messages_copied": 0,
            "errors": 0,
            "files_processed": 0,
            "files_skipped": 0,
        }

        # –ò—â–µ–º JSON —Ñ–∞–π–ª—ã –≤ input —á–∞—Ç–µ
        input_json_files = []
        for pattern in ["unknown.json", "result.json", "messages.json", "*.json"]:
            if pattern == "*.json":
                input_json_files.extend(input_chat_dir.glob(pattern))
            else:
                json_file = input_chat_dir / pattern
                if json_file.exists():
                    input_json_files.append(json_file)

        if not input_json_files:
            logger.warning(f"–ù–µ—Ç JSON —Ñ–∞–π–ª–æ–≤ –≤ —á–∞—Ç–µ: {input_chat_dir}")
            chat_stats["files_skipped"] = 1
            return chat_stats

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        existing_ids, existing_hashes = self.load_existing_messages(chats_chat_dir)

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ chats –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not dry_run:
            chats_chat_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Ç: {input_chat_dir.name}")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(existing_ids)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(input_json_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π JSON —Ñ–∞–π–ª
        for input_json_file in input_json_files:
            try:
                chat_stats["files_processed"] += 1

                # –ß–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ input
                messages = []
                with open(input_json_file, encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if not line:
                            continue

                        try:
                            message = json.loads(line)
                            messages.append(message)
                            chat_stats["messages_input"] += 1
                        except json.JSONDecodeError as e:
                            logger.warning(
                                f"–û—à–∏–±–∫–∞ JSON –≤ {input_json_file}:{line_num}: {e}"
                            )
                            chat_stats["errors"] += 1

                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
                filtered_messages = self.filter_messages(
                    messages, existing_ids, existing_hashes, filter_by_date
                )

                if filtered_messages:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                    output_file = chats_chat_dir / input_json_file.name

                    if not dry_run:
                        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                        with open(output_file, "a", encoding="utf-8") as f:
                            for message in filtered_messages:
                                json.dump(message, f, ensure_ascii=False)
                                f.write("\n")

                    chat_stats["messages_copied"] += len(filtered_messages)
                    logger.info(
                        f"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {len(filtered_messages)} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ {output_file}"
                    )
                else:
                    logger.info(f"–ù–µ—Ç –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–∞–π–ª–µ {input_json_file.name}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {input_json_file}: {e}")
                chat_stats["errors"] += 1

        chat_stats["messages_output"] = chat_stats["messages_copied"]
        return chat_stats

    def extract_all_messages(
        self,
        dry_run: bool = False,
        filter_by_date: bool = True,
        chat_filter: Optional[str] = None,
    ) -> Dict[str, int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö —á–∞—Ç–æ–≤."""
        if not self.input_dir.exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.input_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return self.stats

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é chats –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not dry_run:
            self.chats_dir.mkdir(parents=True, exist_ok=True)

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∞—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if chat_filter:
            input_chat_dirs = (
                [self.input_dir / chat_filter]
                if (self.input_dir / chat_filter).exists()
                else []
            )
        else:
            input_chat_dirs = [d for d in self.input_dir.iterdir() if d.is_dir()]

        self.stats["total_chats"] = len(input_chat_dirs)

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(input_chat_dirs)} —á–∞—Ç–æ–≤ –≤ input –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        if filter_by_date:
            logger.info(
                f"–§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ: —Å–æ–æ–±—â–µ–Ω–∏—è —Å {self.cutoff_date.strftime('%Y-%m-%d')} –∏ –Ω–æ–≤–µ–µ"
            )
        else:
            logger.info("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –æ—Ç–∫–ª—é—á–µ–Ω–∞")

        if dry_run:
            logger.info("–†–ï–ñ–ò–ú –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø - –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

        for input_chat_dir in input_chat_dirs:
            try:
                chats_chat_dir = self.chats_dir / input_chat_dir.name
                chat_stats = self.extract_chat_messages(
                    input_chat_dir, chats_chat_dir, dry_run, filter_by_date
                )

                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.stats["processed_chats"] += 1
                self.stats["total_messages_input"] += chat_stats["messages_input"]
                self.stats["total_messages_output"] += chat_stats["messages_output"]
                self.stats["messages_copied"] += chat_stats["messages_copied"]
                self.stats["errors"] += chat_stats["errors"]
                self.stats["files_processed"] += chat_stats["files_processed"]
                self.stats["files_skipped"] += chat_stats["files_skipped"]

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Ç–∞ {input_chat_dir}: {e}")
                self.stats["errors"] += 1
                self.stats["skipped_chats"] += 1

        return self.stats

    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        print("\n" + "=" * 80)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –ù–û–í–´–• –°–û–û–ë–©–ï–ù–ò–ô")
        print("=" * 80)
        print(
            f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞—Ç–æ–≤: {self.stats['processed_chats']}/{self.stats['total_chats']}"
        )
        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —á–∞—Ç–æ–≤: {self.stats['skipped_chats']}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {self.stats['files_processed']}")
        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {self.stats['files_skipped']}")
        print(f"–í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ input: {self.stats['total_messages_input']}")
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {self.stats['messages_copied']}")
        print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –¥–∞—Ç–µ: {self.stats['messages_filtered_by_date']}")
        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates_skipped']}")
        print(f"–û—à–∏–±–æ–∫: {self.stats['errors']}")

        if self.stats["total_messages_input"] > 0:
            copy_percent = (
                self.stats["messages_copied"] / self.stats["total_messages_input"]
            ) * 100
            print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö: {copy_percent:.2f}%")


class MessageDeduplicator:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –ø–æ–ª—é 'id'."""

    def __init__(self, chats_dir: str = "chats"):
        self.chats_dir = Path(chats_dir)
        self.stats = {
            "total_chats": 0,
            "processed_chats": 0,
            "total_messages": 0,
            "duplicates_removed": 0,
            "unique_messages": 0,
            "errors": 0,
        }

    def deduplicate_chat(self, chat_dir: Path) -> Dict[str, int]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –æ–¥–Ω–æ–º —á–∞—Ç–µ."""
        chat_stats = {
            "messages_before": 0,
            "messages_after": 0,
            "duplicates_removed": 0,
            "errors": 0,
        }

        # –ò—â–µ–º JSON —Ñ–∞–π–ª—ã –≤ —á–∞—Ç–µ (unknown.json –∏–ª–∏ result.json)
        json_files = []
        for pattern in ["unknown.json", "result.json"]:
            json_file = chat_dir / pattern
            if json_file.exists():
                json_files.append(json_file)

        if not json_files:
            logger.warning(f"–ù–µ—Ç JSON —Ñ–∞–π–ª–æ–≤ –≤ —á–∞—Ç–µ: {chat_dir}")
            return chat_stats

        for json_file in json_files:
            try:
                # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                messages = []
                with open(json_file, encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if not line:
                            continue

                        try:
                            message = json.loads(line)
                            messages.append(message)
                            chat_stats["messages_before"] += 1
                        except json.JSONDecodeError as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ JSON –≤ {json_file}:{line_num}: {e}")
                            chat_stats["errors"] += 1

                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
                seen_ids = set()
                unique_messages = []
                duplicates_count = 0

                for message in messages:
                    message_id = str(message.get("id", ""))
                    if message_id and message_id not in seen_ids:
                        seen_ids.add(message_id)
                        unique_messages.append(message)
                    else:
                        duplicates_count += 1

                chat_stats["duplicates_removed"] += duplicates_count
                chat_stats["messages_after"] += len(unique_messages)

                # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                if duplicates_count > 0:
                    with open(json_file, "w", encoding="utf-8") as f:
                        for message in unique_messages:
                            json.dump(message, f, ensure_ascii=False)
                            f.write("\n")

                    logger.info(
                        f"–£–¥–∞–ª–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ {json_file.name}"
                    )

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {json_file}: {e}")
                chat_stats["errors"] += 1

        return chat_stats

    def deduplicate_all_chats(self) -> Dict[str, int]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–æ –≤—Å–µ—Ö —á–∞—Ç–∞—Ö."""
        if not self.chats_dir.exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.chats_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return self.stats

        chat_dirs = [d for d in self.chats_dir.iterdir() if d.is_dir()]
        self.stats["total_chats"] = len(chat_dirs)

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(chat_dirs)} —á–∞—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        for chat_dir in chat_dirs:
            try:
                chat_stats = self.deduplicate_chat(chat_dir)

                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.stats["processed_chats"] += 1
                self.stats["total_messages"] += chat_stats["messages_before"]
                self.stats["duplicates_removed"] += chat_stats["duplicates_removed"]
                self.stats["unique_messages"] += chat_stats["messages_after"]
                self.stats["errors"] += chat_stats["errors"]

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Ç–∞ {chat_dir}: {e}")
                self.stats["errors"] += 1

        return self.stats

    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
        print("\n" + "=" * 70)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–î–ê–õ–ï–ù–ò–Ø –î–£–ë–õ–ò–ö–ê–¢–û–í")
        print("=" * 70)
        print(
            f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞—Ç–æ–≤: {self.stats['processed_chats']}/{self.stats['total_chats']}"
        )
        print(f"–í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {self.stats['total_messages']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {self.stats['unique_messages']}")
        print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates_removed']}")
        print(f"–û—à–∏–±–æ–∫: {self.stats['errors']}")

        if self.stats["total_messages"] > 0:
            duplicate_percent = (
                self.stats["duplicates_removed"] / self.stats["total_messages"]
            ) * 100
            print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicate_percent:.2f}%")

        print("=" * 70)


class ProcessManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""

    @staticmethod
    def kill_processes_by_name(pattern: str) -> int:
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –ø–æ –∏–º–µ–Ω–∏"""
        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)

            lines = result.stdout.split("\n")
            killed_count = 0

            for line in lines:
                if pattern in line and "grep" not in line:
                    parts = line.split()
                    if len(parts) >= 2:
                        pid = parts[1]
                        try:
                            os.kill(int(pid), signal.SIGTERM)
                            print(f"‚úÖ –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å {pid}: {line[:80]}...")
                            killed_count += 1
                        except (OSError, ValueError) as e:
                            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å {pid}: {e}")

            return killed_count

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {e}")
            return 0

    @staticmethod
    def stop_ollama():
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä"""
        print("\nüõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞...")

        # –ü–æ–ø—Ä–æ–±—É–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ ollama stop
        try:
            result = subprocess.run(
                ["ollama", "stop"], capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                print("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —á–µ—Ä–µ–∑ ollama stop")
            else:
                print(f"‚ö†Ô∏è  ollama stop –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –∫–æ–¥–æ–º {result.returncode}")
        except subprocess.TimeoutExpired:
            print("‚ö†Ô∏è  ollama stop –ø—Ä–µ–≤—ã—Å–∏–ª –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è")
        except FileNotFoundError:
            print("‚ö†Ô∏è  –ö–æ–º–∞–Ω–¥–∞ ollama –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ Ollama: {e}")

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏–º –ø—Ä–æ—Ü–µ—Å—Å—ã ollama
        ollama_count = ProcessManager.kill_processes_by_name("ollama")
        if ollama_count > 0:
            print(f"‚úÖ –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {ollama_count} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ Ollama")
        else:
            print("‚ÑπÔ∏è  –ü—Ä–æ—Ü–µ—Å—Å—ã Ollama –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    @staticmethod
    def stop_indexing_processes():
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        print("\nüõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")

        patterns = [
            "python.*index",
            "python.*memory_mcp",
            "python.*process_and_index",
            "python.*summarize",
            "python.*markdown",
        ]

        total_killed = 0
        for pattern in patterns:
            count = ProcessManager.kill_processes_by_name(pattern)
            total_killed += count

        if total_killed > 0:
            print(f"‚úÖ –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {total_killed} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        else:
            print("‚ÑπÔ∏è  –ü—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    @staticmethod
    def check_remaining_processes():
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ—Ü–µ—Å—Å—ã"""
        print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤...")

        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)

            lines = result.stdout.split("\n")
            remaining = []

            for line in lines:
                if (
                    any(
                        keyword in line.lower()
                        for keyword in ["index", "memory_mcp", "ollama", "summarize"]
                    )
                    and "grep" not in line
                ):
                    remaining.append(line)

            if remaining:
                print("‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ—Ü–µ—Å—Å—ã:")
                for line in remaining:
                    print(f"   {line[:100]}...")
            else:
                print("‚úÖ –í—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {e}")

    @staticmethod
    def stop_all_indexing():
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        print("üõë –û–°–¢–ê–ù–û–í–ö–ê –í–°–ï–• –ü–†–û–¶–ï–°–°–û–í –ò–ù–î–ï–ö–°–ê–¶–ò–ò")
        print("=" * 50)

        # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        ProcessManager.stop_indexing_processes()

        # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama
        ProcessManager.stop_ollama()

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        ProcessManager.check_remaining_processes()

        print("\n" + "=" * 50)
        print("‚úÖ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print("\nüí° –î–ª—è –ø–æ–ª–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama —Ç–∞–∫–∂–µ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
        print("   pkill -f ollama")
        print("\nüí° –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞:")
        print("   ps aux | grep -E '(index|memory_mcp|ollama)' | grep -v grep")


class TelegramDumpManager:
    """–ì–ª–∞–≤–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Telegram –¥–∞–º–ø–∞–º–∏"""

    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if MCP_AVAILABLE:
            self.mcp = TelegramDumpMCP()
        else:
            self.mcp = None

        if OLLAMA_AVAILABLE:
            self.ollama_client = OllamaEmbeddingClient()
        else:
            self.ollama_client = None

        if INSTRUCTION_MANAGER_AVAILABLE:
            self.instruction_manager = InstructionManager()
        else:
            self.instruction_manager = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.message_extractor = MessageExtractor()
        self.message_deduplicator = MessageDeduplicator()

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        if self.ollama_client:
            await self.ollama_client.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.ollama_client:
            await self.ollama_client.__aexit__(exc_type, exc_val, exc_tb)

    async def check_system(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        print("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
        if self.ollama_client:
            if not await self.ollama_client.check_model_availability():
                print("‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –∏ –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                return False
            print("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω")
        else:
            print("‚ö†Ô∏è OllamaClient –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º ChromaDB
        if self.mcp:
            try:
                self.mcp.collection = self.mcp.chroma_client.get_collection(
                    "telegram_messages"
                )
                count = self.mcp.collection.count()
                print(f"‚úÖ ChromaDB –¥–æ—Å—Ç—É–ø–µ–Ω (—Å–æ–æ–±—â–µ–Ω–∏–π: {count})")
            except:
                print("‚ö†Ô∏è ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        else:
            print("‚ö†Ô∏è MCP —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        return True

    def set_summarization_instruction(
        self,
        *,
        chat: Optional[str] = None,
        mode: Optional[str] = None,
        instruction: Optional[str] = None,
        clear: bool = False,
    ) -> None:
        """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è MCP –∏ CLI."""
        if not self.instruction_manager:
            print("‚ö†Ô∏è InstructionManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return

        target_count = sum(1 for value in (chat, mode) if value)
        if target_count != 1:
            raise ValueError("–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä: chat –∏–ª–∏ mode")

        if clear:
            if chat:
                self.instruction_manager.clear_chat_instruction(chat)
            else:
                self.instruction_manager.clear_mode_instruction(mode)
            return

        if instruction is None or not instruction.strip():
            raise ValueError("–ù–µ –ø–µ—Ä–µ–¥–∞–Ω —Ç–µ–∫—Å—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏")

        if chat:
            self.instruction_manager.set_chat_instruction(chat, instruction)
        else:
            self.instruction_manager.set_mode_instruction(mode, instruction)

    async def get_stats(self) -> None:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã...")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º
        if self.mcp:
            try:
                self.mcp.collection = self.mcp.chroma_client.get_collection(
                    "telegram_messages"
                )
                message_count = self.mcp.collection.count()
                print(f"üìö –°–æ–æ–±—â–µ–Ω–∏–π –≤ –∏–Ω–¥–µ–∫—Å–µ: {message_count}")
            except:
                print("üìö –°–æ–æ–±—â–µ–Ω–∏–π –≤ –∏–Ω–¥–µ–∫—Å–µ: 0")
        else:
            print("üìö MCP —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–∞–π–ª–∞–º
        chats_path = Path("chats")
        if chats_path.exists():
            json_files = list(chats_path.glob("**/*.json"))
            print(f"üìÅ JSON —Ñ–∞–π–ª–æ–≤: {len(json_files)}")
        else:
            print("üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è chats –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        md_files = (
            list(Path("summaries").glob("*.md")) if Path("summaries").exists() else []
        )
        print(f"üìÑ MD —Ñ–∞–π–ª–æ–≤: {len(md_files)}")

    async def extract_messages(
        self,
        dry_run: bool = False,
        filter_by_date: bool = True,
        chat_filter: Optional[str] = None,
        input_dir: str = "input",
        chats_dir: str = "chats",
    ) -> None:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ input –≤ chats"""
        print("üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π...")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç–∏ –≤ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–µ
        self.message_extractor.input_dir = Path(input_dir)
        self.message_extractor.chats_dir = Path(chats_dir)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        self.message_extractor.extract_all_messages(
            dry_run=dry_run, filter_by_date=filter_by_date, chat_filter=chat_filter
        )

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.message_extractor.print_stats()

    async def deduplicate_messages(self, chats_dir: str = "chats") -> None:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π"""
        print("üßπ –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π...")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –≤ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–µ
        self.message_deduplicator.chats_dir = Path(chats_dir)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
        self.message_deduplicator.deduplicate_all_chats()

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.message_deduplicator.print_stats()

    async def stop_indexing(self) -> None:
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        print("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
        ProcessManager.stop_all_indexing()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="Telegram Dump Manager")
    parser.add_argument(
        "command",
        choices=[
            "check",
            "stats",
            "extract-messages",
            "deduplicate",
            "stop-indexing",
        ],
        help="–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è",
    )
    parser.add_argument("--query", "-q", help="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
    parser.add_argument("--limit", "-l", type=int, default=10, help="–õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument(
        "--max-files", type=int, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    )
    parser.add_argument(
        "--batch-size", type=int, default=100, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"
    )
    parser.add_argument(
        "--dry-run", action="store_true", help="–†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
    )
    parser.add_argument(
        "--no-date-filter", action="store_true", help="–û—Ç–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –¥–∞—Ç–µ"
    )
    parser.add_argument("--chat", help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–π —á–∞—Ç")
    parser.add_argument("--input-dir", default="input", help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ input")
    parser.add_argument("--chats-dir", default="chats", help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ chats")

    args = parser.parse_args()

    async with TelegramDumpManager() as manager:
        if args.command == "check":
            await manager.check_system()

        elif args.command == "stats":
            await manager.get_stats()

        elif args.command == "extract-messages":
            await manager.extract_messages(
                dry_run=args.dry_run,
                filter_by_date=not args.no_date_filter,
                chat_filter=args.chat,
                input_dir=args.input_dir,
                chats_dir=args.chats_dir,
            )

        elif args.command == "deduplicate":
            await manager.deduplicate_messages(args.chats_dir)

        elif args.command == "stop-indexing":
            await manager.stop_indexing()


if __name__ == "__main__":
    asyncio.run(main())
