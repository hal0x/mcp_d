#!/usr/bin/env python3
"""
üöÄ MCP –°–µ—Ä–≤–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ Telegram –¥–∞–º–ø–∞–º

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º —á–µ—Ä–µ–∑ Model Context Protocol (MCP).
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º, —Å–µ—Å—Å–∏—è–º –∏ –∑–∞–¥–∞—á–∞–º.
"""
# mypy: ignore-errors

import asyncio
import json
import logging
import sys
from importlib import metadata
from pathlib import Path
from typing import Any, Dict, List, Optional

import chromadb
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Resource, TextContent, Tool

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# –ò–º–ø–æ—Ä—Ç—ã –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è PYTHONPATH
from memory_mcp.core.indexer import TwoLevelIndexer  # noqa: E402
from memory_mcp.core.ollama_client import OllamaEmbeddingClient  # noqa: E402
from memory_mcp.utils.russian_tokenizer import normalize_word, tokenize_text  # noqa: E402

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class TelegramDumpMCPBase:
    """MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ Telegram –¥–∞–º–ø–∞–º"""

    def __init__(
        self,
        chroma_path: str = "./chroma_db",
        chats_path: str = "./chats",
        artifacts_path: str = "./artifacts",
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MCP —Å–µ—Ä–≤–µ—Ä–∞

        Args:
            chroma_path: –ü—É—Ç—å –∫ –±–∞–∑–µ ChromaDB
            chats_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —á–∞—Ç–∞–º–∏
            artifacts_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        """
        self.chroma_path = Path(chroma_path)
        self.chats_path = Path(chats_path)
        self.artifacts_path = Path(artifacts_path)
        self.server = Server("memory-mcp")

        # –ö–ª–∏–µ–Ω—Ç—ã (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self._chroma_client: Optional[chromadb.Client] = None
        self._ollama_client: Optional[OllamaEmbeddingClient] = None

        # –ö–æ–ª–ª–µ–∫—Ü–∏–∏
        self._collections: Dict[str, Any] = {}

        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        self._register_handlers()

    @property
    def chroma_client(self) -> chromadb.Client:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB –∫–ª–∏–µ–Ω—Ç–∞"""
        if self._chroma_client is None:
            self._chroma_client = chromadb.PersistentClient(path=str(self.chroma_path))
            logger.info(f"‚úÖ ChromaDB –ø–æ–¥–∫–ª—é—á–µ–Ω: {self.chroma_path}")
        return self._chroma_client

    @property
    def ollama_client(self) -> OllamaEmbeddingClient:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
        if self._ollama_client is None:
            self._ollama_client = OllamaEmbeddingClient()
            logger.info("‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return self._ollama_client

    def _get_collection(self, collection_name: str):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if collection_name not in self._collections:
            try:
                self._collections[collection_name] = self.chroma_client.get_collection(
                    collection_name
                )
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {collection_name}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}: {e}")
                return None
        return self._collections[collection_name]

    async def _health_payload(self) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤–∞—è health-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: –ø—É—Ç–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤."""
        try:
            stats_raw = await self._get_stats()
            stats = json.loads(stats_raw) if isinstance(stats_raw, str) else stats_raw
        except Exception as exc:  # pragma: no cover - best effort
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è health: %s", exc)
            stats = {"error": str(exc)}

        return {
            "status": "healthy" if "error" not in stats else "degraded",
            "paths": {
                "chroma": str(self.chroma_path),
                "chats": str(self.chats_path),
                "artifacts": str(self.artifacts_path),
            },
            "stats": stats,
        }

    def _version_payload(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ—Ä—Å–∏–∏ –ø–∞–∫–µ—Ç–∞ memory-mcp."""
        try:  # pragma: no cover - metadata lookup
            version = metadata.version("memory_mcp")
        except metadata.PackageNotFoundError:
            version = "0.0.0"
        return {
            "name": "memory-mcp",
            "version": version,
        }

    def _config_snapshot(self) -> Dict[str, Any]:
        """–°–≤–æ–¥–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—É—Ç–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        return {
            "chroma_path": str(self.chroma_path),
            "chats_path": str(self.chats_path),
            "artifacts_path": str(self.artifacts_path),
        }

    def _register_handlers(self):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ MCP"""

        @self.server.list_resources()
        async def list_resources() -> List[Resource]:
            """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
            resources = []

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            resources.append(
                Resource(
                    uri="telegram://stats",
                    name="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤",
                    mimeType="application/json",
                    description="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º",
                )
            )

            # –°–ø–∏—Å–æ–∫ —á–∞—Ç–æ–≤
            resources.append(
                Resource(
                    uri="telegram://chats",
                    name="–°–ø–∏—Å–æ–∫ —á–∞—Ç–æ–≤",
                    mimeType="application/json",
                    description="–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —á–∞—Ç–æ–≤ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π",
                )
            )

            return resources

        @self.server.read_resource()
        async def read_resource(uri: str) -> str:
            """–ß—Ç–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞"""
            if uri == "telegram://stats":
                return await self._get_stats()
            elif uri == "telegram://chats":
                return await self._get_chats_list()
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ—Å—É—Ä—Å: {uri}")

        @self.server.list_tools()
        async def list_tools() -> List[Tool]:
            """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
            return [
                Tool(
                    name="health",
                    description="–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è MCP —Å–µ—Ä–≤–µ—Ä–∞, –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –ø—É—Ç–µ–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤",
                    inputSchema={"type": "object", "properties": {}},
                ),
                Tool(
                    name="version",
                    description="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ—Ä—Å–∏–∏ memory-mcp –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö",
                    inputSchema={"type": "object", "properties": {}},
                ),
                Tool(
                    name="search_messages",
                    description=(
                        "–ü–æ–∏—Å–∫ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º –≤ Telegram —á–∞—Ç–∞—Ö. "
                        "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏. "
                        "–ü—Ä–∏ –≥–ª—É–±–∏–Ω–µ 'deep' –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–µ—Å—Å–∏–π."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å",
                            },
                            "chat_filter": {
                                "type": "string",
                                "description": "–§–∏–ª—å—Ç—Ä –ø–æ –∏–º–µ–Ω–∏ —á–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                            },
                            "limit": {
                                "type": "integer",
                                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                                "default": 10,
                            },
                            "depth": {
                                "type": "string",
                                "description": "–ì–ª—É–±–∏–Ω–∞ –ø–æ–∏—Å–∫–∞: 'shallow' (—Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã), 'medium' (—Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏), 'deep' (—Å –ø–æ–ª–Ω—ã–º–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏)",
                                "enum": ["shallow", "medium", "deep"],
                                "default": "shallow",
                            },
                        },
                        "required": ["query"],
                    },
                ),
                Tool(
                    name="search_sessions",
                    description=(
                        "–ü–æ–∏—Å–∫ –ø–æ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è–º —Å–µ—Å—Å–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤. "
                        "–°–µ—Å—Å–∏—è = —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º. "
                        "–ü—Ä–∏ –≥–ª—É–±–∏–Ω–µ 'deep' –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –æ—Ç—á—ë—Ç—ã —Å–µ—Å—Å–∏–π."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å",
                            },
                            "chat_filter": {
                                "type": "string",
                                "description": "–§–∏–ª—å—Ç—Ä –ø–æ –∏–º–µ–Ω–∏ —á–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                            },
                            "limit": {
                                "type": "integer",
                                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                                "default": 5,
                            },
                            "depth": {
                                "type": "string",
                                "description": "–ì–ª—É–±–∏–Ω–∞ –ø–æ–∏—Å–∫–∞: 'shallow' (—Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã), 'medium' (—Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏), 'deep' (—Å –ø–æ–ª–Ω—ã–º–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏)",
                                "enum": ["shallow", "medium", "deep"],
                                "default": "shallow",
                            },
                            "include_metadata": {
                                "type": "boolean",
                                "description": "–í–∫–ª—é—á–∞—Ç—å –ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (Risks, Actions, Attachments, Uncertainties) –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
                                "default": False,
                            },
                        },
                        "required": ["query"],
                    },
                ),
                Tool(
                    name="search_tasks",
                    description=(
                        "–ü–æ–∏—Å–∫ –ø–æ –∑–∞–¥–∞—á–∞–º –∏ action items, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–º –∏–∑ —á–∞—Ç–æ–≤. "
                        "–í–∫–ª—é—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ, –≤–ª–∞–¥–µ–ª—å—Ü–µ –∏ —Å—Ä–æ–∫–∞—Ö."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å",
                            },
                            "chat_filter": {
                                "type": "string",
                                "description": "–§–∏–ª—å—Ç—Ä –ø–æ –∏–º–µ–Ω–∏ —á–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                            },
                            "limit": {
                                "type": "integer",
                                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                                "default": 5,
                            },
                        },
                        "required": ["query"],
                    },
                ),
                Tool(
                    name="get_chat_info",
                    description="–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —á–∞—Ç–µ",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "chat_name": {"type": "string", "description": "–ò–º—è —á–∞—Ç–∞"}
                        },
                        "required": ["chat_name"],
                    },
                ),
                Tool(
                    name="get_stats",
                    description="–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –∏–Ω–¥–µ–∫—Å–∞–º",
                    inputSchema={"type": "object", "properties": {}},
                ),
                Tool(
                    name="tokenize_text",
                    description=(
                        "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. "
                        "–í–∫–ª—é—á–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —á–∏—Å–µ–ª, –≤–∞–ª—é—Ç, –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "–¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏",
                            },
                        },
                        "required": ["text"],
                    },
                ),
                Tool(
                    name="search_numeric_data",
                    description=(
                        "–ü–æ–∏—Å–∫ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö. "
                        "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –≤–∞–ª—é—Ç–∞–º, —Å—É–º–º–∞–º, –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º –∏ –±–æ–ª—å—à–∏–º —á–∏—Å–ª–∞–º."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏",
                            },
                            "chat_filter": {
                                "type": "string",
                                "description": "–§–∏–ª—å—Ç—Ä –ø–æ –∏–º–µ–Ω–∏ —á–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                            },
                            "limit": {
                                "type": "integer",
                                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                                "default": 10,
                            },
                        },
                        "required": ["query"],
                    },
                ),
                Tool(
                    name="analyze_chat_content",
                    description=(
                        "–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. "
                        "–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –≤–∞–ª—é—Ç—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "chat_name": {
                                "type": "string",
                                "description": "–ò–º—è —á–∞—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                            },
                            "sample_size": {
                                "type": "integer",
                                "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                                "default": 100,
                            },
                        },
                        "required": ["chat_name"],
                    },
                ),
                Tool(
                    name="read_session_report",
                    description=(
                        "–ß—Ç–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ —Å–µ—Å—Å–∏–∏ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏. "
                        "–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Å—Å–∏–∏ –≤–∫–ª—é—á–∞—è Topics, Discussion, Actions, Risks."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "chat_name": {
                                "type": "string",
                                "description": "–ò–º—è —á–∞—Ç–∞",
                            },
                            "session_id": {
                                "type": "string",
                                "description": "ID —Å–µ—Å—Å–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–µ–º—å—è-S0001)",
                            },
                        },
                        "required": ["chat_name", "session_id"],
                    },
                ),
                Tool(
                    name="read_chat_context",
                    description=(
                        "–ß—Ç–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —á–∞—Ç–∞ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤. "
                        "–°–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—â—É—é—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞—Ç–µ –∏ –µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞—Ö."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "chat_name": {
                                "type": "string",
                                "description": "–ò–º—è —á–∞—Ç–∞",
                            },
                        },
                        "required": ["chat_name"],
                    },
                ),
                Tool(
                    name="list_chat_artifacts",
                    description=(
                        "–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —á–∞—Ç–∞. "
                        "–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–µ—Å—Å–∏–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –∏ –¥—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "chat_name": {
                                "type": "string",
                                "description": "–ò–º—è —á–∞—Ç–∞",
                            },
                        },
                        "required": ["chat_name"],
                    },
                ),
                Tool(
                    name="get_chats_list",
                    description=(
                        "–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —á–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞. "
                        "–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–º —á–∞—Ç–µ –≤–∫–ª—é—á–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –¥–∞—Ç—ã."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {},
                    },
                ),
                Tool(
                    name="index_chat",
                    description=(
                        "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ–º –≤—Å–µ—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤. "
                        "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—É—é –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é, –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤."
                    ),
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "chat_name": {
                                "type": "string",
                                "description": "–ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏",
                            },
                            "force_full": {
                                "type": "boolean",
                                "description": "–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –≤—Å–µ—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ —á–∞—Ç–∞",
                                "default": False,
                            },
                            "recent_days": {
                                "type": "integer",
                                "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (0 = –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è)",
                                "default": 0,
                            },
                            "enable_clustering": {
                                "type": "boolean",
                                "description": "–í–∫–ª—é—á–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é —Å–µ—Å—Å–∏–π",
                                "default": False,
                            },
                            "enable_smart_aggregation": {
                                "type": "boolean",
                                "description": "–í–∫–ª—é—á–∏—Ç—å —É–º–Ω—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É —Å —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏",
                                "default": True,
                            },
                            "max_messages_per_group": {
                                "type": "integer",
                                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–µ",
                                "default": 200,
                            },
                            "max_session_hours": {
                                "type": "integer",
                                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏ –≤ —á–∞—Å–∞—Ö",
                                "default": 12,
                            },
                            "gap_minutes": {
                                "type": "integer",
                                "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö",
                                "default": 120,
                            },
                        },
                        "required": ["chat_name"],
                    },
                ),
            ]

        @self.server.call_tool()
        async def call_tool(name: str, arguments: dict) -> List[TextContent]:
            """–í—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
            try:
                if name == "health":
                    payload = await self._health_payload()
                    result = (
                        payload
                        if isinstance(payload, str)
                        else json.dumps(payload, ensure_ascii=False, indent=2)
                    )
                elif name == "version":
                    payload = self._version_payload()
                    result = (
                        payload
                        if isinstance(payload, str)
                        else json.dumps(payload, ensure_ascii=False, indent=2)
                    )
                elif name == "search_messages":
                    result = await self._search_collection(
                        collection_name="chat_messages",
                        query=arguments["query"],
                        chat_filter=arguments.get("chat_filter"),
                        limit=arguments.get("limit", 10),
                        depth=arguments.get("depth", "shallow"),
                    )
                elif name == "search_sessions":
                    result = await self._search_collection(
                        collection_name="chat_sessions",
                        query=arguments["query"],
                        chat_filter=arguments.get("chat_filter"),
                        limit=arguments.get("limit", 5),
                        depth=arguments.get("depth", "shallow"),
                        include_metadata=arguments.get("include_metadata", False),
                    )
                elif name == "search_tasks":
                    result = await self._search_collection(
                        collection_name="chat_tasks",
                        query=arguments["query"],
                        chat_filter=arguments.get("chat_filter"),
                        limit=arguments.get("limit", 5),
                    )
                elif name == "get_chat_info":
                    result = await self._get_chat_info(arguments["chat_name"])
                elif name == "get_stats":
                    result = await self._get_stats()
                elif name == "tokenize_text":
                    result = await self._tokenize_text(arguments["text"])
                elif name == "search_numeric_data":
                    result = await self._search_numeric_data(
                        query=arguments["query"],
                        chat_filter=arguments.get("chat_filter"),
                        limit=arguments.get("limit", 10),
                    )
                elif name == "analyze_chat_content":
                    result = await self._analyze_chat_content(
                        chat_name=arguments["chat_name"],
                        sample_size=arguments.get("sample_size", 100),
                    )
                elif name == "read_session_report":
                    result = await self._read_session_report(
                        chat_name=arguments["chat_name"],
                        session_id=arguments["session_id"],
                    )
                elif name == "read_chat_context":
                    result = await self._read_chat_context(arguments["chat_name"])
                elif name == "list_chat_artifacts":
                    result = await self._list_chat_artifacts(arguments["chat_name"])
                elif name == "get_chats_list":
                    result = await self._get_chats_list()
                elif name == "index_chat":
                    result = await self._index_chat(
                        chat_name=arguments["chat_name"],
                        force_full=arguments.get("force_full", False),
                        recent_days=arguments.get("recent_days", 0),
                        enable_clustering=arguments.get("enable_clustering", False),
                        enable_smart_aggregation=arguments.get(
                            "enable_smart_aggregation", True
                        ),
                        max_messages_per_group=arguments.get(
                            "max_messages_per_group", 200
                        ),
                        max_session_hours=arguments.get("max_session_hours", 12),
                        gap_minutes=arguments.get("gap_minutes", 120),
                    )
                else:
                    result = json.dumps({"error": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {name}"})

                result_text = (
                    result
                    if isinstance(result, str)
                    else json.dumps(result, ensure_ascii=False, indent=2)
                )
                return [TextContent(type="text", text=result_text)]

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ {name}: {e}", exc_info=True)
                error_result = json.dumps(
                    {"error": str(e), "tool": name, "arguments": arguments}
                )
                return [TextContent(type="text", text=error_result)]

    async def _search_collection(
        self,
        collection_name: str,
        query: str,
        chat_filter: Optional[str] = None,
        limit: int = 10,
        depth: str = "shallow",
        include_metadata: bool = False,
    ) -> str:
        """
        –ü–æ–∏—Å–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏

        Args:
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            chat_filter: –§–∏–ª—å—Ç—Ä –ø–æ —á–∞—Ç—É
            limit: –õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            depth: –ì–ª—É–±–∏–Ω–∞ –ø–æ–∏—Å–∫–∞ (shallow, medium, deep)
            include_metadata: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (Risks, Actions, Attachments)

        Returns:
            JSON —Å—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        collection = self._get_collection(collection_name)
        if collection is None:
            return json.dumps(
                {
                    "error": f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                    "hint": "–ó–∞–ø—É—Å—Ç–∏—Ç–µ 'memory_mcp index' –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤",
                },
                ensure_ascii=False,
            )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        async with self.ollama_client:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            normalized_query = normalize_word(query)

            query_embedding = await self.ollama_client._generate_single_embedding(
                normalized_query
            )

            if not query_embedding:
                return json.dumps(
                    {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"},
                    ensure_ascii=False,
                )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        try:
            where_filter = {"chat": chat_filter} if chat_filter else None

            results = collection.query(
                query_embeddings=[query_embedding], n_results=limit, where=where_filter
            )

            if not results["documents"] or not results["documents"][0]:
                return json.dumps(
                    {
                        "query": query,
                        "collection": collection_name,
                        "results": [],
                        "total": 0,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            formatted_results = []
            for doc, metadata, distance in zip(
                results["documents"][0],
                results["metadatas"][0],
                results["distances"][0],
            ):
                # L2 distance: –º–µ–Ω—å—à–µ = –ª—É—á—à–µ (0 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã)
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, –±–µ–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

                result_item = {
                    "text": doc,
                    "distance": round(distance, 3),  # L2 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                    "metadata": metadata,
                }

                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                if collection_name == "chat_messages":
                    result_item["chat"] = metadata.get("chat", "Unknown")
                    result_item["date"] = metadata.get("date_utc", "Unknown")
                elif collection_name == "chat_sessions":
                    result_item["session_id"] = metadata.get("session_id", "Unknown")
                    result_item["chat"] = metadata.get("chat", "Unknown")
                    result_item["time_range"] = metadata.get("time_span", "Unknown")
                elif collection_name == "chat_tasks":
                    result_item["chat"] = metadata.get("chat", "Unknown")
                    result_item["priority"] = metadata.get("priority", "normal")
                    result_item["owner"] = metadata.get("owner", "N/A")
                    result_item["due_date"] = metadata.get("due_date", "N/A")

                formatted_results.append(result_item)

            # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –ø—Ä–∏ –≥–ª—É–±–æ–∫–æ–º –ø–æ–∏—Å–∫–µ
            artifacts = {}
            if depth == "deep":
                artifacts = await self._get_artifacts_for_results(
                    formatted_results, collection_name
                )

            result = {
                "query": query,
                "collection": collection_name,
                "chat_filter": chat_filter,
                "depth": depth,
                "include_metadata": include_metadata,
                "results": formatted_results,
                "total": len(formatted_results),
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
            if include_metadata and collection_name == "chat_sessions":
                metadata_results = await self._get_metadata_for_sessions(
                    formatted_results
                )
                if metadata_results:
                    result["metadata"] = metadata_results

            # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≥–ª—É–±–æ–∫–æ–º –ø–æ–∏—Å–∫–µ
            if depth == "deep" and artifacts:
                result["artifacts"] = artifacts

            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}", exc_info=True)
            return json.dumps(
                {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {str(e)}"}, ensure_ascii=False
            )

    async def _get_stats(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º"""
        try:
            stats = {}

            for collection_name in ["chat_messages", "chat_sessions", "chat_tasks"]:
                collection = self._get_collection(collection_name)
                if collection:
                    stats[collection_name] = collection.count()
                else:
                    stats[collection_name] = 0

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∞—Ç–∞–º
            chats = []
            if self.chats_path.exists():
                for chat_dir in sorted(self.chats_path.iterdir()):
                    if chat_dir.is_dir():
                        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON —Ñ–∞–π–ª (–º–æ–∂–µ—Ç –±—ã—Ç—å result.json –∏–ª–∏ unknown.json)
                        json_file = chat_dir / "result.json"
                        if not json_file.exists():
                            json_file = chat_dir / "unknown.json"
                        if json_file.exists():
                            chats.append(chat_dir.name)

            return json.dumps(
                {
                    "collections": stats,
                    "total_records": sum(stats.values()),
                    "total_chats": len(chats),
                    "chroma_path": str(self.chroma_path),
                    "chats_path": str(self.chats_path),
                },
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def _get_chats_list(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∞—Ç–æ–≤"""
        try:
            chats = []

            if not self.chats_path.exists():
                return json.dumps(
                    {
                        "error": "–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                        "path": str(self.chats_path),
                    },
                    ensure_ascii=False,
                )

            for chat_dir in sorted(self.chats_path.iterdir()):
                if not chat_dir.is_dir():
                    continue

                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON —Ñ–∞–π–ª (–º–æ–∂–µ—Ç –±—ã—Ç—å result.json –∏–ª–∏ unknown.json)
                json_file = chat_dir / "result.json"
                if not json_file.exists():
                    json_file = chat_dir / "unknown.json"
                if not json_file.exists():
                    continue

                try:
                    messages = []
                    first_message_date = None
                    last_message_date = None

                    with open(json_file, encoding="utf-8") as f:
                        for line_num, line in enumerate(f, 1):
                            line = line.strip()
                            if not line:
                                continue
                            try:
                                message = json.loads(line)
                                messages.append(message)

                                # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É —Å–æ–æ–±—â–µ–Ω–∏—è
                                message_date = message.get("date_utc") or message.get(
                                    "date"
                                )
                                if message_date:
                                    if first_message_date is None:
                                        first_message_date = message_date
                                    last_message_date = message_date

                            except json.JSONDecodeError as e:
                                logger.warning(
                                    f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {line_num} –≤ {json_file}: {e}"
                                )
                                continue

                    chat_info = {
                        "name": chat_dir.name,
                        "type": "unknown",
                        "message_count": len(messages),
                        "path": str(chat_dir),
                    }

                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                    if first_message_date:
                        chat_info["first_message"] = first_message_date
                    if last_message_date:
                        chat_info["last_message"] = last_message_date

                    chats.append(chat_info)

                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —á–∞—Ç–∞ {chat_dir.name}: {e}")
                    continue

            return json.dumps(
                {"chats": chats, "total": len(chats)}, ensure_ascii=False, indent=2
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —á–∞—Ç–æ–≤: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def _get_chat_info(self, chat_name: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —á–∞—Ç–µ"""
        try:
            # –ò—â–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —á–∞—Ç–∞
            chat_dir = None
            for d in self.chats_path.iterdir():
                if d.is_dir() and d.name == chat_name:
                    chat_dir = d
                    break

            if not chat_dir:
                return json.dumps(
                    {"error": f"–ß–∞—Ç '{chat_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω"}, ensure_ascii=False
                )

            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON —Ñ–∞–π–ª (–º–æ–∂–µ—Ç –±—ã—Ç—å result.json –∏–ª–∏ unknown.json)
            json_file = chat_dir / "result.json"
            if not json_file.exists():
                json_file = chat_dir / "unknown.json"
            if not json_file.exists():
                return json.dumps(
                    {"error": f"JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —á–∞—Ç–∞ '{chat_name}'"},
                    ensure_ascii=False,
                )

            # –ß–∏—Ç–∞–µ–º JSON Lines —Ñ–æ—Ä–º–∞—Ç (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç)
            messages = []
            with open(json_file, encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        try:
                            message = json.loads(line)
                            messages.append(message)
                        except json.JSONDecodeError:
                            continue

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            info = {
                "name": chat_name,
                "type": "unknown",
                "id": "unknown",
                "message_count": len(messages),
            }

            if messages:
                info["first_message"] = messages[0].get("date_utc", "Unknown")
                info["last_message"] = messages[-1].get("date_utc", "Unknown")

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Å–æ–æ–±—â–µ–Ω–∏–π
                text_count = sum(1 for m in messages if m.get("text"))
                info["text_messages"] = text_count

            return json.dumps(info, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞—Ç–µ: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def _tokenize_text(self, text: str) -> str:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        try:
            tokens = tokenize_text(text)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã —Ç–æ–∫–µ–Ω–æ–≤
            money_tokens = [token for token in tokens if token.startswith("money_")]
            amount_tokens = [token for token in tokens if token.startswith("amount_")]
            value_tokens = [token for token in tokens if token.startswith("value_")]
            type_tokens = [
                token
                for token in tokens
                if token in ["billion", "million", "thousand", "percentage"]
            ]
            russian_tokens = [
                token
                for token in tokens
                if any(c in "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è" for c in token.lower())
            ]
            english_tokens = [
                token
                for token in tokens
                if token.isalpha()
                and not any(
                    c in "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è" for c in token.lower()
                )
            ]

            result = {
                "original_text": text,
                "tokens": tokens,
                "token_count": len(tokens),
                "analysis": {
                    "money_tokens": money_tokens,
                    "amount_tokens": amount_tokens,
                    "value_tokens": value_tokens,
                    "type_tokens": type_tokens,
                    "russian_tokens": russian_tokens,
                    "english_tokens": english_tokens,
                },
                "statistics": {
                    "money_count": len(money_tokens),
                    "amount_count": len(amount_tokens),
                    "value_count": len(value_tokens),
                    "type_count": len(type_tokens),
                    "russian_count": len(russian_tokens),
                    "english_count": len(english_tokens),
                },
            }

            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def _search_numeric_data(
        self,
        query: str,
        chat_filter: Optional[str] = None,
        limit: int = 10,
    ) -> str:
        """–ü–æ–∏—Å–∫ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            tokens = tokenize_text(query)

            # –ò—â–µ–º —á–∏—Å–ª–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
            numeric_tokens = []
            for token in tokens:
                if (
                    token.startswith("money_")
                    or token.startswith("amount_")
                    or token.startswith("value_")
                    or token in ["billion", "million", "thousand", "percentage"]
                ):
                    numeric_tokens.append(token)

            if not numeric_tokens:
                return json.dumps(
                    {
                        "query": query,
                        "tokens": tokens,
                        "numeric_tokens": [],
                        "message": "–í –∑–∞–ø—Ä–æ—Å–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                        "results": [],
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
            collection = self._get_collection("chat_messages")
            if collection is None:
                return json.dumps(
                    {
                        "error": "–ö–æ–ª–ª–µ–∫—Ü–∏—è chat_messages –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                        "hint": "–ó–∞–ø—É—Å—Ç–∏—Ç–µ 'memory_mcp index' –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤",
                    },
                    ensure_ascii=False,
                )

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            async with self.ollama_client:
                query_embedding = await self.ollama_client._generate_single_embedding(
                    query
                )

                if not query_embedding:
                    return json.dumps(
                        {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"},
                        ensure_ascii=False,
                    )

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            where_filter = {"chat": chat_filter} if chat_filter else None

            results = collection.query(
                query_embeddings=[query_embedding], n_results=limit, where=where_filter
            )

            if not results["documents"] or not results["documents"][0]:
                return json.dumps(
                    {
                        "query": query,
                        "tokens": tokens,
                        "numeric_tokens": numeric_tokens,
                        "results": [],
                        "total": 0,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            analyzed_results = []
            for doc, metadata, distance in zip(
                results["documents"][0],
                results["metadatas"][0],
                results["distances"][0],
            ):
                # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                doc_tokens = tokenize_text(doc)
                doc_numeric_tokens = []

                for token in doc_tokens:
                    if (
                        token.startswith("money_")
                        or token.startswith("amount_")
                        or token.startswith("value_")
                        or token in ["billion", "million", "thousand", "percentage"]
                    ):
                        doc_numeric_tokens.append(token)

                result_item = {
                    "text": doc,
                    "distance": round(distance, 3),
                    "metadata": metadata,
                    "tokens": doc_tokens,
                    "numeric_tokens": doc_numeric_tokens,
                    "numeric_match": len(set(numeric_tokens) & set(doc_numeric_tokens))
                    > 0,
                }

                analyzed_results.append(result_item)

            return json.dumps(
                {
                    "query": query,
                    "tokens": tokens,
                    "numeric_tokens": numeric_tokens,
                    "chat_filter": chat_filter,
                    "results": analyzed_results,
                    "total": len(analyzed_results),
                    "numeric_matches": len(
                        [r for r in analyzed_results if r["numeric_match"]]
                    ),
                },
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def _analyze_chat_content(
        self, chat_name: str, sample_size: int = 100
    ) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
        try:
            # –ò—â–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —á–∞—Ç–∞
            chat_dir = None
            for d in self.chats_path.iterdir():
                if d.is_dir() and d.name == chat_name:
                    chat_dir = d
                    break

            if not chat_dir:
                return json.dumps(
                    {"error": f"–ß–∞—Ç '{chat_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω"}, ensure_ascii=False
                )

            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON —Ñ–∞–π–ª
            json_file = chat_dir / "result.json"
            if not json_file.exists():
                json_file = chat_dir / "unknown.json"
            if not json_file.exists():
                return json.dumps(
                    {"error": f"JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —á–∞—Ç–∞ '{chat_name}'"},
                    ensure_ascii=False,
                )

            # –ß–∏—Ç–∞–µ–º JSON Lines —Ñ–æ—Ä–º–∞—Ç (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç)
            messages = []
            with open(json_file, encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        try:
                            message = json.loads(line)
                            messages.append(message)
                        except json.JSONDecodeError:
                            continue

            # –ë–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É —Å–æ–æ–±—â–µ–Ω–∏–π
            sample_messages = (
                messages[:sample_size] if len(messages) > sample_size else messages
            )

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
            all_tokens = []
            money_tokens = []
            amount_tokens = []
            value_tokens = []
            type_tokens = []
            russian_tokens = []
            english_tokens = []

            for message in sample_messages:
                text = message.get("text", "")
                if text:
                    tokens = tokenize_text(text)
                    all_tokens.extend(tokens)

                    # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
                    for token in tokens:
                        if token.startswith("money_"):
                            money_tokens.append(token)
                        elif token.startswith("amount_"):
                            amount_tokens.append(token)
                        elif token.startswith("value_"):
                            value_tokens.append(token)
                        elif token in ["billion", "million", "thousand", "percentage"]:
                            type_tokens.append(token)
                        elif any(
                            c in "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"
                            for c in token.lower()
                        ):
                            russian_tokens.append(token)
                        elif token.isalpha():
                            english_tokens.append(token)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            analysis = {
                "chat_name": chat_name,
                "total_messages": len(messages),
                "analyzed_messages": len(sample_messages),
                "total_tokens": len(all_tokens),
                "unique_tokens": len(set(all_tokens)),
                "token_statistics": {
                    "money_tokens": len(money_tokens),
                    "amount_tokens": len(amount_tokens),
                    "value_tokens": len(value_tokens),
                    "type_tokens": len(type_tokens),
                    "russian_tokens": len(russian_tokens),
                    "english_tokens": len(english_tokens),
                },
                "top_tokens": dict(
                    sorted(
                        [
                            (token, count)
                            for token, count in [
                                (token, all_tokens.count(token))
                                for token in set(all_tokens)
                            ]
                        ],
                        key=lambda x: x[1],
                        reverse=True,
                    )[:20]
                ),
                "unique_money_tokens": list(set(money_tokens)),
                "unique_amount_tokens": list(set(amount_tokens)),
                "unique_value_tokens": list(set(value_tokens)),
                "unique_type_tokens": list(set(type_tokens)),
            }

            return json.dumps(analysis, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —á–∞—Ç–∞: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def run(self):
        """–ó–∞–ø—É—Å–∫ MCP —Å–µ—Ä–≤–µ—Ä–∞"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ MCP —Å–µ—Ä–≤–µ—Ä–∞...")
        logger.info(f"üìÅ –ü—É—Ç—å –∫ ChromaDB: {self.chroma_path}")
        logger.info(f"üí¨ –ü—É—Ç—å –∫ —á–∞—Ç–∞–º: {self.chats_path}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if not self.chroma_path.exists():
            logger.warning(f"‚ö†Ô∏è  ChromaDB –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.chroma_path}")

        if not self.chats_path.exists():
            logger.warning(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —á–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.chats_path}")

        async with stdio_server() as (read_stream, write_stream):
            logger.info("‚úÖ MCP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
            await self.server.run(
                read_stream, write_stream, self.server.create_initialization_options()
            )

    async def _get_metadata_for_sessions(self, results: list) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (Risks, Actions, Attachments) –¥–ª—è —Å–µ—Å—Å–∏–π"""
        metadata = {}

        try:
            for result in results:
                session_id = result.get("session_id", "")
                chat = result.get("chat", "Unknown")

                if not session_id:
                    continue

                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª –æ—Ç—á—ë—Ç–∞ —Å–µ—Å—Å–∏–∏
                chat_dir_name = self._normalize_chat_name(chat)
                report_paths = [
                    self.artifacts_path
                    / "reports"
                    / chat_dir_name
                    / "sessions"
                    / f"{session_id}.md",
                    self.artifacts_path
                    / "reports"
                    / chat_dir_name
                    / "sessions"
                    / f"{session_id}-needs-review.md",
                ]

                report_path = None
                for path in report_paths:
                    if path.exists():
                        report_path = path
                        break

                if not report_path:
                    continue

                try:
                    with open(report_path, encoding="utf-8") as f:
                        content = f.read()

                    # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ markdown —Ñ–∞–π–ª–∞
                    session_metadata = self._parse_session_metadata(content)
                    if session_metadata:
                        metadata[f"{chat}_{session_id}"] = {
                            "session_id": session_id,
                            "chat": chat,
                            "file_path": str(report_path),
                            **session_metadata,
                        }

                except Exception as e:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}: {e}"
                    )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π: {e}", exc_info=True)

        return metadata

    def _parse_session_metadata(self, content: str) -> dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ markdown —Ñ–∞–π–ª–∞ —Å–µ—Å—Å–∏–∏"""
        metadata = {}

        try:
            lines = content.split("\n")
            current_section = None
            current_data = []

            for line in lines:
                line = line.strip()

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ü–∏–∏
                if line.startswith("## Actions"):
                    current_section = "actions"
                    current_data = []
                elif line.startswith("## Risks"):
                    current_section = "risks"
                    current_data = []
                elif line.startswith("## Attachments"):
                    current_section = "attachments"
                    current_data = []
                elif line.startswith("## Uncertainties"):
                    current_section = "uncertainties"
                    current_data = []
                elif line.startswith("## ") and current_section:
                    # –ù–æ–≤–∞—è —Å–µ–∫—Ü–∏—è - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é
                    if current_data:
                        metadata[current_section] = current_data
                    current_section = None
                    current_data = []
                elif current_section and line and not line.startswith("#"):
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—É—â—É—é —Å–µ–∫—Ü–∏—é
                    current_data.append(line)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
            if current_section and current_data:
                metadata[current_section] = current_data

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

        return metadata

    async def _get_artifacts_for_results(
        self, results: list, collection_name: str
    ) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –ø—Ä–∏ –≥–ª—É–±–æ–∫–æ–º –ø–æ–∏—Å–∫–µ"""
        artifacts = {}

        try:
            # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —á–∞—Ç—ã –∏ —Å–µ—Å—Å–∏–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            chats = set()
            sessions = set()

            for result in results:
                chat = result.get("chat", "Unknown")
                chats.add(chat)

                if collection_name == "chat_sessions":
                    session_id = result.get("session_id", "")
                    if session_id:
                        sessions.add((chat, session_id))

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã —á–∞—Ç–æ–≤
            for chat in chats:
                try:
                    context_result = await self._read_chat_context(chat)
                    context_data = json.loads(context_result)
                    if "content" in context_data:
                        artifacts[f"context_{self._normalize_chat_name(chat)}"] = {
                            "type": "chat_context",
                            "chat": chat,
                            "content": context_data["content"],
                        }
                except Exception as e:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —á–∞—Ç–∞ {chat}: {e}"
                    )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç—á—ë—Ç—ã —Å–µ—Å—Å–∏–π
            for chat, session_id in sessions:
                try:
                    session_result = await self._read_session_report(chat, session_id)
                    session_data = json.loads(session_result)
                    if "content" in session_data:
                        artifacts[
                            f"session_{self._normalize_chat_name(chat)}_{session_id}"
                        ] = {
                            "type": "session_report",
                            "chat": chat,
                            "session_id": session_id,
                            "content": session_data["content"],
                        }
                except Exception as e:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Ç—á—ë—Ç —Å–µ—Å—Å–∏–∏ {session_id} –¥–ª—è —á–∞—Ç–∞ {chat}: {e}"
                    )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}", exc_info=True)

        return artifacts

    async def _read_session_report(self, chat_name: str, session_id: str) -> str:
        """–ß—Ç–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ —Å–µ—Å—Å–∏–∏ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–º—è —á–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
            chat_dir_name = self._normalize_chat_name(chat_name)
            report_path = (
                self.artifacts_path
                / "reports"
                / chat_dir_name
                / "sessions"
                / f"{session_id}.md"
            )

            if not report_path.exists():
                return json.dumps(
                    {
                        "error": f"–û—Ç—á—ë—Ç —Å–µ—Å—Å–∏–∏ {session_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —á–∞—Ç–∞ {chat_name}",
                        "path": str(report_path),
                    },
                    ensure_ascii=False,
                )

            with open(report_path, encoding="utf-8") as f:
                content = f.read()

            return json.dumps(
                {
                    "chat_name": chat_name,
                    "session_id": session_id,
                    "content": content,
                    "path": str(report_path),
                },
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞ —Å–µ—Å—Å–∏–∏: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def _read_chat_context(self, chat_name: str) -> str:
        """–ß—Ç–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —á–∞—Ç–∞ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        try:
            # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —Ñ–∞–π–ª –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            context_paths = [
                self.artifacts_path / "chat_contexts" / f"{chat_name}_context.md",
                self.artifacts_path
                / "reports"
                / self._normalize_chat_name(chat_name)
                / f"{self._normalize_chat_name(chat_name)}_context.md",
            ]

            context_path = None
            for path in context_paths:
                if path.exists():
                    context_path = path
                    break

            if not context_path:
                return json.dumps(
                    {
                        "error": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —á–∞—Ç–∞ {chat_name}",
                        "searched_paths": [str(p) for p in context_paths],
                    },
                    ensure_ascii=False,
                )

            with open(context_path, encoding="utf-8") as f:
                content = f.read()

            return json.dumps(
                {"chat_name": chat_name, "content": content, "path": str(context_path)},
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–∞—Ç–∞: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    async def _list_chat_artifacts(self, chat_name: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —á–∞—Ç–∞"""
        try:
            chat_dir_name = self._normalize_chat_name(chat_name)
            artifacts = {
                "chat_name": chat_name,
                "normalized_name": chat_dir_name,
                "reports": {},
                "contexts": [],
                "sessions": [],
            }

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—á—ë—Ç—ã
            reports_dir = self.artifacts_path / "reports" / chat_dir_name
            if reports_dir.exists():
                # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á—ë—Ç
                main_report = reports_dir / f"{chat_dir_name}.md"
                if main_report.exists():
                    artifacts["reports"]["main"] = str(main_report)

                # –°–µ—Å—Å–∏–∏
                sessions_dir = reports_dir / "sessions"
                if sessions_dir.exists():
                    for session_file in sessions_dir.glob("*.md"):
                        artifacts["sessions"].append(
                            {"session_id": session_file.stem, "path": str(session_file)}
                        )

                # –ö–æ–Ω—Ç–µ–∫—Å—Ç
                context_file = reports_dir / f"{chat_dir_name}_context.md"
                if context_file.exists():
                    artifacts["contexts"].append(
                        {"type": "report_context", "path": str(context_file)}
                    )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –≤ chat_contexts
            context_file = (
                self.artifacts_path / "chat_contexts" / f"{chat_name}_context.md"
            )
            if context_file.exists():
                artifacts["contexts"].append(
                    {"type": "chat_context", "path": str(context_file)}
                )

            return json.dumps(artifacts, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}", exc_info=True)
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    def _normalize_chat_name(self, chat_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ —á–∞—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        normalized = chat_name.replace(" ", "_").replace("/", "_").replace("\\", "_")
        normalized = "".join(c for c in normalized if c.isalnum() or c in "_-")
        normalized = normalized.lower()

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —á–∞—Ç–æ–≤
        if normalized == "—Å–µ–º—å—è":
            return "semya"

        return normalized

    async def _index_chat(
        self,
        chat_name: str,
        force_full: bool = False,
        recent_days: int = 0,
        enable_clustering: bool = False,
        enable_smart_aggregation: bool = True,
        max_messages_per_group: int = 200,
        max_session_hours: int = 12,
        gap_minutes: int = 120,
    ) -> str:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        Args:
            chat_name: –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            force_full: –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –≤—Å–µ—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            recent_days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (0 = –≤—Å–µ)
            enable_clustering: –í–∫–ª—é—á–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é —Å–µ—Å—Å–∏–π
            enable_smart_aggregation: –í–∫–ª—é—á–∏—Ç—å —É–º–Ω—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É
            max_messages_per_group: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–µ
            max_session_hours: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏ –≤ —á–∞—Å–∞—Ö
            gap_minutes: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö

        Returns:
            JSON —Å—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        """
        try:
            logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —á–∞—Ç–∞: {chat_name}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —á–∞—Ç–∞
            chat_dir = None
            for d in self.chats_path.iterdir():
                if d.is_dir() and d.name == chat_name:
                    chat_dir = d
                    break

            if not chat_dir:
                return json.dumps(
                    {
                        "error": f"–ß–∞—Ç '{chat_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω",
                        "available_chats": [
                            d.name for d in self.chats_path.iterdir() if d.is_dir()
                        ],
                    },
                    ensure_ascii=False,
                )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ JSON —Ñ–∞–π–ª–∞
            json_file = chat_dir / "result.json"
            if not json_file.exists():
                json_file = chat_dir / "unknown.json"
            if not json_file.exists():
                return json.dumps(
                    {
                        "error": f"JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —á–∞—Ç–∞ '{chat_name}'",
                        "path": str(chat_dir),
                    },
                    ensure_ascii=False,
                )

            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
            indexer = TwoLevelIndexer(
                chroma_path=str(self.chroma_path),
                artifacts_path=str(self.artifacts_path),
                enable_clustering=enable_clustering,
                enable_smart_aggregation=enable_smart_aggregation,
                max_messages_per_group=max_messages_per_group,
                max_session_hours=max_session_hours,
                gap_minutes=gap_minutes,
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
            logger.info("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:")
            logger.info(f"   - –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞: {force_full}")
            logger.info(
                f"   - –ü–æ—Å–ª–µ–¥–Ω–∏–µ –¥–Ω–∏: {recent_days if recent_days > 0 else '–≤—Å–µ'}"
            )
            logger.info(f"   - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {enable_clustering}")
            logger.info(f"   - –£–º–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞: {enable_smart_aggregation}")

            stats = await indexer.build_index(
                scope="chat",
                chat=chat_name,
                force_full=force_full,
                recent_days=recent_days,
            )

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "success": True,
                "chat_name": chat_name,
                "parameters": {
                    "force_full": force_full,
                    "recent_days": recent_days,
                    "enable_clustering": enable_clustering,
                    "enable_smart_aggregation": enable_smart_aggregation,
                    "max_messages_per_group": max_messages_per_group,
                    "max_session_hours": max_session_hours,
                    "gap_minutes": gap_minutes,
                },
                "statistics": stats,
                "artifacts_created": {
                    "reports_path": str(
                        self.artifacts_path
                        / "reports"
                        / self._normalize_chat_name(chat_name)
                    ),
                    "chroma_collections": [
                        "chat_sessions",
                        "chat_messages",
                        "chat_tasks",
                    ],
                },
                "message": f"–ß–∞—Ç '{chat_name}' —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω",
            }

            logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞—Ç–∞ '{chat_name}' –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            logger.info(f"   - –°–µ—Å—Å–∏–π: {stats.get('sessions_indexed', 0)}")
            logger.info(f"   - –°–æ–æ–±—â–µ–Ω–∏–π: {stats.get('messages_indexed', 0)}")
            logger.info(f"   - –ó–∞–¥–∞—á: {stats.get('tasks_indexed', 0)}")

            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —á–∞—Ç–∞ '{chat_name}': {e}", exc_info=True
            )
            return json.dumps(
                {
                    "success": False,
                    "chat_name": chat_name,
                    "error": str(e),
                    "error_type": type(e).__name__,
                },
                ensure_ascii=False,
                indent=2,
            )
