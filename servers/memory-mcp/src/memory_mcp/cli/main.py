#!/usr/bin/env python3
"""CLI Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Telegram Dump Manager v2.0."""

import asyncio
import json
import logging
import math
import os
import re
import signal
import subprocess
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import click

from ..utils.russian_tokenizer import tokenize_text as enhanced_tokenize

# ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ñ‚ĞµĞ»ĞµĞ¼ĞµÑ‚Ñ€Ğ¸Ñ ChromaDB
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY_IMPL"] = ""

from ..analysis.insight_graph import SummaryInsightAnalyzer
from ..analysis.instruction_manager import InstructionManager
from ..core.indexer import TwoLevelIndexer
from ..indexing import TelegramIndexer
from ..memory.ingest import MemoryIngestor
from ..memory.typed_graph import TypedGraphMemory
from ..utils.message_extractor import MessageExtractor

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MessageDeduplicator:
    """ĞšĞ»Ğ°ÑÑ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ñ 'id'."""

    def __init__(self, chats_dir: str = "chats"):
        self.chats_dir = Path(chats_dir)
        self.stats = {
            "total_chats": 0,
            "processed_chats": 0,
            "total_messages": 0,
            "duplicates_removed": 0,
            "unique_messages": 0,
            "errors": 0,
        }

    def deduplicate_chat(self, chat_dir: Path) -> Dict[str, int]:
        """Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ñ‡Ğ°Ñ‚Ğµ."""
        chat_stats = {
            "total_messages": 0,
            "duplicates_removed": 0,
            "unique_messages": 0,
            "errors": 0,
        }

        if not chat_dir.exists():
            return chat_stats

        all_messages = []
        for json_file in chat_dir.glob("*.json"):
            try:
                with open(json_file, encoding="utf-8") as f:
                    for line in f:
                        try:
                            message = json.loads(line.strip())
                            if isinstance(message, dict):
                                all_messages.append(message)
                        except json.JSONDecodeError:
                            continue
            except Exception as e:
                logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° {json_file}: {e}")
                chat_stats["errors"] += 1

        chat_stats["total_messages"] = len(all_messages)

        from ..utils.deduplication import deduplicate_by_id

        unique_messages = deduplicate_by_id(all_messages)
        chat_stats["duplicates_removed"] = len(all_messages) - len(unique_messages)
        chat_stats["unique_messages"] = len(unique_messages)

        if unique_messages != all_messages:
            temp_file = chat_dir / "temp_dedup.json"
            try:
                with open(temp_file, "w", encoding="utf-8") as f:
                    for message in unique_messages:
                        f.write(json.dumps(message, ensure_ascii=False) + "\n")

                for json_file in chat_dir.glob("*.json"):
                    if json_file.name != "temp_dedup.json":
                        json_file.unlink()

                final_file = chat_dir / "messages.json"
                temp_file.rename(final_file)

            except Exception as e:
                logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° {chat_dir.name}: {e}")
                chat_stats["errors"] += 1
                if temp_file.exists():
                    temp_file.unlink()

        return chat_stats

    def deduplicate_all_chats(self) -> Dict[str, int]:
        """Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‡Ğ°Ñ‚Ğ°Ñ…."""
        if not self.chats_dir.exists():
            logger.error(f"Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ {self.chats_dir} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
            return self.stats

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ñ‡Ğ°Ñ‚Ğ¾Ğ²
        chat_dirs = [d for d in self.chats_dir.iterdir() if d.is_dir()]
        self.stats["total_chats"] = len(chat_dirs)

        for chat_dir in chat_dirs:
            logger.info(f"Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‡Ğ°Ñ‚Ğ°: {chat_dir.name}")

            # Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ‡Ğ°Ñ‚Ğµ
            chat_stats = self.deduplicate_chat(chat_dir)

            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
            for key, value in chat_stats.items():
                self.stats[key] += value

            self.stats["processed_chats"] += 1

            logger.info(
                f"Ğ§Ğ°Ñ‚Ğ° {chat_dir.name}: {chat_stats['duplicates_removed']} Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾"
            )

        return self.stats

    def print_stats(self):
        """Ğ’Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸."""
        print("\n" + "=" * 60)
        print("ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ Ğ”Ğ•Ğ”Ğ£ĞŸĞ›Ğ˜ĞšĞĞ¦Ğ˜Ğ˜")
        print("=" * 60)
        print(f"ğŸ“ Ğ’ÑĞµĞ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ²: {self.stats['total_chats']}")
        print(f"âœ… ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ²: {self.stats['processed_chats']}")
        print(f"ğŸ“¨ Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: {self.stats['total_messages']}")
        print(f"ğŸ”„ Ğ”ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾: {self.stats['duplicates_removed']}")
        print(f"âœ¨ Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: {self.stats['unique_messages']}")
        print(f"âŒ ĞÑˆĞ¸Ğ±Ğ¾Ğº: {self.stats['errors']}")
        print("=" * 60)


class ProcessManager:
    """ĞšĞ»Ğ°ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸."""

    @staticmethod
    def kill_processes_by_name(pattern: str) -> int:
        """ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸."""
        killed_count = 0
        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)
            lines = result.stdout.split("\n")

            for line in lines:
                if pattern in line and "grep" not in line:
                    parts = line.split()
                    if len(parts) >= 2:
                        pid = parts[1]
                        try:
                            os.kill(int(pid), signal.SIGTERM)
                            killed_count += 1
                            logger.info(f"ĞŸÑ€Ğ¾Ñ†ĞµÑÑ {pid} ({pattern}) Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
                        except (ValueError, ProcessLookupError):
                            continue
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² {pattern}: {e}")

        return killed_count

    @staticmethod
    def stop_ollama():
        """ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ°."""
        logger.info("ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ°...")

        try:
            result = subprocess.run(
                ["ollama", "stop"], capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                logger.info("âœ… Ollama ÑĞµÑ€Ğ²ĞµÑ€ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
            else:
                logger.warning("âš ï¸ Ollama stop Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ», Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ kill")
                ProcessManager.kill_processes_by_name("ollama")
        except subprocess.TimeoutExpired:
            logger.warning("âš ï¸ Timeout Ğ¿Ñ€Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞµ Ollama, Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ kill")
            ProcessManager.kill_processes_by_name("ollama")
        except FileNotFoundError:
            logger.warning("âš ï¸ Ollama Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² PATH, Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ kill")
            ProcessManager.kill_processes_by_name("ollama")
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ollama: {e}")

    @staticmethod
    def stop_indexing_processes():
        """ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸."""
        logger.info("ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸...")

        patterns = [
            "tg_dump.py",
            "index_messages.py",
            "summarize_chats.py",
            "index_summaries.py",
            "cross_analyze.py",
            "ollama",
        ]

        total_killed = 0
        for pattern in patterns:
            killed = ProcessManager.kill_processes_by_name(pattern)
            total_killed += killed

        if total_killed > 0:
            logger.info(f"âœ… ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²: {total_killed}")
        else:
            logger.info("â„¹ï¸ ĞŸÑ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")

    @staticmethod
    def check_remaining_processes():
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²."""
        logger.info("ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²...")

        patterns = [
            "tg_dump.py",
            "index_messages.py",
            "summarize_chats.py",
            "index_summaries.py",
            "cross_analyze.py",
            "ollama",
        ]

        remaining = []
        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)
            lines = result.stdout.split("\n")

            for line in lines:
                for pattern in patterns:
                    if pattern in line and "grep" not in line:
                        remaining.append(line.strip())
                        break
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²: {e}")

        if remaining:
            logger.warning(f"âš ï¸ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(remaining)} Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²:")
            for proc in remaining:
                logger.warning(f"   {proc}")
        else:
            logger.info("âœ… Ğ’ÑĞµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹")

    @staticmethod
    def stop_all_indexing():
        """ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸."""
        logger.info("ğŸ›‘ ĞĞ¡Ğ¢ĞĞĞĞ’ĞšĞ Ğ’Ğ¡Ğ•Ğ¥ ĞŸĞ ĞĞ¦Ğ•Ğ¡Ğ¡ĞĞ’ Ğ˜ĞĞ”Ğ•ĞšĞ¡ĞĞ¦Ğ˜Ğ˜")
        logger.info("=" * 50)

        ProcessManager.stop_indexing_processes()
        ProcessManager.stop_ollama()

        import time
        time.sleep(2)

        ProcessManager.check_remaining_processes()

        logger.info("=" * 50)
        logger.info("âœ… ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")


@click.group()
@click.version_option(version="2.0.0", prog_name="memory_mcp")
@click.option("--verbose", "-v", is_flag=True, help="ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´")
@click.option("--quiet", "-q", is_flag=True, help="Ğ¢Ğ¸Ñ…Ğ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼")
def cli(verbose, quiet):
    """ğŸš€ Telegram Dump Manager v2.0 - Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ¼Ğ¿Ğ°Ğ¼Ğ¸ Telegram Ñ‡Ğ°Ñ‚Ğ¾Ğ²

    Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ CLI Ğ´Ğ»Ñ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Telegram Ñ‡Ğ°Ñ‚Ğ¾Ğ².

    ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:
      â€¢ index              - Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ (ÑĞµÑÑĞ¸Ğ¸ + ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ + Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸)
      â€¢ ingest-telegram    - ĞŸÑ€ÑĞ¼Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ‡Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
      â€¢ indexing-progress  - Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¾Ğ¼ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸
      â€¢ update-summaries   - ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ markdown-Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸
      â€¢ review-summaries   - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ĞµĞ²ÑŒÑ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹
      â€¢ rebuild-vector-db  - ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²
      â€¢ search             - ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼
      â€¢ insight-graph      - ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹
      â€¢ stats              - Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
      â€¢ check              - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
      â€¢ extract-messages   - Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· input Ğ² chats
      â€¢ deduplicate        - Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
      â€¢ stop-indexing      - ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸
    """
    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    elif quiet:
        logging.getLogger().setLevel(logging.WARNING)
    else:
        logging.getLogger().setLevel(logging.INFO)


@cli.command("ingest-telegram")
@click.option(
    "--chats-dir",
    default="chats",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Telegram",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="ĞŸÑƒÑ‚ÑŒ Ğº SQLite Ğ±Ğ°Ğ·Ğµ Ñ‚Ğ¸Ğ¿Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
)
@click.option(
    "--chat",
    "selected_chats",
    multiple=True,
    help="Ğ˜Ğ¼Ñ Ñ‡Ğ°Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾)",
)
def ingest_telegram(chats_dir: Path, db_path: Path, selected_chats: tuple[str, ...]):
    """ğŸ“š Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Telegram Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ² Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."""

    chosen = [chat for chat in selected_chats if chat] or None
    indexer = TelegramIndexer(chats_dir=str(chats_dir), selected_chats=chosen)
    graph: TypedGraphMemory | None = None

    try:
        indexer.prepare()
        graph = TypedGraphMemory(db_path=str(db_path))
        ingestor = MemoryIngestor(graph)
        ingest_stats = ingestor.ingest(indexer.iter_records())
        index_stats = indexer.finalize()
    except Exception as exc:
        raise click.ClickException(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ: {exc}") from exc
    try:
        indexer.close()
    except Exception:  # pragma: no cover - best effort
        logger.debug("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ‚Ğ¾Ñ€", exc_info=True)

    try:
        if graph is not None:
            graph.conn.close()
    except Exception:  # pragma: no cover - best effort
        logger.debug("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ‘Ğ” Ğ³Ñ€Ğ°Ñ„Ğ°", exc_info=True)

    skipped = max(0, index_stats.records_indexed - ingest_stats.records_ingested)

    click.echo("")
    click.echo("ğŸ“¥ Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Telegram Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")
    click.echo(f"â€¢ Ğ§Ğ°Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾: {index_stats.sources_processed}")
    click.echo(
        f"â€¢ Ğ—Ğ°Ğ¿Ğ¸ÑĞµĞ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾: {ingest_stats.records_ingested} "
        f"(Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ: {ingest_stats.attachments_ingested})"
    )
    if skipped:
        click.echo(f"â€¢ ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·-Ğ·Ğ° Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²: {skipped}")
    if index_stats.warnings:
        click.echo("")
        click.echo("âš ï¸  ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ:")
        for warning in index_stats.warnings:
            click.echo(f"  - {warning}")


@cli.command()
@click.option(
    "--embedding-model", default=None, help="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº)"
)
def check(embedding_model):
    """ğŸ”§ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹"""

    async def _check():
        import chromadb

        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings

        click.echo("ğŸ”§ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹...")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ LM Studio Server
        try:
            settings = get_settings()
            lmstudio_client = LMStudioEmbeddingClient(
                model_name=embedding_model or settings.lmstudio_model,
                llm_model_name=settings.lmstudio_llm_model,
                base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
            )
            async with lmstudio_client:
                available = await lmstudio_client.test_connection()
                if not available or not available.get("lmstudio_available", False):
                    click.echo("âŒ LM Studio Server Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½")
                    click.echo(f"Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ LM Studio Server Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ğ½Ğ° {settings.lmstudio_host}:{settings.lmstudio_port}")
                    return False

                if not available.get("model_available", False):
                    click.echo("âŒ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                    click.echo(f"Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ {embedding_model or settings.lmstudio_model} Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ² LM Studio Server")
                    return False

                click.echo("âœ… Ollama Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½")
        except Exception as e:
            click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ Ollama: {e}")
            return False

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ChromaDB ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸
        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")

            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸
            collections_status = []
            try:
                sessions_collection = chroma_client.get_collection("chat_sessions")
                click.echo(
                    f"âœ… ChromaDB chat_sessions: {sessions_collection.count()} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"
                )
                collections_status.append(True)
            except:
                click.echo("âš ï¸  ChromaDB ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ chat_sessions Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                collections_status.append(False)

            try:
                messages_collection = chroma_client.get_collection("chat_messages")
                click.echo(
                    f"âœ… ChromaDB chat_messages: {messages_collection.count()} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"
                )
                collections_status.append(True)
            except:
                click.echo("âš ï¸  ChromaDB ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ chat_messages Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                collections_status.append(False)

            try:
                tasks_collection = chroma_client.get_collection("chat_tasks")
                click.echo(f"âœ… ChromaDB chat_tasks: {tasks_collection.count()} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
                collections_status.append(True)
            except:
                click.echo("âš ï¸  ChromaDB ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ chat_tasks Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                collections_status.append(False)

            if not any(collections_status):
                click.echo(
                    "\nğŸ’¡ ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°: Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ 'memory_mcp index' Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ²"
                )

        except Exception as e:
            click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ ChromaDB: {e}")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹
        chats_path = Path("chats")
        if chats_path.exists():
            json_files = list(chats_path.glob("**/*.json"))
            click.echo(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ JSON Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(json_files)}")
        else:
            click.echo("âŒ Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ chats Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        summaries_path = Path("artifacts/reports")
        if summaries_path.exists():
            md_files = list(summaries_path.glob("**/*.md"))
            click.echo(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ MD Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(md_files)}")
        else:
            click.echo(
                "âš ï¸  Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ artifacts/reports Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° (Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸)"
            )

        click.echo("\nğŸ‰ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ° Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ!")
        return True

    asyncio.run(_check())


@cli.command()
@click.option(
    "--scope",
    default="all",
    type=click.Choice(["all", "chat"]),
    help="ĞĞ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸: all (Ğ²ÑĞµ Ñ‡Ğ°Ñ‚Ñ‹) Ğ¸Ğ»Ğ¸ chat (Ğ¾Ğ´Ğ¸Ğ½ Ñ‡Ğ°Ñ‚)",
)
@click.option("--chat", help="ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ°Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ (ĞµÑĞ»Ğ¸ scope=chat)")
@click.option("--force-full", is_flag=True, help="ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµÑĞ±Ğ¾Ñ€ĞºĞ° Ğ¸Ğ½Ğ´ĞµĞºÑĞ°")
@click.option(
    "--recent-days", default=7, type=int, help="ĞŸĞµÑ€ĞµÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N Ğ´Ğ½ĞµĞ¹"
)
@click.option(
    "--no-quality-check",
    is_flag=True,
    help="ĞÑ‚ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ)",
)
@click.option(
    "--no-improvement",
    is_flag=True,
    help="ĞÑ‚ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
)
@click.option(
    "--min-quality", default=90.0, type=float, help="ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ» ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° (0-100)"
)
@click.option(
    "--enable-clustering",
    is_flag=True,
    help="Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞµÑÑĞ¸Ğ¹ Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸",
)
@click.option(
    "--clustering-threshold",
    default=0.8,
    type=float,
    help="ĞŸĞ¾Ñ€Ğ¾Ğ³ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (0.0-1.0)",
)
@click.option(
    "--min-cluster-size", default=2, type=int, help="ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ° ÑĞµÑÑĞ¸Ğ¹"
)
@click.option(
    "--max-messages-per-group",
    default=200,
    type=int,
    help="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ = Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞµÑÑĞ¸Ğ¹)",
)
@click.option(
    "--max-session-hours",
    default=12,
    type=int,
    help="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞµÑÑĞ¸Ğ¸ Ğ² Ñ‡Ğ°ÑĞ°Ñ… (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ = Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞµÑÑĞ¸Ğ¹)",
)
@click.option(
    "--gap-minutes",
    default=120,
    type=int,
    help="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ°Ñ… (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ = Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞµÑÑĞ¸Ğ¹)",
)
@click.option(
    "--enable-smart-aggregation",
    is_flag=True,
    help="Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ ÑƒĞ¼Ğ½ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾ĞºĞ½Ğ°Ğ¼Ğ¸ (NOW/FRESH/RECENT/OLD)",
)
@click.option(
    "--aggregation-strategy",
    default="smart",
    type=click.Choice(["smart", "channel", "legacy"]),
    help="Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸: smart (ÑƒĞ¼Ğ½Ğ°Ñ), channel (Ğ´Ğ»Ñ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²), legacy (ÑÑ‚Ğ°Ñ€Ğ°Ñ)",
)
@click.option(
    "--now-window-hours",
    default=24,
    type=int,
    help="Ğ Ğ°Ğ·Ğ¼ĞµÑ€ NOW Ğ¾ĞºĞ½Ğ° Ğ² Ñ‡Ğ°ÑĞ°Ñ… (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ: 24)",
)
@click.option(
    "--fresh-window-days",
    default=14,
    type=int,
    help="Ğ Ğ°Ğ·Ğ¼ĞµÑ€ FRESH Ğ¾ĞºĞ½Ğ° Ğ² Ğ´Ğ½ÑÑ… (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ: 14)",
)
@click.option(
    "--recent-window-days",
    default=30,
    type=int,
    help="Ğ Ğ°Ğ·Ğ¼ĞµÑ€ RECENT Ğ¾ĞºĞ½Ğ° Ğ² Ğ´Ğ½ÑÑ… (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ: 30)",
)
@click.option(
    "--strategy-threshold",
    default=1000,
    type=int,
    help="ĞŸĞ¾Ñ€Ğ¾Ğ³ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ: 1000)",
)
@click.option(
    "--force",
    is_flag=True,
    help="ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹",
)
@click.option(
    "--embedding-model", 
    default=None, 
    help="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº)"
)
def index(
    scope,
    chat,
    force_full,
    recent_days,
    no_quality_check,
    no_improvement,
    min_quality,
    enable_clustering,
    clustering_threshold,
    min_cluster_size,
    max_messages_per_group,
    max_session_hours,
    gap_minutes,
    enable_smart_aggregation,
    aggregation_strategy,
    now_window_hours,
    fresh_window_days,
    recent_window_days,
    strategy_threshold,
    force,
    embedding_model,
):
    """ğŸ“š Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ñ‡Ğ°Ñ‚Ğ¾Ğ² (L1: ÑĞµÑÑĞ¸Ğ¸ + ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸, L2: ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, L3: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸)

    ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ°Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞµÑÑĞ¸Ğ¸,
    Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼ Markdown Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ².
    """

    async def _index():
        click.echo("=" * 80)
        click.echo("ğŸš€ Telegram Dump Manager - Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ v2.0")
        click.echo("=" * 80)
        click.echo()

        if scope == "chat" and not chat:
            click.echo("âŒ Ğ”Ğ»Ñ scope='chat' Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ --chat")
            return

        click.echo("ğŸ“¦ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ‚Ğ¾Ñ€Ğ°...")
        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings
        
        settings = get_settings()
        embedding_client = LMStudioEmbeddingClient(
            model_name=embedding_model or settings.lmstudio_model,
            llm_model_name=settings.lmstudio_llm_model,
            base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
        )
        chroma_path = os.getenv("MEMORY_MCP_CHROMA_PATH") or settings.chroma_path
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        db_path = settings.db_path
        if not os.path.isabs(db_path):
            # Ğ Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµĞ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ½Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
            current_dir = Path(__file__).parent
            project_root = current_dir
            while project_root.parent != project_root:
                if (project_root / "pyproject.toml").exists():
                    break
                project_root = project_root.parent
            if not (project_root / "pyproject.toml").exists():
                project_root = Path.cwd()
            db_path = str(project_root / db_path)
        
        graph = TypedGraphMemory(db_path=db_path)
        logger.info(f"Ğ“Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½: {db_path}")
        
        indexer = TwoLevelIndexer(
            chroma_path=chroma_path,
            artifacts_path=settings.artifacts_path,
            embedding_client=embedding_client,
            enable_quality_check=not no_quality_check,
            enable_iterative_refinement=not no_improvement,
            min_quality_score=min_quality,
            enable_clustering=enable_clustering,
            clustering_threshold=clustering_threshold,
            min_cluster_size=min_cluster_size,
            max_messages_per_group=max_messages_per_group,
            max_session_hours=max_session_hours,
            gap_minutes=gap_minutes,
            enable_smart_aggregation=enable_smart_aggregation,
            aggregation_strategy=aggregation_strategy,
            now_window_hours=now_window_hours,
            fresh_window_days=fresh_window_days,
            recent_window_days=recent_window_days,
            strategy_threshold=strategy_threshold,
            force=force,
            graph=graph,  # ĞŸĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼ Ğ³Ñ€Ğ°Ñ„ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹
        )
        click.echo("âœ… Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ‚Ğ¾Ñ€ Ğ³Ğ¾Ñ‚Ğ¾Ğ²")
        click.echo()

        click.echo("âš™ï¸  ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸:")
        click.echo(f"   - Scope: {scope}")
        click.echo(f"   - Chat: {chat or 'Ğ²ÑĞµ Ñ‡Ğ°Ñ‚Ñ‹'}")
        click.echo(f"   - Force full rebuild: {force_full}")
        click.echo(f"   - Force artifacts: {force}")
        click.echo(f"   - Recent days resummary: {recent_days}")
        click.echo()
        click.echo("ğŸ¯ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:")
        click.echo(
            f"   - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: {'âŒ ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ°' if no_quality_check else 'âœ… Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ°'}"
        )
        click.echo(
            f"   - ĞĞ²Ñ‚Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: {'âŒ ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾' if no_improvement else 'âœ… Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾'}"
        )
        click.echo(
            f"   - ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ»: {min_quality}/100 {'(ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼)' if min_quality >= 80 else '(ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼)' if min_quality >= 60 else '(Ğ¼ÑĞ³ĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼)'}"
        )
        click.echo()
        click.echo("ğŸ”— ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞµÑÑĞ¸Ğ¹:")
        click.echo(
            f"   - ĞšĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: {'âœ… Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ°' if enable_clustering else 'âŒ ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ°'}"
        )
        if enable_clustering:
            click.echo(f"   - ĞŸĞ¾Ñ€Ğ¾Ğ³ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°: {clustering_threshold}")
            click.echo(f"   - ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°: {min_cluster_size}")
        click.echo()
        click.echo("ğŸ“Š ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞµÑÑĞ¸Ğ¹:")
        click.echo(f"   - ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ: {max_messages_per_group}")
        click.echo(f"   - ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞµÑÑĞ¸Ğ¸: {max_session_hours} Ñ‡Ğ°ÑĞ¾Ğ²")
        click.echo(f"   - ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸: {gap_minutes} Ğ¼Ğ¸Ğ½ÑƒÑ‚")
        click.echo()

        if enable_smart_aggregation:
            click.echo("ğŸ§  Ğ£Ğ¼Ğ½Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾ĞºĞ½Ğ°Ğ¼Ğ¸:")
            click.echo(f"   - Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: {aggregation_strategy}")
            click.echo(f"   - NOW Ğ¾ĞºĞ½Ğ¾: {now_window_hours} Ñ‡Ğ°ÑĞ¾Ğ² (ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ)")
            click.echo(f"   - FRESH Ğ¾ĞºĞ½Ğ¾: {fresh_window_days} Ğ´Ğ½ĞµĞ¹ (Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾)")
            click.echo(f"   - RECENT Ğ¾ĞºĞ½Ğ¾: {recent_window_days} Ğ´Ğ½ĞµĞ¹ (Ğ¿Ğ¾ Ğ½ĞµĞ´ĞµĞ»ÑĞ¼)")
            click.echo(f"   - OLD Ğ¾ĞºĞ½Ğ¾: >{recent_window_days} Ğ´Ğ½ĞµĞ¹ (Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼)")
            click.echo(f"   - ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹: {strategy_threshold} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹")
            click.echo("   - ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ°Ñ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ NOW Ğ¾ĞºĞ½Ğ°")
            click.echo("   - ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Ollama")
        else:
            click.echo("ğŸ“Š ĞšĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸:")
            click.echo("   - ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ Ğ´Ğ½ÑĞ¼")
            click.echo("   - 10-100 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ")
            click.echo("   - Ğ•ÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ñ‹ Ğ² Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… (>4 Ñ‡Ğ°ÑĞ¾Ğ²)")
            click.echo("   - Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑÑ‚Ñ‹Ñ… Ğ¸ ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹")
            click.echo("   - Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹")
            click.echo("   - ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿")
        click.echo()

        click.echo("ğŸ”„ ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸...")
        click.echo()

        try:
            stats = await indexer.build_index(
                scope=scope, chat=chat, force_full=force_full, recent_days=recent_days
            )

            click.echo()
            click.echo("=" * 80)
            click.echo("âœ… Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾!")
            click.echo("=" * 80)
            click.echo()
            click.echo("ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:")
            click.echo(f"   - ĞŸÑ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ²: {len(stats['indexed_chats'])}")
            click.echo(f"   - Ğ¡ĞµÑÑĞ¸Ğ¹ (L1): {stats['sessions_indexed']}")
            click.echo(f"   - Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ (L2): {stats['messages_indexed']}")
            click.echo(f"   - Ğ—Ğ°Ğ´Ğ°Ñ‡ (L3): {stats['tasks_indexed']}")
            click.echo()

            if stats["indexed_chats"]:
                click.echo("ğŸ“ ĞŸÑ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡Ğ°Ñ‚Ñ‹:")
                for chat_name in stats["indexed_chats"]:
                    click.echo(f"   - {chat_name}")
                click.echo()

            click.echo("ğŸ“‚ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²:")
            click.echo("   - Markdown Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹: ./artifacts/reports/")
            click.echo("   - Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ°: ./chroma_db/")
            click.echo("   - ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸: chat_sessions, chat_messages, chat_tasks")
            click.echo("   - Ğ“Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: ./data/memory_graph.db")
            click.echo()

        except Exception as e:
            click.echo()
            click.echo("=" * 80)
            click.echo("âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸!")
            click.echo("=" * 80)
            click.echo(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
            click.echo()
            import traceback

            traceback.print_exc()
        finally:
            # Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸
            try:
                graph.conn.close()
                logger.info("Ğ“Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚")
            except Exception:
                pass

    asyncio.run(_index())


@cli.command("set-instruction")
@click.option(
    "--chat", help="ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ°Ñ‚Ğ° (ĞºĞ°Ğº Ğ¿Ğ°Ğ¿ĞºĞ° Ğ² chats/) Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸"
)
@click.option(
    "--mode",
    type=click.Choice(["group", "channel"]),
    help="ĞĞ±Ñ‰Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‡Ğ°Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°",
)
@click.option("--text", help="Ğ¢ĞµĞºÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğµ")
@click.option(
    "--file",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="ĞŸÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹",
)
@click.option(
    "--clear",
    is_flag=True,
    help="Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½ÑƒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ° Ğ¸Ğ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°",
)
def set_instruction(chat, mode, text, file, clear):
    """ğŸ“ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."""
    target_count = sum(1 for value in (chat, mode) if value)
    if target_count != 1:
        raise click.UsageError(
            "ĞÑƒĞ¶Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ€Ğ¾Ğ²Ğ½Ğ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²: --chat Ğ¸Ğ»Ğ¸ --mode"
        )

    manager = InstructionManager()

    if clear:
        if chat:
            manager.clear_chat_instruction(chat)
            click.echo(f"ğŸ—‘ï¸ Ğ˜Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ '{chat}' ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ°")
        else:
            manager.clear_mode_instruction(mode)
            click.echo(f"ğŸ—‘ï¸ ĞĞ±Ñ‰Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¸Ğ¿Ğ° '{mode}' Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ°")
        return

    instruction_text = text or ""
    if file:
        instruction_text = file.read_text(encoding="utf-8")
    if not instruction_text.strip():
        raise click.UsageError(
            "ĞĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· --text Ğ¸Ğ»Ğ¸ --file (Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ --clear)."
        )

    if chat:
        manager.set_chat_instruction(chat, instruction_text)
        click.echo(f"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° '{chat}'")
    else:
        manager.set_mode_instruction(mode, instruction_text)
        click.echo(f"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ¾Ğ±Ñ‰Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¸Ğ¿Ğ° '{mode}'")


@cli.command("list-instructions")
def list_instructions():
    """ğŸ“‹ ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."""
    manager = InstructionManager()
    data = manager.export()

    click.echo("ğŸ“Œ Ğ˜Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ğ°Ğ¼:")
    if data["chats"]:
        for name, instruction in sorted(data["chats"].items()):
            preview = instruction.strip().replace("\n", " ")
            if len(preview) > 120:
                preview = preview[:117] + "..."
            click.echo(f"  â€¢ {name}: {preview}")
    else:
        click.echo("  (ĞĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹)")

    click.echo("\nğŸ“Œ Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ñ‡Ğ°Ñ‚Ğ¾Ğ²:")
    for mode in ("group", "channel"):
        instruction = data["modes"].get(mode, "").strip()
        if instruction:
            preview = instruction.replace("\n", " ")
            if len(preview) > 120:
                preview = preview[:117] + "..."
            click.echo(f"  â€¢ {mode}: {preview}")
        else:
            click.echo(f"  â€¢ {mode}: (Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¾)")


def highlight_text(text: str, query: str) -> str:
    """ĞŸĞ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ."""
    keywords = [
        word.strip().lower() for word in query.split() if len(word.strip()) >= 3
    ]

    if not keywords:
        return text

    result = text
    for keyword in keywords:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        result = pattern.sub(
            lambda m: click.style(m.group(0), fg="yellow", bold=True), result
        )

    return result


TOKEN_PATTERN = re.compile(r"\w+", re.UNICODE)
MIN_TOKEN_LENGTH = 3

HYBRID_WEIGHTS = {
    "messages": (0.65, 0.35),
    "sessions": (0.6, 0.4),
    "tasks": (0.6, 0.4),
}

RELEVANCE_THRESHOLDS = {
    "messages": 0.32,
    "sessions": 0.30,
    "tasks": 0.28,
}


def _tokenize(text: str) -> list[str]:
    """Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ fallback Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ."""
    if not text:
        return []

    try:
        return enhanced_tokenize(text)
    except Exception as e:
        logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback: {e}")
        return [
            token
            for token in TOKEN_PATTERN.findall(text.lower())
            if len(token) >= MIN_TOKEN_LENGTH
        ]


def _bm25_scores(
    query_tokens: list[str], documents_tokens: list[list[str]]
) -> list[float]:
    """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ BM25 Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."""
    if not query_tokens or not documents_tokens:
        return [0.0] * len(documents_tokens)

    num_docs = len(documents_tokens)
    doc_freq = Counter()
    doc_lengths = []
    for tokens in documents_tokens:
        unique_tokens = set(tokens)
        if unique_tokens:
            doc_freq.update(unique_tokens)
        doc_lengths.append(len(tokens))

    avgdl = sum(doc_lengths) / num_docs if num_docs else 0
    if avgdl == 0:
        return [0.0] * len(documents_tokens)

    idf = {}
    for token, freq in doc_freq.items():
        idf[token] = math.log(((num_docs - freq + 0.5) / (freq + 0.5)) + 1.0)

    scores = []
    k1, b = 1.5, 0.75  # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ BM25
    for tokens, doc_len in zip(documents_tokens, doc_lengths):
        if not tokens:
            scores.append(0.0)
            continue

        term_freq = Counter(tokens)
        score = 0.0
        for token in query_tokens:
            token_idf = idf.get(token)
            tf = term_freq.get(token)
            if not token_idf or not tf:
                continue
            denom = tf + k1 * (1 - b + b * (doc_len / avgdl))
            score += token_idf * (tf * (k1 + 1) / denom)

        scores.append(score)

    return scores


@cli.command()
@click.argument("query")
@click.option("--limit", "-l", default=10, help="Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²")
@click.option(
    "--collection",
    "-c",
    type=click.Choice(["messages", "sessions", "tasks"]),
    default="messages",
    help="ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
)
@click.option("--chat", help="Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ñƒ (Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ°Ñ‚Ğ°)")
@click.option(
    "--highlight/--no-highlight", default=True, help="ĞŸĞ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ²"
)
@click.option(
    "--embedding-model", 
    default=None, 
    help="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº)"
)
def search(query, limit, collection, chat, highlight, embedding_model):
    """ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼

    ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼:
    - messages: ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼
    - sessions: ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼ ÑĞµÑÑĞ¸Ğ¹
    - tasks: ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ (Action Items)
    """

    async def _search():
        import chromadb

        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings

        click.echo(f"ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ² ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ '{collection}': '{query}'")
        if chat:
            click.echo(f"ğŸ“‹ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ñƒ: '{chat}'")

        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")
            settings = get_settings()
            embedding_client = LMStudioEmbeddingClient(
                model_name=embedding_model or settings.lmstudio_model,
                base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
            )

            collection_name = f"chat_{collection}"
            try:
                coll = chroma_client.get_collection(collection_name)
            except:
                click.echo(f"âŒ ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ {collection_name} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
                click.echo("ğŸ’¡ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ 'memory_mcp index' Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ²")
                return

            async with embedding_client:
                query_embedding = await embedding_client._generate_single_embedding(query)

                if not query_embedding:
                    click.echo("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°")
                    return

                where_filter = {"chat": chat} if chat else None
                vector_limit = max(limit * 4, 20)
                results = coll.query(
                    query_embeddings=[query_embedding],
                    n_results=vector_limit,
                    where=where_filter,
                )

                documents = results.get("documents")
                if not documents or not documents[0]:
                    click.echo("âŒ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
                    return

                raw_ids = results.get("ids") or [[]]
                raw_ids = raw_ids[0] if raw_ids else []
                metadatas = results.get("metadatas", [[]])[0]
                distances = results.get("distances", [[]])[0]

                def resolve_doc_id(raw_id, metadata, doc_text):
                    if raw_id:
                        return raw_id
                    metadata = metadata or {}
                    for key in ("msg_id", "session_id", "task_id", "id"):
                        value = metadata.get(key)
                        if value:
                            return value
                    return f"doc-{abs(hash((doc_text or '')[:80]))}"

                vector_scores: dict[str, float] = {}
                vector_distances: dict[str, float] = {}
                vector_candidates = []

                for doc, metadata, distance, raw_id in zip(
                    documents[0], metadatas, distances, raw_ids
                ):
                    if not doc:
                        continue
                    doc_id = resolve_doc_id(raw_id, metadata, doc)
                    vector_candidates.append(
                        {
                            "id": doc_id,
                            "doc": doc,
                            "metadata": metadata or {},
                            "distance": distance,
                        }
                    )
                    vector_distances[doc_id] = distance

                if not vector_candidates:
                    click.echo("âŒ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
                    return

                available_distances = [
                    item["distance"]
                    for item in vector_candidates
                    if item.get("distance") is not None
                ]
                if available_distances:
                    min_distance = min(available_distances)
                    max_distance = max(available_distances)
                    denominator = max(max_distance - min_distance, 1e-6)
                    for item in vector_candidates:
                        doc_id = item["id"]
                        distance = item.get("distance")
                        if distance is None:
                            continue
                        if max_distance == min_distance:
                            vector_scores[doc_id] = 1.0
                        else:
                            score = (max_distance - distance) / denominator
                            vector_scores[doc_id] = max(score, 0.0)

                get_kwargs = {"include": ["documents", "metadatas"]}
                if where_filter:
                    get_kwargs["where"] = where_filter

                corpus = coll.get(**get_kwargs)
                corpus_docs = corpus.get("documents", [])
                corpus_meta = corpus.get("metadatas", [])

                doc_store: dict[str, dict[str, object]] = {}
                lexical_entries: list[str] = []
                lexical_tokens: list[list[str]] = []

                for idx, (doc_text, metadata) in enumerate(
                    zip(corpus_docs, corpus_meta)
                ):
                    raw_id = f"doc_{idx}_{hash(doc_text or '')}"
                    resolved_id = resolve_doc_id(raw_id, metadata, doc_text)
                    doc_store[resolved_id] = {
                        "doc": doc_text or "",
                        "metadata": metadata or {},
                    }
                    lexical_entries.append(resolved_id)
                    lexical_tokens.append(_tokenize(doc_text or ""))

                query_tokens = _tokenize(query)
                lexical_scores_list = _bm25_scores(query_tokens, lexical_tokens)
                lexical_scores = dict(zip(lexical_entries, lexical_scores_list))
                max_lexical_score = (
                    max(lexical_scores.values()) if lexical_scores else 0.0
                )
                lexical_norm = {
                    doc_id: (score / max_lexical_score)
                    if max_lexical_score > 0
                    else 0.0
                    for doc_id, score in lexical_scores.items()
                }

                weight_vector, weight_lexical = HYBRID_WEIGHTS.get(
                    collection, (0.6, 0.4)
                )
                if not query_tokens or max_lexical_score == 0:
                    weight_vector, weight_lexical = 1.0, 0.0
                weight_sum = weight_vector + weight_lexical
                if weight_sum == 0:
                    weight_vector, weight_lexical = 1.0, 0.0
                else:
                    weight_vector /= weight_sum
                    weight_lexical /= weight_sum

                candidate_ids = set(vector_scores.keys())
                if lexical_scores:
                    sorted_lexical = sorted(
                        lexical_scores.items(), key=lambda item: item[1], reverse=True
                    )
                    top_lexical = [
                        doc_id for doc_id, score in sorted_lexical if score > 0
                    ][: max(limit * 3, 15)]
                    candidate_ids.update(top_lexical)

                final_candidates = []
                for doc_id in candidate_ids:
                    payload = doc_store.get(doc_id)
                    if not payload:
                        continue
                    vector_component = vector_scores.get(doc_id, 0.0)
                    lexical_component = lexical_norm.get(doc_id, 0.0)
                    hybrid_score = (
                        vector_component * weight_vector
                        + lexical_component * weight_lexical
                    )
                    final_candidates.append(
                        {
                            "id": doc_id,
                            "doc": payload["doc"],
                            "metadata": payload["metadata"],
                            "score": hybrid_score,
                            "vector_component": vector_component,
                            "lexical_component": lexical_component,
                            "vector_distance": vector_distances.get(doc_id),
                        }
                    )

                if not final_candidates:
                    click.echo("âŒ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
                    return

                final_candidates.sort(key=lambda item: item["score"], reverse=True)

                threshold = RELEVANCE_THRESHOLDS.get(collection, 0.0)
                filtered_candidates = [
                    candidate
                    for candidate in final_candidates
                    if candidate["score"] >= threshold
                ]
                filtered_out = len(final_candidates) - len(filtered_candidates)

                if not filtered_candidates:
                    filtered_candidates = final_candidates[:limit]
                    filtered_out = 0
                else:
                    filtered_candidates = filtered_candidates[:limit]

                click.echo(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: {len(filtered_candidates)}")
                if filtered_out > 0:
                    click.echo(f"   (Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¾ Ğ¿Ğ¾ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ñƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸: {filtered_out})")
                click.echo()

                for index, candidate in enumerate(filtered_candidates, 1):
                    metadata = candidate.get("metadata") or {}
                    chat_name = metadata.get(
                        "chat", metadata.get("chat_name", "Unknown")
                    )
                    signal_parts = []
                    if candidate.get("vector_component", 0) > 0:
                        signal_parts.append("vec")
                    if candidate.get("lexical_component", 0) > 0:
                        signal_parts.append("lex")
                    signals = "+".join(signal_parts) if signal_parts else "-"
                    header = f"{index}. {chat_name} (score: {candidate['score'] * 100:.1f} | signals: {signals}"
                    distance = candidate.get("vector_distance")
                    if distance is not None:
                        header += f" | distance: {distance:.1f}"
                    header += ")"
                    click.echo(header)

                    doc_text = candidate.get("doc") or ""

                    if collection == "messages":
                        text = (
                            doc_text[:200] + "..." if len(doc_text) > 200 else doc_text
                        )
                        if highlight:
                            text = highlight_text(text, query)
                        click.echo(f"   {text}")
                    elif collection == "sessions":
                        session_id = metadata.get("session_id", "N/A")
                        time_range = metadata.get("time_span", "N/A")
                        click.echo(f"   Session: {session_id}")
                        click.echo(f"   Time: {time_range}")
                        summary = (
                            doc_text[:150] + "..." if len(doc_text) > 150 else doc_text
                        )
                        if highlight:
                            summary = highlight_text(summary, query)
                        click.echo(f"   Summary: {summary}")
                    elif collection == "tasks":
                        task_text = (
                            doc_text[:200] + "..." if len(doc_text) > 200 else doc_text
                        )
                        if highlight:
                            task_text = highlight_text(task_text, query)
                        owner = metadata.get("owner", "N/A")
                        due_date = metadata.get("due", "N/A")
                        priority = metadata.get("priority", "N/A")
                        click.echo(f"   Task: {task_text}")
                        click.echo(
                            f"   Owner: {owner} | Due: {due_date} | Priority: {priority}"
                        )

                    click.echo()

        except Exception as e:
            click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(_search())


@cli.command()
@click.option(
    "--threshold", default=0.76, type=float, help="ĞŸĞ¾Ñ€Ğ¾Ğ³ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡Ğ°Ñ‚Ğ°Ğ¼Ğ¸"
)
@click.option("--graphml", type=click.Path(), help="ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ GraphML-Ñ„Ğ°Ğ¹Ğ»Ğ°")
def insight_graph(threshold, graphml):
    """ğŸ§  ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹

    Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ ÑĞ²ÑĞ·ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹
    Ğ¸ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡Ğ°Ñ‚Ğ°Ğ¼Ğ¸.
    """

    async def _run():
        click.echo("ğŸ§  ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²...")
        click.echo(f"   ĞŸĞ¾Ñ€Ğ¾Ğ³ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸: {threshold}")
        click.echo()

        analyzer = SummaryInsightAnalyzer(
            summaries_dir=Path("artifacts/reports"),
            similarity_threshold=threshold,
        )

        try:
            # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ Ğ³Ñ€Ğ°Ñ„
            async with analyzer:
                result = await analyzer.analyze()

            # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
            click.echo("\n" + "=" * 80)
            click.echo("âœ… Ğ“Ñ€Ğ°Ñ„ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½!")
            click.echo("=" * 80)
            click.echo()

            graph_metrics = result.metrics.get("graph", {})
            click.echo("ğŸ“Š ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ³Ñ€Ğ°Ñ„Ğ°:")
            click.echo(f"   - Ğ£Ğ·Ğ»Ğ¾Ğ² (Ñ‡Ğ°Ñ‚Ğ¾Ğ²): {graph_metrics.get('nodes', 0)}")
            click.echo(f"   - Ğ Ñ‘Ğ±ĞµÑ€ (ÑĞ²ÑĞ·ĞµĞ¹): {graph_metrics.get('edges', 0)}")
            click.echo(f"   - ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: {graph_metrics.get('components', 0)}")
            click.echo(f"   - ĞŸĞ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ°: {graph_metrics.get('density', 0.0):.3f}")
            click.echo()

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
            report_path = Path("insight_graph_report.md")
            report_content = analyzer.generate_report(result)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)
            click.echo(f"ğŸ“„ ĞÑ‚Ñ‡Ñ‘Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {report_path}")

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ GraphML ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ¿ÑƒÑ‚ÑŒ
            if graphml:
                export_path = analyzer.export_graphml(result, Path(graphml))
                if export_path:
                    click.echo(f"ğŸ“ GraphML ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {export_path}")

        except Exception as e:
            click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ°: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(_run())


@cli.command()
def stats():
    """ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""

    async def _stats():
        import chromadb

        click.echo("ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹...")
        click.echo()

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ChromaDB ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸
        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")

            # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸ÑĞ¼
            total_records = 0
            for coll_name in ["chat_sessions", "chat_messages", "chat_tasks"]:
                try:
                    coll = chroma_client.get_collection(coll_name)
                    count = coll.count()
                    total_records += count
                    icon = "âœ…" if count > 0 else "âš ï¸ "
                    click.echo(f"{icon} {coll_name}: {count} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
                except:
                    click.echo(f"âŒ {coll_name}: Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")

            click.echo()
            click.echo(f"ğŸ“¦ Ğ’ÑĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ…: {total_records}")

        except Exception as e:
            click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞµ ChromaDB: {e}")

        click.echo()

        # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼
        chats_path = Path("chats")
        if chats_path.exists():
            json_files = list(chats_path.glob("**/*.json"))
            click.echo(f"ğŸ“ JSON Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(json_files)}")

            # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ²
            chat_dirs = [d for d in chats_path.iterdir() if d.is_dir()]
            click.echo(f"ğŸ’¬ Ğ§Ğ°Ñ‚Ğ¾Ğ²: {len(chat_dirs)}")
        else:
            click.echo("ğŸ“ JSON Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: 0")

        # Markdown Ñ„Ğ°Ğ¹Ğ»Ñ‹
        summaries_path = Path("artifacts/reports")
        if summaries_path.exists():
            md_files = list(summaries_path.glob("**/*.md"))
            session_files = list(summaries_path.glob("**/sessions/*.md"))
            click.echo(f"ğŸ“„ MD Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(md_files)}")
            click.echo(f"ğŸ“ Ğ¡Ğ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ ÑĞµÑÑĞ¸Ğ¹: {len(session_files)}")
        else:
            click.echo("ğŸ“„ MD Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: 0")

    asyncio.run(_stats())


@cli.command("indexing-progress")
@click.option("--chat", help="ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ°")
@click.option(
    "--reset",
    is_flag=True,
    help="Ğ¡Ğ±Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ (Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸)",
)
def indexing_progress(chat, reset):
    """ğŸ”„ Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¾Ğ¼ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸

    ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ°
    Ğ¸Ğ»Ğ¸ ÑĞ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸.
    """

    import chromadb

    try:
        chroma_client = chromadb.PersistentClient(path="./chroma_db")

        try:
            progress_collection = chroma_client.get_collection("indexing_progress")
        except:
            click.echo("âš ï¸  ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ indexing_progress Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
            click.echo("ğŸ’¡ Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ ĞµÑ‰Ñ‘ Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ğ»Ğ°ÑÑŒ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ‚Ğ°Ñ€Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ")
            return

        if reset:
            if chat:
                # Ğ¡Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ°
                from ..utils.naming import slugify

                progress_id = f"progress_{slugify(chat)}"
                try:
                    progress_collection.delete(ids=[progress_id])
                    click.echo(f"âœ… ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° '{chat}' ÑĞ±Ñ€Ğ¾ÑˆĞµĞ½")
                    click.echo(
                        "ğŸ’¡ ĞŸÑ€Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ Ñ‡Ğ°Ñ‚ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾"
                    )
                except Exception as e:
                    click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ±Ñ€Ğ¾ÑĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°: {e}")
            else:
                # Ğ¡Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ²ĞµÑÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ
                try:
                    result = progress_collection.get()
                    if result["ids"]:
                        progress_collection.delete(ids=result["ids"])
                        click.echo(
                            f"âœ… ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ ÑĞ±Ñ€Ğ¾ÑˆĞµĞ½ Ğ´Ğ»Ñ {len(result['ids'])} Ñ‡Ğ°Ñ‚Ğ¾Ğ²"
                        )
                        click.echo(
                            "ğŸ’¡ ĞŸÑ€Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ Ğ²ÑĞµ Ñ‡Ğ°Ñ‚Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾"
                        )
                    else:
                        click.echo("âš ï¸  ĞĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸")
                except Exception as e:
                    click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ±Ñ€Ğ¾ÑĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°: {e}")
        else:
            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ
            click.echo("ğŸ”„ ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸:")
            click.echo()

            try:
                if chat:
                    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ°
                    from ..utils.naming import slugify

                    progress_id = f"progress_{slugify(chat)}"
                    result = progress_collection.get(
                        ids=[progress_id], include=["metadatas"]
                    )

                    if result["ids"]:
                        metadata = result["metadatas"][0]
                        click.echo(f"ğŸ“‹ Ğ§Ğ°Ñ‚: {metadata.get('chat_name', chat)}")
                        click.echo(
                            f"   ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ: {metadata.get('last_indexed_date', 'N/A')}"
                        )
                        click.echo(
                            f"   ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ: {metadata.get('last_indexing_time', 'N/A')}"
                        )
                        click.echo(
                            f"   Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: {metadata.get('total_messages', 0)}"
                        )
                        click.echo(
                            f"   Ğ’ÑĞµĞ³Ğ¾ ÑĞµÑÑĞ¸Ğ¹: {metadata.get('total_sessions', 0)}"
                        )
                    else:
                        click.echo(f"âš ï¸  ĞĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞµ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° '{chat}'")
                else:
                    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‡Ğ°Ñ‚Ğ¾Ğ²
                    result = progress_collection.get(include=["metadatas"])

                    if result["ids"]:
                        click.echo(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹: {len(result['ids'])}")
                        click.echo()

                        for i, metadata in enumerate(result["metadatas"], 1):
                            chat_name = metadata.get("chat_name", "Unknown")
                            last_date = metadata.get("last_indexed_date", "N/A")
                            last_time = metadata.get("last_indexing_time", "N/A")
                            total_msgs = metadata.get("total_messages", 0)
                            total_sessions = metadata.get("total_sessions", 0)

                            click.echo(f"{i}. {chat_name}")
                            click.echo(f"   ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ: {last_date}")
                            click.echo(f"   ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ: {last_time}")
                            click.echo(
                                f"   Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: {total_msgs}, Ğ¡ĞµÑÑĞ¸Ğ¹: {total_sessions}"
                            )
                            click.echo()
                    else:
                        click.echo("âš ï¸  ĞĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸")
                        click.echo("ğŸ’¡ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹: memory_mcp index")
            except Exception as e:
                click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°: {e}")
                import traceback

                traceback.print_exc()

    except Exception as e:
        click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğº ChromaDB: {e}")


@cli.command("update-summaries")
@click.option("--chat", help="ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚Ğ°")
@click.option(
    "--force",
    is_flag=True,
    help="ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹",
)
def update_summaries(chat, force):
    """ğŸ“ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ markdown-Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸

    Ğ§Ğ¸Ñ‚Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ JSON-ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ markdown-Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹,
    Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ» "ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ° 30 Ğ´Ğ½ĞµĞ¹".
    """
    import json
    from datetime import datetime, timedelta
    from zoneinfo import ZoneInfo

    from ..analysis.markdown_renderer import MarkdownRenderer

    async def _update_summaries():
        click.echo("ğŸ“ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ markdown-Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ²...")
        click.echo()

        reports_dir = Path("artifacts/reports")

        if not reports_dir.exists():
            click.echo("âŒ Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ artifacts/reports Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
            click.echo("ğŸ’¡ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ: memory_mcp index")
            return

        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ñ‡Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        if chat:
            chat_dirs = [reports_dir / chat] if (reports_dir / chat).exists() else []
            if not chat_dirs:
                click.echo(f"âŒ Ğ§Ğ°Ñ‚ '{chat}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² artifacts/reports/")
                return
        else:
            chat_dirs = [
                d
                for d in reports_dir.iterdir()
                if d.is_dir() and (d / "sessions").exists()
            ]

        if not chat_dirs:
            click.echo("âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ² Ñ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸")
            return

        click.echo(f"ğŸ“ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ²: {len(chat_dirs)}")
        click.echo()

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ renderer
        renderer = MarkdownRenderer(output_dir=reports_dir)

        def parse_message_time(date_str: str) -> datetime:
            try:
                from ..utils.datetime_utils import parse_datetime_utc

                return parse_datetime_utc(date_str, default=datetime.now(ZoneInfo("UTC")), use_zoneinfo=True)
            except Exception:
                return datetime.now(ZoneInfo("UTC"))

        def load_session_summaries(chat_dir: Path) -> list:
            sessions = []
            sessions_dir = chat_dir / "sessions"
            if not sessions_dir.exists():
                return sessions

            json_files = list(sessions_dir.glob("*.json"))
            for json_file in json_files:
                try:
                    with open(json_file, encoding="utf-8") as f:
                        session = json.load(f)
                        sessions.append(session)
                except Exception as e:
                    click.echo(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ {json_file.name}: {e}")
                    continue
            return sessions

        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚
        updated = 0

        for chat_dir in chat_dirs:
            chat_name = chat_dir.name.replace("_", " ").title()
            click.echo(f"ğŸ“‹ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡Ğ°Ñ‚Ğ°: {chat_name}")

            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            sessions = load_session_summaries(chat_dir)

            if not sessions:
                click.echo("   âš ï¸  ĞĞµÑ‚ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ")
                continue

            click.echo(f"   ğŸ“Š ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹: {len(sessions)}")

            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ ÑĞµÑÑĞ¸Ğ¸ Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 30 Ğ´Ğ½ĞµĞ¹
            now = datetime.now(ZoneInfo("UTC"))
            thirty_days_ago = now - timedelta(days=30)

            recent_sessions = []
            for session in sessions:
                end_time_str = session.get("meta", {}).get("end_time_utc", "")
                if end_time_str:
                    end_time = parse_message_time(end_time_str)
                    if end_time >= thirty_days_ago:
                        recent_sessions.append(session)

            click.echo(f"   ğŸ“… Ğ¡ĞµÑÑĞ¸Ğ¹ Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 30 Ğ´Ğ½ĞµĞ¹: {len(recent_sessions)}")

            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ
            top_sessions = sorted(
                recent_sessions,
                key=lambda s: s.get("quality", {}).get("score", 0),
                reverse=True,
            )

            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹
            try:
                renderer.render_chat_summary(
                    chat_name, sessions, top_sessions=top_sessions, force=force
                )
                renderer.render_cumulative_context(chat_name, sessions, force=force)
                renderer.render_chat_index(chat_name, sessions, force=force)
                click.echo("   âœ… ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹ Ğ²ÑĞµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹")
                updated += 1
            except Exception as e:
                click.echo(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸: {e}")

        # Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
        click.echo("=" * 80)
        click.echo(f"ğŸ“Š ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ²: {updated}")
        click.echo("ğŸ“‚ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ²: ./artifacts/reports/")

    asyncio.run(_update_summaries())


@cli.command("rebuild-vector-db")
@click.option(
    "--force",
    is_flag=True,
    help="ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
)
@click.option(
    "--keep-reports",
    is_flag=True,
    help="Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ markdown Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹ Ğ¸ JSON ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ChromaDB)",
)
@click.option(
    "--backup",
    is_flag=True,
    help="Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½ÑƒÑ ĞºĞ¾Ğ¿Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ´ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
)
@click.option(
    "--no-progress",
    is_flag=True,
    help="ĞÑ‚ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€ (Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸)",
)
def rebuild_vector_db(force, keep_reports, backup, no_progress):
    """ğŸ”„ ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ChromaDB

    Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞµÑ‘ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾,
    Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ (JSON ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, markdown Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹).

    ĞŸĞ¾Ğ»ĞµĞ·Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ°:
    - Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ°
    - ĞÑƒĞ¶Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑÑ…ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¹
    - ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸

    Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: Ğ­Ñ‚Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ChromaDB!
    """

    async def _rebuild():
        import json
        import shutil
        from pathlib import Path

        click.echo("=" * 80)
        click.echo("ğŸ”„ ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ChromaDB")
        click.echo("=" * 80)
        click.echo()

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²
        reports_dir = Path("artifacts/reports")
        chroma_dir = Path("chroma_db")

        if not reports_dir.exists():
            click.echo("âŒ Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ artifacts/reports Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
            click.echo("ğŸ’¡ Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ: memory_mcp index")
            return

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ JSON ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹
        json_files = list(reports_dir.glob("**/*.json"))
        if not json_files:
            click.echo("âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ JSON Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹")
            click.echo("ğŸ’¡ Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ: memory_mcp index")
            return

        click.echo(f"ğŸ“ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ JSON Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹: {len(json_files)}")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        if chroma_dir.exists():
            try:
                import chromadb

                chroma_client = chromadb.PersistentClient(path=str(chroma_dir))

                # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸ÑÑ…
                collections_info = []
                for collection_name in [
                    "chat_sessions",
                    "chat_messages",
                    "chat_tasks",
                    "session_clusters",
                    "indexing_progress",
                ]:
                    try:
                        collection = chroma_client.get_collection(collection_name)
                        count = collection.count()
                        collections_info.append(
                            f"   - {collection_name}: {count} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹"
                        )
                    except:
                        collections_info.append(f"   - {collection_name}: Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")

                click.echo("ğŸ“Š Ğ¢ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ChromaDB:")
                for info in collections_info:
                    click.echo(info)
                click.echo()

            except Exception as e:
                click.echo(f"âš ï¸  ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğº ChromaDB: {e}")
                click.echo("   Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ°")
                click.echo()

        # ĞŸĞ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ
        if not force:
            click.echo("âš ï¸  Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: Ğ­Ñ‚Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ChromaDB!")
            click.echo("   Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹.")
            click.echo()

            if not click.confirm("ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ?"):
                click.echo("âŒ ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°")
                return

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½ÑƒÑ ĞºĞ¾Ğ¿Ğ¸Ñ ĞµÑĞ»Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑˆĞµĞ½Ğ¾
        if backup and chroma_dir.exists():
            backup_dir = Path(
                f"chroma_db_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            click.echo(f"ğŸ“¦ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¿Ğ¸Ğ¸: {backup_dir}")
            try:
                shutil.copytree(chroma_dir, backup_dir)
                click.echo(f"âœ… Ğ ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¿Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°: {backup_dir}")
            except Exception as e:
                click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¿Ğ¸Ğ¸: {e}")
                if not click.confirm("ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ· Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¿Ğ¸Ğ¸?"):
                    return
            click.echo()

        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        if chroma_dir.exists():
            click.echo("ğŸ—‘ï¸  Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ ChromaDB...")
            try:
                shutil.rmtree(chroma_dir)
                click.echo("âœ… Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ°")
            except Exception as e:
                click.echo(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
                return
            click.echo()

        # ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²
        click.echo("ğŸ”„ ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²...")
        click.echo()

        try:
            # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ‚Ğ¾Ñ€
            from ..core.indexer import TwoLevelIndexer

            click.echo("ğŸ“¦ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ‚Ğ¾Ñ€Ğ°...")
            indexer = TwoLevelIndexer()
            click.echo("âœ… Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ‚Ğ¾Ñ€ Ğ³Ğ¾Ñ‚Ğ¾Ğ²")
            click.echo()

            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            click.echo("ğŸ“š Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹...")

            sessions_data = []
            for json_file in json_files:
                try:
                    with open(json_file, encoding="utf-8") as f:
                        session_data = json.load(f)
                        sessions_data.append(session_data)
                except Exception as e:
                    click.echo(f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ {json_file.name}: {e}")
                    continue

            click.echo(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹: {len(sessions_data)}")
            click.echo()

            if not sessions_data:
                click.echo("âŒ ĞĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹")
                return

            # ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸
            click.echo("ğŸ”„ ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¹ ChromaDB...")

            # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ğ°Ğ¼
            chats_data = {}
            for session in sessions_data:
                chat_name = session.get("meta", {}).get("chat_name", "Unknown")
                if chat_name not in chats_data:
                    chats_data[chat_name] = []
                chats_data[chat_name].append(session)

            click.echo(f"ğŸ“‹ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‡Ğ°Ñ‚Ğ¾Ğ²: {len(chats_data)}")

            # Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµĞ¼ ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑĞµÑÑĞ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€Ğ¾Ğ¼
            total_sessions = len(sessions_data)
            indexed_sessions = 0
            indexed_messages = 0
            indexed_tasks = 0

            # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ tqdm Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€Ğ°
            from tqdm import tqdm

            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€
            show_progress = not no_progress

            if show_progress:
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑĞµÑÑĞ¸Ğ¹
                with tqdm(
                    total=total_sessions,
                    desc="ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹",
                    unit="ÑĞµÑÑĞ¸Ñ",
                ) as pbar:
                    for chat_name, chat_sessions in chats_data.items():
                        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€Ğ°
                        pbar.set_description(f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡Ğ°Ñ‚Ğ°: {chat_name}")

                        for session in chat_sessions:
                            try:
                                # L1: Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸ ÑĞµÑÑĞ¸Ğ¸
                                await indexer._index_session_l1(session)
                                indexed_sessions += 1

                                # L2: Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
                                messages_count = await indexer._index_messages_l2(
                                    session
                                )
                                indexed_messages += messages_count

                                # L3: Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡
                                tasks_count = await indexer._index_tasks(session)
                                indexed_tasks += tasks_count

                            except Exception as e:
                                click.echo(
                                    f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ ÑĞµÑÑĞ¸Ğ¸ {session.get('session_id', 'Unknown')}: {e}"
                                )
                                continue

                            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹
                            pbar.set_postfix(
                                {
                                    "ÑĞµÑÑĞ¸Ğ¹": indexed_sessions,
                                    "ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹": indexed_messages,
                                    "Ğ·Ğ°Ğ´Ğ°Ñ‡": indexed_tasks,
                                }
                            )
                            pbar.update(1)
            else:
                # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ-Ğ±Ğ°Ñ€Ğ°
                for chat_name, chat_sessions in chats_data.items():
                    click.echo(
                        f"ğŸ“ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡Ğ°Ñ‚Ğ°: {chat_name} ({len(chat_sessions)} ÑĞµÑÑĞ¸Ğ¹)"
                    )

                    for session in chat_sessions:
                        try:
                            # L1: Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸ ÑĞµÑÑĞ¸Ğ¸
                            await indexer._index_session_l1(session)
                            indexed_sessions += 1

                            # L2: Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
                            messages_count = await indexer._index_messages_l2(session)
                            indexed_messages += messages_count

                            # L3: Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡
                            tasks_count = await indexer._index_tasks(session)
                            indexed_tasks += tasks_count

                        except Exception as e:
                            click.echo(
                                f"âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ ÑĞµÑÑĞ¸Ğ¸ {session.get('session_id', 'Unknown')}: {e}"
                            )
                            continue

                    click.echo(f"   âœ… ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞµÑÑĞ¸Ğ¹: {len(chat_sessions)}")

            click.echo()
            click.echo("=" * 80)
            click.echo("âœ… Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°!")
            click.echo("=" * 80)
            click.echo()
            click.echo("ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:")
            click.echo(f"   - ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ ÑĞµÑÑĞ¸Ğ¹ (L1): {indexed_sessions}")
            click.echo(f"   - ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ (L2): {indexed_messages}")
            click.echo(f"   - ĞŸĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ (L3): {indexed_tasks}")
            click.echo()
            click.echo("ğŸ“‚ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:")
            click.echo("   - Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ°: ./chroma_db/")
            click.echo("   - ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸: chat_sessions, chat_messages, chat_tasks")
            if keep_reports:
                click.echo("   - Markdown Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² ./artifacts/reports/")
            click.echo()
            click.echo("ğŸ’¡ Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº: memory_mcp search")

        except Exception as e:
            click.echo()
            click.echo("=" * 80)
            click.echo("âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹!")
            click.echo("=" * 80)
            click.echo(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
            click.echo()
            import traceback

            traceback.print_exc()

    asyncio.run(_rebuild())


@cli.command("extract-messages")
@click.option("--dry-run", is_flag=True, help="Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²")
@click.option("--no-date-filter", is_flag=True, help="ĞÑ‚ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ")
@click.option("--chat", help="Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°Ñ‚Ğ°")
@click.option("--input-dir", default="input", help="Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸")
@click.option(
    "--chats-dir", default="chats", help="Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹"
)
def extract_messages(dry_run, no_date_filter, chat, input_dir, chats_dir):
    """ğŸ“¥ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· input Ğ² chats

    Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ input Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² chats,
    Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ Ğ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹.
    """

    async def _extract_messages():
        click.echo("ğŸ“¥ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹...")
        click.echo(f"   Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ: {input_dir}")
        click.echo(f"   Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ: {chats_dir}")
        click.echo(
            f"   Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ: {'âŒ ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½' if no_date_filter else 'âœ… Ğ’ĞºĞ»ÑÑ‡ĞµĞ½'}"
        )
        click.echo(f"   Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ñƒ: {chat or 'Ğ²ÑĞµ Ñ‡Ğ°Ñ‚Ñ‹'}")
        click.echo(f"   Ğ ĞµĞ¶Ğ¸Ğ¼: {'ğŸ”¸ DRY RUN' if dry_run else 'âœ… Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ'}")
        click.echo()

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€
        extractor = MessageExtractor(input_dir=input_dir, chats_dir=chats_dir)

        # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ
        extractor.extract_all_messages(
            dry_run=dry_run, filter_by_date=not no_date_filter, chat_filter=chat
        )

        # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
        extractor.print_stats()

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
        click.echo("=" * 80)

    asyncio.run(_extract_messages())


@cli.command("deduplicate")
@click.option(
    "--chats-dir", default="chats", help="Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"
)
def deduplicate(chats_dir):
    """ğŸ§¹ Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹

    Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ñ 'id' Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‡Ğ°Ñ‚Ğ°Ñ….
    """

    async def _deduplicate():
        click.echo("ğŸ§¹ Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹...")
        click.echo(f"   Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ: {chats_dir}")
        click.echo()

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
        deduplicator = MessageDeduplicator(chats_dir=chats_dir)

        # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
        deduplicator.deduplicate_all_chats()

        # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
        deduplicator.print_stats()

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
        click.echo("=" * 80)

    asyncio.run(_deduplicate())


@cli.command("sync-chromadb")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="ĞŸÑƒÑ‚ÑŒ Ğº SQLite Ğ±Ğ°Ğ·Ğµ Ñ‚Ğ¸Ğ¿Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
)
@click.option(
    "--chroma-path",
    default="chroma_db",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="ĞŸÑƒÑ‚ÑŒ Ğº ChromaDB",
)
@click.option(
    "--chat",
    help="Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Ğ ĞµĞ¶Ğ¸Ğ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹",
)
def sync_chromadb(db_path: Path, chroma_path: Path, chat: Optional[str], dry_run: bool):
    """Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸Ğ· ChromaDB Ğ² Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.
    
    Ğ­Ñ‚Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ¼Ğ¸Ğ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ¸Ğ· ChromaDB ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¹
    (chat_messages, chat_sessions, chat_tasks) Ğ² Ğ³Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ TypedGraphMemory.
    Ğ­Ğ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ.
    """
    import chromadb
    from ..memory.ingest import MemoryIngestor
    from ..indexing import MemoryRecord
    from ..utils.datetime_utils import parse_datetime_utc
    from datetime import datetime, timezone
    
    logger.info("ğŸ”„ ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ChromaDB â†’ Ğ“Ñ€Ğ°Ñ„ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸")
    
    if dry_run:
        logger.info("ğŸ” Ğ ĞµĞ¶Ğ¸Ğ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (dry-run), Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹")
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ°
    graph = TypedGraphMemory(db_path=str(db_path))
    ingestor = MemoryIngestor(graph)
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ChromaDB
    chroma_client = chromadb.PersistentClient(path=str(chroma_path))
    
    total_synced = 0
    total_errors = 0
    
    collections_to_sync = ["chat_messages", "chat_sessions", "chat_tasks"]
    
    for collection_name in collections_to_sync:
        try:
            collection = chroma_client.get_collection(collection_name)
            total_count = collection.count()
            
            if total_count == 0:
                logger.info(f"  ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ {collection_name}: Ğ¿ÑƒÑÑ‚Ğ°, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼")
                continue
            
            logger.info(f"  ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ {collection_name}: {total_count} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸
            offset = 0
            batch_size = 100
            synced_in_collection = 0
            
            while offset < total_count:
                try:
                    result = collection.get(
                        limit=batch_size,
                        offset=offset,
                        include=["documents", "metadatas", "embeddings"]
                    )
                    
                    ids = result.get("ids", [])
                    if not ids:
                        break
                    
                    documents = result.get("documents", [])
                    metadatas = result.get("metadatas", [])
                    embeddings = result.get("embeddings", [])
                    
                    records_to_ingest = []
                    
                    for idx, record_id in enumerate(ids):
                        try:
                            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ¸ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ
                            if record_id in graph.graph:
                                continue
                            
                            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ñƒ, ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½
                            metadata = metadatas[idx] if idx < len(metadatas) else {}
                            if chat and metadata.get("chat") != chat:
                                continue
                            
                            doc = documents[idx] if idx < len(documents) else ""
                            embedding = embeddings[idx] if idx < len(embeddings) else None
                            
                            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ timestamp
                            date_utc = metadata.get("date_utc") or metadata.get("start_time_utc") or metadata.get("end_time_utc")
                            timestamp = None
                            if date_utc:
                                try:
                                    timestamp = parse_datetime_utc(date_utc, use_zoneinfo=True)
                                except Exception:
                                    timestamp = datetime.now(timezone.utc)
                            else:
                                timestamp = datetime.now(timezone.utc)
                            
                            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°
                            author = metadata.get("sender") or metadata.get("author") or metadata.get("username")
                            
                            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚ĞµĞ³Ğ¸ Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸
                            tags = metadata.get("tags", [])
                            if isinstance(tags, str):
                                tags = [tags] if tags else []
                            
                            entities = metadata.get("entities", [])
                            if isinstance(entities, str):
                                entities = [entities] if entities else []
                            
                            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ MemoryRecord
                            record = MemoryRecord(
                                record_id=record_id,
                                source=metadata.get("chat", collection_name.replace("chat_", "")),
                                content=doc,
                                timestamp=timestamp,
                                author=author,
                                tags=tags if isinstance(tags, list) else [],
                                entities=entities if isinstance(entities, list) else [],
                                attachments=[],
                                metadata={
                                    "collection": collection_name,
                                    "chat": metadata.get("chat", ""),
                                    **metadata,
                                },
                            )
                            
                            records_to_ingest.append((record, embedding))
                            
                        except Exception as e:
                            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ {record_id}: {e}")
                            total_errors += 1
                            continue
                    
                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Ğ³Ñ€Ğ°Ñ„
                    if records_to_ingest and not dry_run:
                        try:
                            records_only = [r for r, _ in records_to_ingest]
                            ingestor.ingest(records_only)
                            
                            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸
                            for record, embedding in records_to_ingest:
                                if embedding:
                                    try:
                                        graph.update_node(record.record_id, embedding=embedding)
                                    except Exception as e:
                                        logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ {record.record_id}: {e}")
                            
                            synced_in_collection += len(records_to_ingest)
                            total_synced += len(records_to_ingest)
                            
                        except Exception as e:
                            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ğ³Ñ€Ğ°Ñ„: {e}")
                            total_errors += len(records_to_ingest)
                    elif records_to_ingest and dry_run:
                        synced_in_collection += len(records_to_ingest)
                        total_synced += len(records_to_ingest)
                    
                    offset += len(ids)
                    if len(ids) < batch_size:
                        break
                    
                except Exception as e:
                    logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ°Ñ‚Ñ‡Ğ° (offset={offset}): {e}")
                    total_errors += batch_size
                    offset += batch_size
            
            if synced_in_collection > 0:
                logger.info(f"  âœ… Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ {synced_in_collection} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸Ğ· {collection_name}")
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ {collection_name}: {e}")
            total_errors += 1
    
    if dry_run:
        logger.info(f"ğŸ” Ğ ĞµĞ¶Ğ¸Ğ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ±Ñ‹Ğ»Ğ¾ Ğ±Ñ‹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ {total_synced} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
    else:
        logger.info(f"âœ… Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°: {total_synced} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, {total_errors} Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº")
    
    graph.conn.close()


@cli.command("stop-indexing")
def stop_indexing():
    """ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸

    ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ollama ÑĞµÑ€Ğ²ĞµÑ€.
    """

    async def _stop_indexing():
        click.echo("ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸...")
        click.echo()

        # ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹
        ProcessManager.stop_all_indexing()

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
        click.echo("=" * 80)

    asyncio.run(_stop_indexing())


@cli.command("review-summaries")
@click.option("--dry-run", is_flag=True, help="Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²")
@click.option("--chat", help="ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚")
@click.option("--limit", type=int, help="ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸")
def review_summaries(dry_run, chat, limit):
    """ğŸ” ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ĞµĞ²ÑŒÑ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ñ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ¼ -needs-review

    ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ°Ğ¹Ğ»Ñ‹ *-needs-review.md, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· LLM Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚
    Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ±ĞµĞ· ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ° -needs-review.
    """
    import json

    from ..core.lmstudio_client import LMStudioEmbeddingClient
    from ..config import get_settings

    async def _review_summaries():
        click.echo("ğŸ” ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ĞµĞ²ÑŒÑ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹")
        click.echo()

        if dry_run:
            click.echo("ğŸ”¸ Ğ ĞµĞ¶Ğ¸Ğ¼ DRY RUN - Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ½Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ñ‹")
            click.echo()

        reports_dir = Path("artifacts/reports")

        if not reports_dir.exists():
            click.echo("âŒ Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ artifacts/reports Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
            return

        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ñ -needs-review
        needs_review_files = []
        for md_file in reports_dir.rglob("*-needs-review.md"):
            json_file = md_file.with_suffix(".json")

            file_info = {
                "md_file": md_file,
                "json_file": json_file if json_file.exists() else None,
                "session_id": md_file.stem.replace("-needs-review", ""),
                "chat": md_file.parent.parent.name,
            }

            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ñƒ ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½
            if chat and chat.lower() not in file_info["chat"].lower():
                continue

            needs_review_files.append(file_info)

        # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚
        if limit:
            needs_review_files = needs_review_files[:limit]

        if not needs_review_files:
            click.echo("âœ… ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ ÑÑƒÑ„Ñ„Ğ¸ĞºÑĞ¾Ğ¼ -needs-review")
            return

        click.echo(f"ğŸ“ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {len(needs_review_files)}")
        click.echo()

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ LLM ĞºĞ»Ğ¸ĞµĞ½Ñ‚
        settings = get_settings()
        embedding_client = LMStudioEmbeddingClient(
            model_name=settings.lmstudio_model,
            base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
        )

        async def review_summary(md_content: str) -> dict:
            prompt = f"""Ğ¢Ñ‹ - ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ñ‡Ğ°Ñ‚Ğ¾Ğ².

ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸ ĞµÑ‘, ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾:

{md_content}

Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°:
1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
2. Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
3. Ğ£Ğ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ
4. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ÑŒÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ ÑĞµĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾
5. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ° Ğ¾Ñ‡ĞµĞ²Ğ¸Ğ´Ğ½Ğ° Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°

Ğ’ĞĞ–ĞĞ:
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ markdown (Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸, ÑĞ¿Ğ¸ÑĞºĞ¸, Ğ¸ Ñ‚.Ğ´.)
- ĞĞµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ½ĞµÑ‚ Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğµ
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸ Ğ²ÑĞµ Ğ´Ğ°Ñ‚Ñ‹, Ğ¸Ğ¼ĞµĞ½Ğ° Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸
- Ğ•ÑĞ»Ğ¸ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ - Ğ²ĞµÑ€Ğ½Ğ¸ ĞµÑ‘ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹

Ğ’ĞµÑ€Ğ½Ğ¸ Ğ¢ĞĞ›Ğ¬ĞšĞ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ markdown-Ñ‚ĞµĞºÑÑ‚ Ğ‘Ğ•Ğ— Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ²."""

            try:
                async with embedding_client:
                    improved = await embedding_client.generate_summary(
                        prompt,
                        temperature=0.3,
                        max_tokens=8000,
                    )
                    improved = improved.strip()

                    if improved:

                        # ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ
                        issues_found = []
                        if (
                            "_(ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)_" in md_content
                            or "_(Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚)_" in md_content
                        ):
                            issues_found.append("Ğ•ÑÑ‚ÑŒ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ ÑĞµĞºÑ†Ğ¸Ğ¸")
                        if len(md_content) < 200:
                            issues_found.append("Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")
                        if md_content.count("##") < 2:
                            issues_found.append("ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")

                        improvements = []
                        if md_content != improved:
                            improvements.append("Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸")
                        if len(improved) > len(md_content) * 1.1:
                            improvements.append("Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚")
                        if not improvements:
                            improvements.append("Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ")

                        return {
                            "improved_content": improved,
                            "issues_found": issues_found,
                            "improvements": improvements,
                            "success": True,
                        }
                    else:
                        return {
                            "improved_content": md_content,
                            "issues_found": [],
                            "improvements": [],
                            "success": False,
                            "error": "ĞĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¾Ñ‚ LLM",
                        }

            except Exception as e:
                return {
                    "improved_content": md_content,
                    "issues_found": [],
                    "improvements": [],
                    "success": False,
                    "error": str(e),
                }

        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»

        for file_info in needs_review_files:
            md_file = file_info["md_file"]
            json_file = file_info["json_file"]
            session_id = file_info["session_id"]

            click.echo(f"ğŸ“„ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: {md_file.name}")

            # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ markdown
            try:
                with open(md_file, encoding="utf-8") as f:
                    md_content = f.read()
            except Exception as e:
                click.echo(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ MD: {e}")
                continue

            # ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ñ€ĞµĞ²ÑŒÑ
            click.echo("   ğŸ” ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‡ĞµÑ€ĞµĞ· LLM...")
            review_result = await review_summary(md_content)

            if not review_result["success"]:
                click.echo(
                    f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {review_result.get('error', 'Unknown')}"
                )
                continue

            # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
            if review_result["issues_found"]:
                click.echo(
                    f"   âš ï¸  ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼: {', '.join(review_result['issues_found'])}"
                )

            if review_result["improvements"]:
                click.echo(
                    f"   âœ¨ Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: {', '.join(review_result['improvements'])}"
                )

            if dry_run:
                click.echo("   ğŸ”¸ DRY RUN - Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½Ñ‘Ğ½")
                continue

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ
            new_md_file = md_file.parent / f"{session_id}.md"
            new_json_file = md_file.parent / f"{session_id}.json"

            try:
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ markdown
                with open(new_md_file, "w", encoding="utf-8") as f:
                    f.write(review_result["improved_content"])

                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ JSON ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
                if json_file:
                    try:
                        with open(json_file, encoding="utf-8") as f:
                            session_data = json.load(f)

                        session_data["session_id"] = session_id

                        with open(new_json_file, "w", encoding="utf-8") as f:
                            json.dump(session_data, f, ensure_ascii=False, indent=2)

                        if new_json_file != json_file:
                            json_file.unlink()
                            click.echo(f"   ğŸ—‘ï¸  Ğ£Ğ´Ğ°Ğ»Ñ‘Ğ½ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¹ JSON: {json_file.name}")
                    except Exception as e:
                        click.echo(f"   âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ JSON: {e}")

                if new_md_file != md_file:
                    md_file.unlink()
                    click.echo(f"   ğŸ—‘ï¸  Ğ£Ğ´Ğ°Ğ»Ñ‘Ğ½ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¹ MD: {md_file.name}")

                click.echo(f"   âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»: {new_md_file.name}")

            except Exception as e:
                click.echo(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ: {e}")

            # ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
            await asyncio.sleep(1)

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
        click.echo("=" * 80)

    asyncio.run(_review_summaries())


def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ CLI"""
    cli()


if __name__ == "__main__":
    main()
