#!/usr/bin/env python3
"""CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è Telegram Dump Manager v2.0."""

import asyncio
import json
import logging
import math
import os
import re
import signal
import subprocess
import uuid
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import click

from ..utils.russian_tokenizer import tokenize_text as enhanced_tokenize
from ..utils.paths import find_project_root

# –û—Ç–∫–ª—é—á–∞–µ–º —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é ChromaDB
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY_IMPL"] = ""

from ..analysis.insight_graph import SummaryInsightAnalyzer
from ..analysis.instruction_manager import InstructionManager
from ..core.indexer import TwoLevelIndexer
from ..core.indexing_tracker import IndexingJobTracker
from ..indexing import TelegramIndexer
from ..memory.ingest import MemoryIngestor
from ..memory.typed_graph import TypedGraphMemory
from ..utils.message_extractor import MessageExtractor

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MessageDeduplicator:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –ø–æ–ª—é 'id'."""

    def __init__(self, chats_dir: str = "chats"):
        self.chats_dir = Path(chats_dir)
        self.stats = {
            "total_chats": 0,
            "processed_chats": 0,
            "total_messages": 0,
            "duplicates_removed": 0,
            "unique_messages": 0,
            "errors": 0,
        }

    def deduplicate_chat(self, chat_dir: Path) -> Dict[str, int]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –æ–¥–Ω–æ–º —á–∞—Ç–µ."""
        chat_stats = {
            "total_messages": 0,
            "duplicates_removed": 0,
            "unique_messages": 0,
            "errors": 0,
        }

        if not chat_dir.exists():
            return chat_stats

        all_messages = []
        for json_file in chat_dir.glob("*.json"):
            try:
                with open(json_file, encoding="utf-8") as f:
                    for line in f:
                        try:
                            message = json.loads(line.strip())
                            if isinstance(message, dict):
                                all_messages.append(message)
                        except json.JSONDecodeError:
                            continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {json_file}: {e}")
                chat_stats["errors"] += 1

        chat_stats["total_messages"] = len(all_messages)

        from ..utils.deduplication import deduplicate_by_id

        unique_messages = deduplicate_by_id(all_messages)
        chat_stats["duplicates_removed"] = len(all_messages) - len(unique_messages)
        chat_stats["unique_messages"] = len(unique_messages)

        if unique_messages != all_messages:
            temp_file = chat_dir / "temp_dedup.json"
            try:
                with open(temp_file, "w", encoding="utf-8") as f:
                    for message in unique_messages:
                        f.write(json.dumps(message, ensure_ascii=False) + "\n")

                for json_file in chat_dir.glob("*.json"):
                    if json_file.name != "temp_dedup.json":
                        json_file.unlink()

                final_file = chat_dir / "messages.json"
                temp_file.rename(final_file)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞ –¥–ª—è —á–∞—Ç–∞ {chat_dir.name}: {e}")
                chat_stats["errors"] += 1
                if temp_file.exists():
                    temp_file.unlink()

        return chat_stats

    def deduplicate_all_chats(self) -> Dict[str, int]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤–æ –≤—Å–µ—Ö —á–∞—Ç–∞—Ö."""
        if not self.chats_dir.exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.chats_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return self.stats

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∞—Ç–æ–≤
        chat_dirs = [d for d in self.chats_dir.iterdir() if d.is_dir()]
        self.stats["total_chats"] = len(chat_dirs)

        for chat_dir in chat_dirs:
            logger.info(f"–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–∞—Ç–∞: {chat_dir.name}")

            # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —á–∞—Ç–µ
            chat_stats = self.deduplicate_chat(chat_dir)

            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            for key, value in chat_stats.items():
                self.stats[key] += value

            self.stats["processed_chats"] += 1

            logger.info(
                f"–ß–∞—Ç–∞ {chat_dir.name}: {chat_stats['duplicates_removed']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ"
            )

        return self.stats

    def print_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
        print("\n" + "=" * 60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò")
        print("=" * 60)
        print(f"üìÅ –í—Å–µ–≥–æ —á–∞—Ç–æ–≤: {self.stats['total_chats']}")
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞—Ç–æ–≤: {self.stats['processed_chats']}")
        print(f"üì® –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {self.stats['total_messages']}")
        print(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ: {self.stats['duplicates_removed']}")
        print(f"‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {self.stats['unique_messages']}")
        print(f"‚ùå –û—à–∏–±–æ–∫: {self.stats['errors']}")
        print("=" * 60)


class ProcessManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""

    @staticmethod
    def kill_processes_by_name(pattern: str) -> int:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã –ø–æ –∏–º–µ–Ω–∏."""
        killed_count = 0
        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)
            lines = result.stdout.split("\n")

            for line in lines:
                if pattern in line and "grep" not in line:
                    parts = line.split()
                    if len(parts) >= 2:
                        pid = parts[1]
                        try:
                            os.kill(int(pid), signal.SIGTERM)
                            killed_count += 1
                            logger.info(f"–ü—Ä–æ—Ü–µ—Å—Å {pid} ({pattern}) –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                        except (ValueError, ProcessLookupError):
                            continue
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ {pattern}: {e}")

        return killed_count

    @staticmethod
    def stop_ollama():
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞."""
        logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞...")

        try:
            result = subprocess.run(
                ["ollama", "stop"], capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                logger.info("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            else:
                logger.warning("‚ö†Ô∏è Ollama stop –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º kill")
                ProcessManager.kill_processes_by_name("ollama")
        except subprocess.TimeoutExpired:
            logger.warning("‚ö†Ô∏è Timeout –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ Ollama, –ø—Ä–æ–±—É–µ–º kill")
            ProcessManager.kill_processes_by_name("ollama")
        except FileNotFoundError:
            logger.warning("‚ö†Ô∏è Ollama –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ PATH, –ø—Ä–æ–±—É–µ–º kill")
            ProcessManager.kill_processes_by_name("ollama")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {e}")

    @staticmethod
    def stop_indexing_processes():
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
        logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")

        patterns = [
            "tg_dump.py",
            "index_messages.py",
            "summarize_chats.py",
            "index_summaries.py",
            "cross_analyze.py",
            "ollama",
        ]

        total_killed = 0
        for pattern in patterns:
            killed = ProcessManager.kill_processes_by_name(pattern)
            total_killed += killed

        if total_killed > 0:
            logger.info(f"‚úÖ –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {total_killed}")
        else:
            logger.info("‚ÑπÔ∏è –ü—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    @staticmethod
    def check_remaining_processes():
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤."""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤...")

        patterns = [
            "tg_dump.py",
            "index_messages.py",
            "summarize_chats.py",
            "index_summaries.py",
            "cross_analyze.py",
            "ollama",
        ]

        remaining = []
        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)
            lines = result.stdout.split("\n")

            for line in lines:
                for pattern in patterns:
                    if pattern in line and "grep" not in line:
                        remaining.append(line.strip())
                        break
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {e}")

        if remaining:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(remaining)} –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤:")
            for proc in remaining:
                logger.warning(f"   {proc}")
        else:
            logger.info("‚úÖ –í—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

    @staticmethod
    def stop_all_indexing():
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
        logger.info("üõë –û–°–¢–ê–ù–û–í–ö–ê –í–°–ï–• –ü–†–û–¶–ï–°–°–û–í –ò–ù–î–ï–ö–°–ê–¶–ò–ò")
        logger.info("=" * 50)

        ProcessManager.stop_indexing_processes()
        ProcessManager.stop_ollama()

        import time
        time.sleep(2)

        ProcessManager.check_remaining_processes()

        logger.info("=" * 50)
        logger.info("‚úÖ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


@click.group()
@click.version_option(version="2.0.0", prog_name="memory_mcp")
@click.option("--verbose", "-v", is_flag=True, help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
@click.option("--quiet", "-q", is_flag=True, help="–¢–∏—Ö–∏–π —Ä–µ–∂–∏–º")
def cli(verbose, quiet):
    """üöÄ Telegram Dump Manager v2.0 - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–º–ø–∞–º–∏ Telegram —á–∞—Ç–æ–≤

    –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π CLI –¥–ª—è –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ Telegram —á–∞—Ç–æ–≤.

    –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:
      ‚Ä¢ index              - –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (—Å–µ—Å—Å–∏–∏ + —Å–æ–æ–±—â–µ–Ω–∏—è + –∑–∞–¥–∞—á–∏)
      ‚Ä¢ ingest-telegram    - –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —á–∞—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏
      ‚Ä¢ indexing-progress  - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
      ‚Ä¢ update-summaries   - –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ markdown-–æ—Ç—á–µ—Ç–æ–≤ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
      ‚Ä¢ review-summaries   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–≤—å—é –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
      ‚Ä¢ rebuild-vector-db  - –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
      ‚Ä¢ search             - –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
      ‚Ä¢ insight-graph      - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
      ‚Ä¢ stats              - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã
      ‚Ä¢ check              - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã
      ‚Ä¢ extract-messages   - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ input –≤ chats
      ‚Ä¢ deduplicate        - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π
      ‚Ä¢ stop-indexing      - –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
      
    –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏:
      ‚Ä¢ backup-database    - –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ (SQLite + ChromaDB)
      ‚Ä¢ restore-database   - –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
      ‚Ä¢ optimize-database  - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SQLite (VACUUM, ANALYZE, REINDEX)
      ‚Ä¢ validate-database  - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
      
    –°–∏—Å—Ç–µ–º–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏:
      ‚Ä¢ calculate-importance    - –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏
      ‚Ä¢ prune-memory            - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–µ–≤–∞–∂–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
      ‚Ä¢ update-importance-scores - –ú–∞—Å—Å–æ–≤—ã–π –ø–µ—Ä–µ—Å—á—ë—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏
    """
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    elif quiet:
        logging.getLogger().setLevel(logging.WARNING)
    else:
        logging.getLogger().setLevel(logging.INFO)


@cli.command("ingest-telegram")
@click.option(
    "--chats-dir",
    default="chats",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —ç–∫—Å–ø–æ—Ä—Ç–∞–º–∏ Telegram",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏",
)
@click.option(
    "--chat",
    "selected_chats",
    multiple=True,
    help="–ò–º—è —á–∞—Ç–∞ –¥–ª—è –≤—ã–±–æ—Ä–æ—á–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ)",
)
def ingest_telegram(chats_dir: Path, db_path: Path, selected_chats: tuple[str, ...]):
    """üìö –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π Telegram –Ω–∞–ø—Ä—è–º—É—é –≤ –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏."""

    chosen = [chat for chat in selected_chats if chat] or None
    indexer = TelegramIndexer(chats_dir=str(chats_dir), selected_chats=chosen)
    graph: TypedGraphMemory | None = None

    try:
        indexer.prepare()
        graph = TypedGraphMemory(db_path=str(db_path))
        ingestor = MemoryIngestor(graph)
        ingest_stats = ingestor.ingest(indexer.iter_records())
        index_stats = indexer.finalize()
    except Exception as exc:
        raise click.ClickException(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é: {exc}") from exc
    try:
        indexer.close()
    except Exception:  # pragma: no cover - best effort
        logger.debug("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä", exc_info=True)

    try:
        if graph is not None:
            graph.conn.close()
    except Exception:  # pragma: no cover - best effort
        logger.debug("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –≥—Ä–∞—Ñ–∞", exc_info=True)

    skipped = max(0, index_stats.records_indexed - ingest_stats.records_ingested)

    click.echo("")
    click.echo("üì• –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è Telegram –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    click.echo(f"‚Ä¢ –ß–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {index_stats.sources_processed}")
    click.echo(
        f"‚Ä¢ –ó–∞–ø–∏—Å–µ–π —Å–æ–∑–¥–∞–Ω–æ: {ingest_stats.records_ingested} "
        f"(–≤–ª–æ–∂–µ–Ω–∏—è: {ingest_stats.attachments_ingested})"
    )
    if skipped:
        click.echo(f"‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ –∏–∑-–∑–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {skipped}")
    if index_stats.warnings:
        click.echo("")
        click.echo("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
        for warning in index_stats.warnings:
            click.echo(f"  - {warning}")


@cli.command()
@click.option(
    "--embedding-model", default="text-embedding-qwen3-embedding-0.6b", help="–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
)
def check(embedding_model):
    """üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π"""

    async def _check():
        import chromadb

        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings

        click.echo("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º LM Studio Server
        try:
            settings = get_settings()
            lmstudio_client = LMStudioEmbeddingClient(
                model_name=embedding_model or settings.lmstudio_model,
                llm_model_name=settings.lmstudio_llm_model,
                base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
            )
            async with lmstudio_client:
                available = await lmstudio_client.test_connection()
                if not available or not available.get("lmstudio_available", False):
                    click.echo("‚ùå LM Studio Server –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                    click.echo(f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LM Studio Server –∑–∞–ø—É—â–µ–Ω –Ω–∞ {settings.lmstudio_host}:{settings.lmstudio_port}")
                    return False

                if not available.get("model_available", False):
                    click.echo("‚ùå –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                    click.echo(f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å {embedding_model or settings.lmstudio_model} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ LM Studio Server")
                    return False

                click.echo("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω")
        except Exception as e:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Ollama: {e}")
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collections_status = []
            try:
                sessions_collection = chroma_client.get_collection("chat_sessions")
                click.echo(
                    f"‚úÖ ChromaDB chat_sessions: {sessions_collection.count()} –∑–∞–ø–∏—Å–µ–π"
                )
                collections_status.append(True)
            except:
                click.echo("‚ö†Ô∏è  ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è chat_sessions –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                collections_status.append(False)

            try:
                messages_collection = chroma_client.get_collection("chat_messages")
                click.echo(
                    f"‚úÖ ChromaDB chat_messages: {messages_collection.count()} –∑–∞–ø–∏—Å–µ–π"
                )
                collections_status.append(True)
            except:
                click.echo("‚ö†Ô∏è  ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è chat_messages –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                collections_status.append(False)

            try:
                tasks_collection = chroma_client.get_collection("chat_tasks")
                click.echo(f"‚úÖ ChromaDB chat_tasks: {tasks_collection.count()} –∑–∞–ø–∏—Å–µ–π")
                collections_status.append(True)
            except:
                click.echo("‚ö†Ô∏è  ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è chat_tasks –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                collections_status.append(False)

            if not any(collections_status):
                click.echo(
                    "\nüí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'memory_mcp index' –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤"
                )

        except Exception as e:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ ChromaDB: {e}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã
        chats_path = Path("chats")
        if chats_path.exists():
            json_files = list(chats_path.glob("**/*.json"))
            click.echo(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤: {len(json_files)}")
        else:
            click.echo("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è chats –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        summaries_path = Path("artifacts/reports")
        if summaries_path.exists():
            md_files = list(summaries_path.glob("**/*.md"))
            click.echo(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ MD —Ñ–∞–π–ª–æ–≤: {len(md_files)}")
        else:
            click.echo(
                "‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è artifacts/reports –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)"
            )

        click.echo("\nüéâ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        return True

    asyncio.run(_check())


@cli.command()
@click.option(
    "--scope",
    default="all",
    type=click.Choice(["all", "chat"]),
    help="–û–±–ª–∞—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: all (–≤—Å–µ —á–∞—Ç—ã) –∏–ª–∏ chat (–æ–¥–∏–Ω —á–∞—Ç)",
)
@click.option("--chat", help="–ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–µ—Å–ª–∏ scope=chat)")
@click.option("--force-full", is_flag=True, help="–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞")
@click.option(
    "--recent-days", default=7, type=int, help="–ü–µ—Ä–µ—Å–∞–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π"
)
@click.option(
    "--no-quality-check",
    is_flag=True,
    help="–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–±—ã—Å—Ç—Ä–µ–µ)",
)
@click.option(
    "--no-improvement",
    is_flag=True,
    help="–û—Ç–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏",
)
@click.option(
    "--min-quality", default=90.0, type=float, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞ (0-100)"
)
@click.option(
    "--enable-clustering",
    is_flag=True,
    help="–í–∫–ª—é—á–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é —Å–µ—Å—Å–∏–π –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏",
)
@click.option(
    "--clustering-threshold",
    default=0.8,
    type=float,
    help="–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (0.0-1.0)",
)
@click.option(
    "--min-cluster-size", default=2, type=int, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å–µ—Å—Å–∏–π"
)
@click.option(
    "--max-messages-per-group",
    default=200,
    type=int,
    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–µ (–±–æ–ª—å—à–µ = –º–µ–Ω—å—à–µ —Å–µ—Å—Å–∏–π)",
)
@click.option(
    "--max-session-hours",
    default=12,
    type=int,
    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏ –≤ —á–∞—Å–∞—Ö (–±–æ–ª—å—à–µ = –º–µ–Ω—å—à–µ —Å–µ—Å—Å–∏–π)",
)
@click.option(
    "--gap-minutes",
    default=120,
    type=int,
    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö (–±–æ–ª—å—à–µ = –º–µ–Ω—å—à–µ —Å–µ—Å—Å–∏–π)",
)
@click.option(
    "--enable-smart-aggregation",
    is_flag=True,
    help="–í–∫–ª—é—á–∏—Ç—å —É–º–Ω—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É —Å —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏ (NOW/FRESH/RECENT/OLD)",
)
@click.option(
    "--aggregation-strategy",
    default="smart",
    type=click.Choice(["smart", "channel", "legacy"]),
    help="–°—Ç—Ä–∞—Ç–µ–≥–∏—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: smart (—É–º–Ω–∞—è), channel (–¥–ª—è –∫–∞–Ω–∞–ª–æ–≤), legacy (—Å—Ç–∞—Ä–∞—è)",
)
@click.option(
    "--now-window-hours",
    default=24,
    type=int,
    help="–†–∞–∑–º–µ—Ä NOW –æ–∫–Ω–∞ –≤ —á–∞—Å–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 24)",
)
@click.option(
    "--fresh-window-days",
    default=14,
    type=int,
    help="–†–∞–∑–º–µ—Ä FRESH –æ–∫–Ω–∞ –≤ –¥–Ω—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 14)",
)
@click.option(
    "--recent-window-days",
    default=30,
    type=int,
    help="–†–∞–∑–º–µ—Ä RECENT –æ–∫–Ω–∞ –≤ –¥–Ω—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 30)",
)
@click.option(
    "--strategy-threshold",
    default=1000,
    type=int,
    help="–ü–æ—Ä–æ–≥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1000)",
)
@click.option(
    "--force",
    is_flag=True,
    help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã",
)
@click.option(
    "--embedding-model", 
    default="text-embedding-qwen3-embedding-0.6b", 
    help="–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
)
def index(
    scope,
    chat,
    force_full,
    recent_days,
    no_quality_check,
    no_improvement,
    min_quality,
    enable_clustering,
    clustering_threshold,
    min_cluster_size,
    max_messages_per_group,
    max_session_hours,
    gap_minutes,
    enable_smart_aggregation,
    aggregation_strategy,
    now_window_hours,
    fresh_window_days,
    recent_window_days,
    strategy_threshold,
    force,
    embedding_model,
):
    """üìö –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞—Ç–æ–≤ (L1: —Å–µ—Å—Å–∏–∏ + —Å–∞–º–º–∞—Ä–∏, L2: —Å–æ–æ–±—â–µ–Ω–∏—è, L3: –∑–∞–¥–∞—á–∏)

    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å —É–º–Ω–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Å–µ—Å—Å–∏–∏,
    –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –∑–∞–¥–∞—á, —Å–æ–∑–¥–∞–Ω–∏–µ–º Markdown –æ—Ç—á—ë—Ç–æ–≤.
    """

    async def _index():
        click.echo("=" * 80)
        click.echo("üöÄ Telegram Dump Manager - –î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è v2.0")
        click.echo("=" * 80)
        click.echo()

        if scope == "chat" and not chat:
            click.echo("‚ùå –î–ª—è scope='chat' –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --chat")
            return

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–∫–µ—Ä–∞ –∑–∞–¥–∞—á –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings
        
        settings = get_settings()
        tracker = IndexingJobTracker(storage_path="data/indexing_jobs.json")
        
        # –°–æ–∑–¥–∞–µ–º job_id –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        job_id = f"cli_{uuid.uuid4().hex[:12]}"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —á–∞—Ç–æ–≤ –¥–ª—è scope="all"
        chats_list = None
        if scope == "all":
            chats_path = Path("chats")
            if chats_path.exists():
                chats_list = [d.name for d in chats_path.iterdir() if d.is_dir()]
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ —Ç—Ä–µ–∫–µ—Ä–µ
        tracker.create_job(
            job_id=job_id,
            scope=scope,
            chat=chat,
            chats=chats_list,
            force_full=force_full,
            recent_days=recent_days,
        )
        
        click.echo(f"üìã –ó–∞–¥–∞—á–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∞: {job_id}")
        click.echo("üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞...")
        embedding_client = LMStudioEmbeddingClient(
            model_name=embedding_model or settings.lmstudio_model,
            llm_model_name=settings.lmstudio_llm_model,
            base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º callback —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        def progress_callback(job_id: str, event: str, data: Dict) -> None:
            """Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
            try:
                if event == "chat_started":
                    tracker.update_job(
                        job_id=job_id,
                        status="running",
                        current_stage=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç–∞ '{data.get('chat')}'",
                        current_chat=data.get("chat"),
                        progress={
                            "completed_chats": data.get("chat_index", 1) - 1,
                        },
                    )
                elif event == "sessions_processing":
                    tracker.update_job(
                        job_id=job_id,
                        current_stage=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ—Å—Å–∏–π —á–∞—Ç–∞ '{data.get('chat')}' ({data.get('session_index')}/{data.get('total_sessions')})",
                        current_chat=data.get("chat"),
                        progress={
                            "current_chat_sessions": data.get("sessions_count", 0),
                            "current_chat_messages": data.get("messages_count", 0),
                        },
                    )
                elif event == "chat_completed":
                    chat_stats = data.get("stats", {})
                    tracker.update_job(
                        job_id=job_id,
                        current_stage=f"–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç–∞ '{data.get('chat')}'",
                        progress={
                            "completed_chats": data.get("chat_index", 0),
                        },
                        stats={
                            "sessions_indexed": chat_stats.get("sessions_indexed", 0),
                            "messages_indexed": chat_stats.get("messages_indexed", 0),
                            "tasks_indexed": chat_stats.get("tasks_indexed", 0),
                        },
                    )
                elif event == "error":
                    tracker.update_job(
                        job_id=job_id,
                        status="failed",
                        error=f"–û—à–∏–±–∫–∞ –≤ —á–∞—Ç–µ '{data.get('chat')}': {data.get('error')}",
                    )
                elif event == "completed":
                    final_stats = data.get("stats", {})
                    tracker.update_job(
                        job_id=job_id,
                        status="completed",
                        current_stage="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                        stats={
                            "sessions_indexed": final_stats.get("sessions_indexed", 0),
                            "messages_indexed": final_stats.get("messages_indexed", 0),
                            "tasks_indexed": final_stats.get("tasks_indexed", 0),
                        },
                    )
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞–ø–∏—Å–µ–π
        from ..memory.typed_graph import TypedGraphMemory
        db_path = settings.db_path
        if not Path(db_path).is_absolute():
            # –ò—â–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ pyproject.toml
            project_root = find_project_root(Path(__file__).parent)
            db_path = str(project_root / db_path)
        db_path_obj = Path(db_path)
        db_path_obj.parent.mkdir(parents=True, exist_ok=True)
        graph = TypedGraphMemory(db_path=str(db_path))
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏: {db_path}")
        
        indexer = TwoLevelIndexer(
            artifacts_path=settings.artifacts_path,
            embedding_client=embedding_client,
            enable_quality_check=not no_quality_check,
            enable_iterative_refinement=not no_improvement,
            min_quality_score=min_quality,
            enable_clustering=enable_clustering,
            clustering_threshold=clustering_threshold,
            min_cluster_size=min_cluster_size,
            max_messages_per_group=max_messages_per_group,
            max_session_hours=max_session_hours,
            gap_minutes=gap_minutes,
            enable_smart_aggregation=enable_smart_aggregation,
            aggregation_strategy=aggregation_strategy,
            now_window_hours=now_window_hours,
            fresh_window_days=fresh_window_days,
            recent_window_days=recent_window_days,
            strategy_threshold=strategy_threshold,
            force=force,
            graph=graph,  # –ü–µ—Ä–µ–¥–∞–µ–º –≥—Ä–∞—Ñ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞–ø–∏—Å–µ–π
            progress_callback=progress_callback,
        )
        click.echo("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤")
        click.echo()

        click.echo("‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:")
        click.echo(f"   - Scope: {scope}")
        click.echo(f"   - Chat: {chat or '–≤—Å–µ —á–∞—Ç—ã'}")
        click.echo(f"   - Force full rebuild: {force_full}")
        click.echo(f"   - Force artifacts: {force}")
        click.echo(f"   - Recent days resummary: {recent_days}")
        click.echo()
        click.echo("üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:")
        click.echo(
            f"   - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {'‚ùå –û—Ç–∫–ª—é—á–µ–Ω–∞' if no_quality_check else '‚úÖ –í–∫–ª—é—á–µ–Ω–∞'}"
        )
        click.echo(
            f"   - –ê–≤—Ç–æ—É–ª—É—á—à–µ–Ω–∏–µ: {'‚ùå –û—Ç–∫–ª—é—á–µ–Ω–æ' if no_improvement else '‚úÖ –í–∫–ª—é—á–µ–Ω–æ'}"
        )
        click.echo(
            f"   - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª: {min_quality}/100 {'(—Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º)' if min_quality >= 80 else '(—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º)' if min_quality >= 60 else '(–º—è–≥–∫–∏–π —Ä–µ–∂–∏–º)'}"
        )
        click.echo()
        click.echo("üîó –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–π:")
        click.echo(
            f"   - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {'‚úÖ –í–∫–ª—é—á–µ–Ω–∞' if enable_clustering else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω–∞'}"
        )
        if enable_clustering:
            click.echo(f"   - –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞: {clustering_threshold}")
            click.echo(f"   - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {min_cluster_size}")
        click.echo()
        click.echo("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–µ—Å—Å–∏–π:")
        click.echo(f"   - –ú–∞–∫—Å–∏–º—É–º —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–µ: {max_messages_per_group}")
        click.echo(f"   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏: {max_session_hours} —á–∞—Å–æ–≤")
        click.echo(f"   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏: {gap_minutes} –º–∏–Ω—É—Ç")
        click.echo()

        if enable_smart_aggregation:
            click.echo("üß† –£–º–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏:")
            click.echo(f"   - –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {aggregation_strategy}")
            click.echo(f"   - NOW –æ–∫–Ω–æ: {now_window_hours} —á–∞—Å–æ–≤ (—Å–µ–≥–æ–¥–Ω—è)")
            click.echo(f"   - FRESH –æ–∫–Ω–æ: {fresh_window_days} –¥–Ω–µ–π (–¥–µ—Ç–∞–ª—å–Ω–æ)")
            click.echo(f"   - RECENT –æ–∫–Ω–æ: {recent_window_days} –¥–Ω–µ–π (–ø–æ –Ω–µ–¥–µ–ª—è–º)")
            click.echo(f"   - OLD –æ–∫–Ω–æ: >{recent_window_days} –¥–Ω–µ–π (–ø–æ –º–µ—Å—è—Ü–∞–º)")
            click.echo(f"   - –ü–æ—Ä–æ–≥ –ø–µ—Ä–µ—Ö–æ–¥–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π: {strategy_threshold} —Å–æ–æ–±—â–µ–Ω–∏–π")
            click.echo("   - –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è NOW –æ–∫–Ω–∞")
            click.echo("   - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Ollama")
        else:
            click.echo("üìä –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏:")
            click.echo("   - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–Ω—è–º")
            click.echo("   - 10-100 —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–µ")
            click.echo("   - –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑—Ä—ã–≤—ã –≤ –æ–±—Å—É–∂–¥–µ–Ω–∏—è—Ö (>4 —á–∞—Å–æ–≤)")
            click.echo("   - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö –∏ —Å–µ—Ä–≤–∏—Å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
            click.echo("   - –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
            click.echo("   - –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞–ª–µ–Ω—å–∫–∏—Ö –≥—Ä—É–ø–ø")
        click.echo()

        click.echo("üîÑ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
        click.echo()

        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –Ω–∞ "running"
            tracker.update_job(job_id=job_id, status="running", current_stage="–ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            
            stats = await indexer.build_index(
                scope=scope, chat=chat, force_full=force_full, recent_days=recent_days, job_id=job_id
            )

            click.echo()
            click.echo("=" * 80)
            click.echo("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            click.echo("=" * 80)
            click.echo()
            click.echo("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            click.echo(f"   - –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ —á–∞—Ç–æ–≤: {len(stats['indexed_chats'])}")
            click.echo(f"   - –°–µ—Å—Å–∏–π (L1): {stats['sessions_indexed']}")
            click.echo(f"   - –°–æ–æ–±—â–µ–Ω–∏–π (L2): {stats['messages_indexed']}")
            click.echo(f"   - –ó–∞–¥–∞—á (L3): {stats['tasks_indexed']}")
            click.echo()

            if stats["indexed_chats"]:
                click.echo("üìÅ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Ç—ã:")
                for chat_name in stats["indexed_chats"]:
                    click.echo(f"   - {chat_name}")
                click.echo()

            click.echo("üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
            click.echo("   - Markdown –æ—Ç—á—ë—Ç—ã: ./artifacts/reports/")
            click.echo("   - –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞: ./chroma_db/")
            click.echo("   - –ö–æ–ª–ª–µ–∫—Ü–∏–∏: chat_sessions, chat_messages, chat_tasks")
            click.echo()

        except Exception as e:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –Ω–∞ "failed"
            tracker.update_job(
                job_id=job_id,
                status="failed",
                error=str(e),
            )
            
            click.echo()
            click.echo("=" * 80)
            click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏!")
            click.echo("=" * 80)
            click.echo(f"–û—à–∏–±–∫–∞: {e}")
            click.echo()
            import traceback

            traceback.print_exc()

    asyncio.run(_index())


@cli.command("set-instruction")
@click.option(
    "--chat", help="–ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞ (–∫–∞–∫ –ø–∞–ø–∫–∞ –≤ chats/) –¥–ª—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"
)
@click.option(
    "--mode",
    type=click.Choice(["group", "channel"]),
    help="–û–±—â–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –≤—Å–µ—Ö —á–∞—Ç–æ–≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞",
)
@click.option("--text", help="–¢–µ–∫—Å—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø—Ä—è–º–æ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–µ")
@click.option(
    "--file",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π",
)
@click.option(
    "--clear",
    is_flag=True,
    help="–£–¥–∞–ª–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —á–∞—Ç–∞ –∏–ª–∏ —Ç–∏–ø–∞",
)
def set_instruction(chat, mode, text, file, clear):
    """üìù –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–ª–∏ —É–¥–∞–ª–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
    target_count = sum(1 for value in (chat, mode) if value)
    if target_count != 1:
        raise click.UsageError(
            "–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: --chat –∏–ª–∏ --mode"
        )

    manager = InstructionManager()

    if clear:
        if chat:
            manager.clear_chat_instruction(chat)
            click.echo(f"üóëÔ∏è –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è '{chat}' —É–¥–∞–ª–µ–Ω–∞")
        else:
            manager.clear_mode_instruction(mode)
            click.echo(f"üóëÔ∏è –û–±—â–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è —Ç–∏–ø–∞ '{mode}' –æ—á–∏—â–µ–Ω–∞")
        return

    instruction_text = text or ""
    if file:
        instruction_text = file.read_text(encoding="utf-8")
    if not instruction_text.strip():
        raise click.UsageError(
            "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ --text –∏–ª–∏ --file (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --clear)."
        )

    if chat:
        manager.set_chat_instruction(chat, instruction_text)
        click.echo(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è —á–∞—Ç–∞ '{chat}'")
    else:
        manager.set_mode_instruction(mode, instruction_text)
        click.echo(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –æ–±—â–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è —Ç–∏–ø–∞ '{mode}'")


@cli.command("list-instructions")
def list_instructions():
    """üìã –ü–æ–∫–∞–∑–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
    manager = InstructionManager()
    data = manager.export()

    click.echo("üìå –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —á–∞—Ç–∞–º:")
    if data["chats"]:
        for name, instruction in sorted(data["chats"].items()):
            preview = instruction.strip().replace("\n", " ")
            if len(preview) > 120:
                preview = preview[:117] + "..."
            click.echo(f"  ‚Ä¢ {name}: {preview}")
    else:
        click.echo("  (–ù–µ—Ç –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π)")

    click.echo("\nüìå –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Ç–∏–ø–∞–º —á–∞—Ç–æ–≤:")
    for mode in ("group", "channel"):
        instruction = data["modes"].get(mode, "").strip()
        if instruction:
            preview = instruction.replace("\n", " ")
            if len(preview) > 120:
                preview = preview[:117] + "..."
            click.echo(f"  ‚Ä¢ {mode}: {preview}")
        else:
            click.echo(f"  ‚Ä¢ {mode}: (–Ω–µ –∑–∞–¥–∞–Ω–æ)")


def highlight_text(text: str, query: str) -> str:
    """–ü–æ–¥—Å–≤–µ—Ç–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
    keywords = [
        word.strip().lower() for word in query.split() if len(word.strip()) >= 3
    ]

    if not keywords:
        return text

    result = text
    for keyword in keywords:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        result = pattern.sub(
            lambda m: click.style(m.group(0), fg="yellow", bold=True), result
        )

    return result


TOKEN_PATTERN = re.compile(r"\w+", re.UNICODE)
MIN_TOKEN_LENGTH = 3

HYBRID_WEIGHTS = {
    "messages": (0.65, 0.35),
    "sessions": (0.6, 0.4),
    "tasks": (0.6, 0.4),
}

RELEVANCE_THRESHOLDS = {
    "messages": 0.32,
    "sessions": 0.30,
    "tasks": 0.28,
}


def _tokenize(text: str) -> list[str]:
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é."""
    if not text:
        return []

    try:
        return enhanced_tokenize(text)
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback: {e}")
        return [
            token
            for token in TOKEN_PATTERN.findall(text.lower())
            if len(token) >= MIN_TOKEN_LENGTH
        ]


def _bm25_scores(
    query_tokens: list[str], documents_tokens: list[list[str]]
) -> list[float]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç BM25 –¥–ª—è –∫–æ—Ä–ø—É—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    if not query_tokens or not documents_tokens:
        return [0.0] * len(documents_tokens)

    num_docs = len(documents_tokens)
    doc_freq = Counter()
    doc_lengths = []
    for tokens in documents_tokens:
        unique_tokens = set(tokens)
        if unique_tokens:
            doc_freq.update(unique_tokens)
        doc_lengths.append(len(tokens))

    avgdl = sum(doc_lengths) / num_docs if num_docs else 0
    if avgdl == 0:
        return [0.0] * len(documents_tokens)

    idf = {}
    for token, freq in doc_freq.items():
        idf[token] = math.log(((num_docs - freq + 0.5) / (freq + 0.5)) + 1.0)

    scores = []
    k1, b = 1.5, 0.75  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã BM25
    for tokens, doc_len in zip(documents_tokens, doc_lengths):
        if not tokens:
            scores.append(0.0)
            continue

        term_freq = Counter(tokens)
        score = 0.0
        for token in query_tokens:
            token_idf = idf.get(token)
            tf = term_freq.get(token)
            if not token_idf or not tf:
                continue
            denom = tf + k1 * (1 - b + b * (doc_len / avgdl))
            score += token_idf * (tf * (k1 + 1) / denom)

        scores.append(score)

    return scores


@cli.command()
@click.argument("query")
@click.option("--limit", "-l", default=10, help="–õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
@click.option(
    "--collection",
    "-c",
    type=click.Choice(["messages", "sessions", "tasks"]),
    default="messages",
    help="–ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞",
)
@click.option("--chat", help="–§–∏–ª—å—Ç—Ä –ø–æ —á–∞—Ç—É (–Ω–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞)")
@click.option(
    "--highlight/--no-highlight", default=True, help="–ü–æ–¥—Å–≤–µ—Ç–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"
)
@click.option(
    "--embedding-model", 
    default="text-embedding-qwen3-embedding-0.6b", 
    help="–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
)
def search(query, limit, collection, chat, highlight, embedding_model):
    """üîç –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º

    –ü–æ–∏—Å–∫ –ø–æ —Ç—Ä—ë–º —É—Ä–æ–≤–Ω—è–º:
    - messages: –ü–æ–∏—Å–∫ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º
    - sessions: –ü–æ–∏—Å–∫ –ø–æ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è–º —Å–µ—Å—Å–∏–π
    - tasks: –ü–æ–∏—Å–∫ –ø–æ –∑–∞–¥–∞—á–∞–º (Action Items)
    """

    async def _search():
        import chromadb

        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings

        click.echo(f"üîç –ü–æ–∏—Å–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection}': '{query}'")
        if chat:
            click.echo(f"üìã –§–∏–ª—å—Ç—Ä –ø–æ —á–∞—Ç—É: '{chat}'")

        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")
            settings = get_settings()
            embedding_client = LMStudioEmbeddingClient(
                model_name=embedding_model or settings.lmstudio_model,
                base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
            )

            collection_name = f"chat_{collection}"
            try:
                coll = chroma_client.get_collection(collection_name)
            except:
                click.echo(f"‚ùå –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                click.echo("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'memory_mcp index' –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤")
                return

            async with embedding_client:
                query_embedding = await embedding_client._generate_single_embedding(query)

                if not query_embedding:
                    click.echo("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
                    return

                where_filter = {"chat": chat} if chat else None
                vector_limit = max(limit * 4, 20)
                results = coll.query(
                    query_embeddings=[query_embedding],
                    n_results=vector_limit,
                    where=where_filter,
                )

                documents = results.get("documents")
                if not documents or not documents[0]:
                    click.echo("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    return

                raw_ids = results.get("ids") or [[]]
                raw_ids = raw_ids[0] if raw_ids else []
                metadatas = results.get("metadatas", [[]])[0]
                distances = results.get("distances", [[]])[0]

                def resolve_doc_id(raw_id, metadata, doc_text):
                    if raw_id:
                        return raw_id
                    metadata = metadata or {}
                    for key in ("msg_id", "session_id", "task_id", "id"):
                        value = metadata.get(key)
                        if value:
                            return value
                    return f"doc-{abs(hash((doc_text or '')[:80]))}"

                vector_scores: dict[str, float] = {}
                vector_distances: dict[str, float] = {}
                vector_candidates = []

                for doc, metadata, distance, raw_id in zip(
                    documents[0], metadatas, distances, raw_ids
                ):
                    if not doc:
                        continue
                    doc_id = resolve_doc_id(raw_id, metadata, doc)
                    vector_candidates.append(
                        {
                            "id": doc_id,
                            "doc": doc,
                            "metadata": metadata or {},
                            "distance": distance,
                        }
                    )
                    vector_distances[doc_id] = distance

                if not vector_candidates:
                    click.echo("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    return

                available_distances = [
                    item["distance"]
                    for item in vector_candidates
                    if item.get("distance") is not None
                ]
                if available_distances:
                    min_distance = min(available_distances)
                    max_distance = max(available_distances)
                    denominator = max(max_distance - min_distance, 1e-6)
                    for item in vector_candidates:
                        doc_id = item["id"]
                        distance = item.get("distance")
                        if distance is None:
                            continue
                        if max_distance == min_distance:
                            vector_scores[doc_id] = 1.0
                        else:
                            score = (max_distance - distance) / denominator
                            vector_scores[doc_id] = max(score, 0.0)

                get_kwargs = {"include": ["documents", "metadatas"]}
                if where_filter:
                    get_kwargs["where"] = where_filter

                corpus = coll.get(**get_kwargs)
                corpus_docs = corpus.get("documents", [])
                corpus_meta = corpus.get("metadatas", [])

                doc_store: dict[str, dict[str, object]] = {}
                lexical_entries: list[str] = []
                lexical_tokens: list[list[str]] = []

                for idx, (doc_text, metadata) in enumerate(
                    zip(corpus_docs, corpus_meta)
                ):
                    raw_id = f"doc_{idx}_{hash(doc_text or '')}"
                    resolved_id = resolve_doc_id(raw_id, metadata, doc_text)
                    doc_store[resolved_id] = {
                        "doc": doc_text or "",
                        "metadata": metadata or {},
                    }
                    lexical_entries.append(resolved_id)
                    lexical_tokens.append(_tokenize(doc_text or ""))

                query_tokens = _tokenize(query)
                lexical_scores_list = _bm25_scores(query_tokens, lexical_tokens)
                lexical_scores = dict(zip(lexical_entries, lexical_scores_list))
                max_lexical_score = (
                    max(lexical_scores.values()) if lexical_scores else 0.0
                )
                lexical_norm = {
                    doc_id: (score / max_lexical_score)
                    if max_lexical_score > 0
                    else 0.0
                    for doc_id, score in lexical_scores.items()
                }

                weight_vector, weight_lexical = HYBRID_WEIGHTS.get(
                    collection, (0.6, 0.4)
                )
                if not query_tokens or max_lexical_score == 0:
                    weight_vector, weight_lexical = 1.0, 0.0
                weight_sum = weight_vector + weight_lexical
                if weight_sum == 0:
                    weight_vector, weight_lexical = 1.0, 0.0
                else:
                    weight_vector /= weight_sum
                    weight_lexical /= weight_sum

                candidate_ids = set(vector_scores.keys())
                if lexical_scores:
                    sorted_lexical = sorted(
                        lexical_scores.items(), key=lambda item: item[1], reverse=True
                    )
                    top_lexical = [
                        doc_id for doc_id, score in sorted_lexical if score > 0
                    ][: max(limit * 3, 15)]
                    candidate_ids.update(top_lexical)

                final_candidates = []
                for doc_id in candidate_ids:
                    payload = doc_store.get(doc_id)
                    if not payload:
                        continue
                    vector_component = vector_scores.get(doc_id, 0.0)
                    lexical_component = lexical_norm.get(doc_id, 0.0)
                    hybrid_score = (
                        vector_component * weight_vector
                        + lexical_component * weight_lexical
                    )
                    final_candidates.append(
                        {
                            "id": doc_id,
                            "doc": payload["doc"],
                            "metadata": payload["metadata"],
                            "score": hybrid_score,
                            "vector_component": vector_component,
                            "lexical_component": lexical_component,
                            "vector_distance": vector_distances.get(doc_id),
                        }
                    )

                if not final_candidates:
                    click.echo("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    return

                final_candidates.sort(key=lambda item: item["score"], reverse=True)

                threshold = RELEVANCE_THRESHOLDS.get(collection, 0.0)
                filtered_candidates = [
                    candidate
                    for candidate in final_candidates
                    if candidate["score"] >= threshold
                ]
                filtered_out = len(final_candidates) - len(filtered_candidates)

                if not filtered_candidates:
                    filtered_candidates = final_candidates[:limit]
                    filtered_out = 0
                else:
                    filtered_candidates = filtered_candidates[:limit]

                click.echo(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(filtered_candidates)}")
                if filtered_out > 0:
                    click.echo(f"   (–æ—Ç—Å–µ—á–µ–Ω–æ –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {filtered_out})")
                click.echo()

                for index, candidate in enumerate(filtered_candidates, 1):
                    metadata = candidate.get("metadata") or {}
                    chat_name = metadata.get(
                        "chat", metadata.get("chat_name", "Unknown")
                    )
                    signal_parts = []
                    if candidate.get("vector_component", 0) > 0:
                        signal_parts.append("vec")
                    if candidate.get("lexical_component", 0) > 0:
                        signal_parts.append("lex")
                    signals = "+".join(signal_parts) if signal_parts else "-"
                    header = f"{index}. {chat_name} (score: {candidate['score'] * 100:.1f} | signals: {signals}"
                    distance = candidate.get("vector_distance")
                    if distance is not None:
                        header += f" | distance: {distance:.1f}"
                    header += ")"
                    click.echo(header)

                    doc_text = candidate.get("doc") or ""

                    if collection == "messages":
                        text = (
                            doc_text[:200] + "..." if len(doc_text) > 200 else doc_text
                        )
                        if highlight:
                            text = highlight_text(text, query)
                        click.echo(f"   {text}")
                    elif collection == "sessions":
                        session_id = metadata.get("session_id", "N/A")
                        time_range = metadata.get("time_span", "N/A")
                        click.echo(f"   Session: {session_id}")
                        click.echo(f"   Time: {time_range}")
                        summary = (
                            doc_text[:150] + "..." if len(doc_text) > 150 else doc_text
                        )
                        if highlight:
                            summary = highlight_text(summary, query)
                        click.echo(f"   Summary: {summary}")
                    elif collection == "tasks":
                        task_text = (
                            doc_text[:200] + "..." if len(doc_text) > 200 else doc_text
                        )
                        if highlight:
                            task_text = highlight_text(task_text, query)
                        owner = metadata.get("owner", "N/A")
                        due_date = metadata.get("due", "N/A")
                        priority = metadata.get("priority", "N/A")
                        click.echo(f"   Task: {task_text}")
                        click.echo(
                            f"   Owner: {owner} | Due: {due_date} | Priority: {priority}"
                        )

                    click.echo()

        except Exception as e:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(_search())


@cli.command()
@click.option(
    "--threshold", default=0.76, type=float, help="–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É —á–∞—Ç–∞–º–∏"
)
@click.option("--graphml", type=click.Path(), help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è GraphML-—Ñ–∞–π–ª–∞")
def insight_graph(threshold, graphml):
    """üß† –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π

    –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π, –≤—ã–¥–µ–ª—è—è –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
    –∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —á–∞—Ç–∞–º–∏.
    """

    async def _run():
        click.echo("üß† –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∏–Ω—Å–∞–π—Ç–æ–≤...")
        click.echo(f"   –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {threshold}")
        click.echo()

        analyzer = SummaryInsightAnalyzer(
            summaries_dir=Path("artifacts/reports"),
            similarity_threshold=threshold,
        )

        try:
            # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ
            async with analyzer:
                result = await analyzer.analyze()

            # –í—ã–≤–æ–¥–∏–º –æ—Ç—á—ë—Ç
            click.echo("\n" + "=" * 80)
            click.echo("‚úÖ –ì—Ä–∞—Ñ –∏–Ω—Å–∞–π—Ç–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω!")
            click.echo("=" * 80)
            click.echo()

            graph_metrics = result.metrics.get("graph", {})
            click.echo("üìä –ú–µ—Ç—Ä–∏–∫–∏ –≥—Ä–∞—Ñ–∞:")
            click.echo(f"   - –£–∑–ª–æ–≤ (—á–∞—Ç–æ–≤): {graph_metrics.get('nodes', 0)}")
            click.echo(f"   - –†—ë–±–µ—Ä (—Å–≤—è–∑–µ–π): {graph_metrics.get('edges', 0)}")
            click.echo(f"   - –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {graph_metrics.get('components', 0)}")
            click.echo(f"   - –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∞: {graph_metrics.get('density', 0.0):.3f}")
            click.echo()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
            report_path = Path("insight_graph_report.md")
            report_content = analyzer.generate_report(result)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)
            click.echo(f"üìÑ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_path}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º GraphML –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
            if graphml:
                export_path = analyzer.export_graphml(result, Path(graphml))
                if export_path:
                    click.echo(f"üìÅ GraphML —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {export_path}")

        except Exception as e:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∞: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(_run())


@cli.command()
def stats():
    """üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""

    async def _stats():
        import chromadb

        click.echo("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã...")
        click.echo()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º
            total_records = 0
            for coll_name in ["chat_sessions", "chat_messages", "chat_tasks"]:
                try:
                    coll = chroma_client.get_collection(coll_name)
                    count = coll.count()
                    total_records += count
                    icon = "‚úÖ" if count > 0 else "‚ö†Ô∏è "
                    click.echo(f"{icon} {coll_name}: {count} –∑–∞–ø–∏—Å–µ–π")
                except:
                    click.echo(f"‚ùå {coll_name}: –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

            click.echo()
            click.echo(f"üì¶ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –∏–Ω–¥–µ–∫—Å–∞—Ö: {total_records}")

        except Exception as e:
            click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ ChromaDB: {e}")

        click.echo()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–∞–π–ª–∞–º
        chats_path = Path("chats")
        if chats_path.exists():
            json_files = list(chats_path.glob("**/*.json"))
            click.echo(f"üìÅ JSON —Ñ–∞–π–ª–æ–≤: {len(json_files)}")

            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Ç–æ–≤
            chat_dirs = [d for d in chats_path.iterdir() if d.is_dir()]
            click.echo(f"üí¨ –ß–∞—Ç–æ–≤: {len(chat_dirs)}")
        else:
            click.echo("üìÅ JSON —Ñ–∞–π–ª–æ–≤: 0")

        # Markdown —Ñ–∞–π–ª—ã
        summaries_path = Path("artifacts/reports")
        if summaries_path.exists():
            md_files = list(summaries_path.glob("**/*.md"))
            session_files = list(summaries_path.glob("**/sessions/*.md"))
            click.echo(f"üìÑ MD —Ñ–∞–π–ª–æ–≤: {len(md_files)}")
            click.echo(f"üìù –°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π —Å–µ—Å—Å–∏–π: {len(session_files)}")
        else:
            click.echo("üìÑ MD —Ñ–∞–π–ª–æ–≤: 0")

    asyncio.run(_stats())


@cli.command("indexing-progress")
@click.option("--chat", help="–ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞")
@click.option(
    "--reset",
    is_flag=True,
    help="–°–±—Ä–æ—Å–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)",
)
def indexing_progress(chat, reset):
    """üîÑ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ —á–∞—Ç–∞
    –∏–ª–∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.
    """

    import chromadb

    try:
        chroma_client = chromadb.PersistentClient(path="./chroma_db")

        try:
            progress_collection = chroma_client.get_collection("indexing_progress")
        except:
            click.echo("‚ö†Ô∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è indexing_progress –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            click.echo("üí° –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –µ—â—ë –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª–∞—Å—å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è")
            return

        if reset:
            if chat:
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞
                from ..utils.naming import slugify

                progress_id = f"progress_{slugify(chat)}"
                try:
                    progress_collection.delete(ids=[progress_id])
                    click.echo(f"‚úÖ –ü—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —á–∞—Ç–∞ '{chat}' —Å–±—Ä–æ—à–µ–Ω")
                    click.echo(
                        "üí° –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—É—Å–∫–µ —á–∞—Ç –±—É–¥–µ—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –∑–∞–Ω–æ–≤–æ"
                    )
                except Exception as e:
                    click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±—Ä–æ—Å–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
            else:
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤–µ—Å—å –ø—Ä–æ–≥—Ä–µ—Å—Å
                try:
                    result = progress_collection.get()
                    if result["ids"]:
                        progress_collection.delete(ids=result["ids"])
                        click.echo(
                            f"‚úÖ –ü—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–±—Ä–æ—à–µ–Ω –¥–ª—è {len(result['ids'])} —á–∞—Ç–æ–≤"
                        )
                        click.echo(
                            "üí° –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—É—Å–∫–µ –≤—Å–µ —á–∞—Ç—ã –±—É–¥—É—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã –∑–∞–Ω–æ–≤–æ"
                        )
                    else:
                        click.echo("‚ö†Ô∏è  –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
                except Exception as e:
                    click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±—Ä–æ—Å–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
        else:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            click.echo("üîÑ –ü—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏:")
            click.echo()

            try:
                if chat:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞
                    from ..utils.naming import slugify

                    progress_id = f"progress_{slugify(chat)}"
                    result = progress_collection.get(
                        ids=[progress_id], include=["metadatas"]
                    )

                    if result["ids"]:
                        metadata = result["metadatas"][0]
                        click.echo(f"üìã –ß–∞—Ç: {metadata.get('chat_name', chat)}")
                        click.echo(
                            f"   –ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {metadata.get('last_indexed_date', 'N/A')}"
                        )
                        click.echo(
                            f"   –ü–æ—Å–ª–µ–¥–Ω—è—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: {metadata.get('last_indexing_time', 'N/A')}"
                        )
                        click.echo(
                            f"   –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {metadata.get('total_messages', 0)}"
                        )
                        click.echo(
                            f"   –í—Å–µ–≥–æ —Å–µ—Å—Å–∏–π: {metadata.get('total_sessions', 0)}"
                        )
                    else:
                        click.echo(f"‚ö†Ô∏è  –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –¥–ª—è —á–∞—Ç–∞ '{chat}'")
                else:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –≤—Å–µ—Ö —á–∞—Ç–æ–≤
                    result = progress_collection.get(include=["metadatas"])

                    if result["ids"]:
                        click.echo(f"–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(result['ids'])}")
                        click.echo()

                        for i, metadata in enumerate(result["metadatas"], 1):
                            chat_name = metadata.get("chat_name", "Unknown")
                            last_date = metadata.get("last_indexed_date", "N/A")
                            last_time = metadata.get("last_indexing_time", "N/A")
                            total_msgs = metadata.get("total_messages", 0)
                            total_sessions = metadata.get("total_sessions", 0)

                            click.echo(f"{i}. {chat_name}")
                            click.echo(f"   –ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {last_date}")
                            click.echo(f"   –ü–æ—Å–ª–µ–¥–Ω—è—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: {last_time}")
                            click.echo(
                                f"   –°–æ–æ–±—â–µ–Ω–∏–π: {total_msgs}, –°–µ—Å—Å–∏–π: {total_sessions}"
                            )
                            click.echo()
                    else:
                        click.echo("‚ö†Ô∏è  –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
                        click.echo("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –∫–æ–º–∞–Ω–¥–æ–π: memory_mcp index")
            except Exception as e:
                click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
                import traceback

                traceback.print_exc()

    except Exception as e:
        click.echo(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ ChromaDB: {e}")


@cli.command("update-summaries")
@click.option("--chat", help="–û–±–Ω–æ–≤–∏—Ç—å –æ—Ç—á–µ—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞")
@click.option(
    "--force",
    is_flag=True,
    help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã",
)
def update_summaries(chat, force):
    """üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ markdown-–æ—Ç—á–µ—Ç–æ–≤ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

    –ß–∏—Ç–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ JSON-—Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç markdown-–æ—Ç—á–µ—Ç—ã,
    –≤–∫–ª—é—á–∞—è —Ä–∞–∑–¥–µ–ª "–ê–∫—Ç—É–∞–ª—å–Ω–æ –∑–∞ 30 –¥–Ω–µ–π".
    """
    import json
    from datetime import datetime, timedelta
    from zoneinfo import ZoneInfo

    from ..analysis.markdown_renderer import MarkdownRenderer

    async def _update_summaries():
        click.echo("üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ markdown-–æ—Ç—á–µ—Ç–æ–≤...")
        click.echo()

        reports_dir = Path("artifacts/reports")

        if not reports_dir.exists():
            click.echo("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è artifacts/reports –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            click.echo("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é: memory_mcp index")
            return

        # –ù–∞—Ö–æ–¥–∏–º —á–∞—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if chat:
            chat_dirs = [reports_dir / chat] if (reports_dir / chat).exists() else []
            if not chat_dirs:
                click.echo(f"‚ùå –ß–∞—Ç '{chat}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ artifacts/reports/")
                return
        else:
            chat_dirs = [
                d
                for d in reports_dir.iterdir()
                if d.is_dir() and (d / "sessions").exists()
            ]

        if not chat_dirs:
            click.echo("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —á–∞—Ç–æ–≤ —Å —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è–º–∏")
            return

        click.echo(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —á–∞—Ç–æ–≤: {len(chat_dirs)}")
        click.echo()

        # –°–æ–∑–¥–∞–µ–º renderer
        renderer = MarkdownRenderer(output_dir=reports_dir)

        def parse_message_time(date_str: str) -> datetime:
            try:
                from ..utils.datetime_utils import parse_datetime_utc

                return parse_datetime_utc(date_str, default=datetime.now(ZoneInfo("UTC")), use_zoneinfo=True)
            except Exception:
                return datetime.now(ZoneInfo("UTC"))

        def load_session_summaries(chat_dir: Path) -> list:
            sessions = []
            sessions_dir = chat_dir / "sessions"
            if not sessions_dir.exists():
                return sessions

            json_files = list(sessions_dir.glob("*.json"))
            for json_file in json_files:
                try:
                    with open(json_file, encoding="utf-8") as f:
                        session = json.load(f)
                        sessions.append(session)
                except Exception as e:
                    click.echo(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {json_file.name}: {e}")
                    continue
            return sessions

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —á–∞—Ç
        updated = 0

        for chat_dir in chat_dirs:
            chat_name = chat_dir.name.replace("_", " ").title()
            click.echo(f"üìã –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç–∞: {chat_name}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            sessions = load_session_summaries(chat_dir)

            if not sessions:
                click.echo("   ‚ö†Ô∏è  –ù–µ—Ç —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
                continue

            click.echo(f"   üìä –ù–∞–π–¥–µ–Ω–æ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π: {len(sessions)}")

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–µ—Å—Å–∏–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
            now = datetime.now(ZoneInfo("UTC"))
            thirty_days_ago = now - timedelta(days=30)

            recent_sessions = []
            for session in sessions:
                end_time_str = session.get("meta", {}).get("end_time_utc", "")
                if end_time_str:
                    end_time = parse_message_time(end_time_str)
                    if end_time >= thirty_days_ago:
                        recent_sessions.append(session)

            click.echo(f"   üìÖ –°–µ—Å—Å–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π: {len(recent_sessions)}")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            top_sessions = sorted(
                recent_sessions,
                key=lambda s: s.get("quality", {}).get("score", 0),
                reverse=True,
            )

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã
            try:
                renderer.render_chat_summary(
                    chat_name, sessions, top_sessions=top_sessions, force=force
                )
                renderer.render_cumulative_context(chat_name, sessions, force=force)
                renderer.render_chat_index(chat_name, sessions, force=force)
                click.echo("   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –æ—Ç—á–µ—Ç—ã")
                updated += 1
            except Exception as e:
                click.echo(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {e}")

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        click.echo("=" * 80)
        click.echo(f"üìä –û–±–Ω–æ–≤–ª–µ–Ω–æ —á–∞—Ç–æ–≤: {updated}")
        click.echo("üìÇ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤: ./artifacts/reports/")

    asyncio.run(_update_summaries())


@cli.command("rebuild-vector-db")
@click.option(
    "--force",
    is_flag=True,
    help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É–¥–∞–ª–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è",
)
@click.option(
    "--keep-reports",
    is_flag=True,
    help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å markdown –æ—Ç—á–µ—Ç—ã –∏ JSON —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å ChromaDB)",
)
@click.option(
    "--backup",
    is_flag=True,
    help="–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º",
)
@click.option(
    "--no-progress",
    is_flag=True,
    help="–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏)",
)
def rebuild_vector_db(force, keep_reports, backup, no_progress):
    """üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ChromaDB

    –£–¥–∞–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç –µ—ë –∑–∞–Ω–æ–≤–æ,
    –∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (JSON —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, markdown –æ—Ç—á–µ—Ç—ã).

    –ü–æ–ª–µ–∑–Ω–æ –∫–æ–≥–¥–∞:
    - –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∞
    - –ù—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ö–µ–º—É –∫–æ–ª–ª–µ–∫—Ü–∏–π
    - –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

    –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ —É–¥–∞–ª–∏—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ ChromaDB!
    """

    async def _rebuild():
        import json
        import shutil
        from pathlib import Path

        click.echo("=" * 80)
        click.echo("üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ChromaDB")
        click.echo("=" * 80)
        click.echo()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        reports_dir = Path("artifacts/reports")
        chroma_dir = Path("chroma_db")

        if not reports_dir.exists():
            click.echo("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è artifacts/reports –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            click.echo("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é: memory_mcp index")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ JSON —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π
        json_files = list(reports_dir.glob("**/*.json"))
        if not json_files:
            click.echo("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π")
            click.echo("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é: memory_mcp index")
            return

        click.echo(f"üìÅ –ù–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π: {len(json_files)}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        if chroma_dir.exists():
            try:
                import chromadb

                chroma_client = chromadb.PersistentClient(path=str(chroma_dir))

                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö
                collections_info = []
                for collection_name in [
                    "chat_sessions",
                    "chat_messages",
                    "chat_tasks",
                    "session_clusters",
                    "indexing_progress",
                ]:
                    try:
                        collection = chroma_client.get_collection(collection_name)
                        count = collection.count()
                        collections_info.append(
                            f"   - {collection_name}: {count} –∑–∞–ø–∏—Å–µ–π"
                        )
                    except:
                        collections_info.append(f"   - {collection_name}: –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

                click.echo("üìä –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ChromaDB:")
                for info in collections_info:
                    click.echo(info)
                click.echo()

            except Exception as e:
                click.echo(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ ChromaDB: {e}")
                click.echo("   –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∞")
                click.echo()

        # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–∏—è
        if not force:
            click.echo("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è —É–¥–∞–ª–∏—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ ChromaDB!")
            click.echo("   –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –±—É–¥—É—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã.")
            click.echo()

            if not click.confirm("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?"):
                click.echo("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
                return

        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if backup and chroma_dir.exists():
            backup_dir = Path(
                f"chroma_db_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            click.echo(f"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {backup_dir}")
            try:
                shutil.copytree(chroma_dir, backup_dir)
                click.echo(f"‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {backup_dir}")
            except Exception as e:
                click.echo(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
                if not click.confirm("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏?"):
                    return
            click.echo()

        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        if chroma_dir.exists():
            click.echo("üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π ChromaDB...")
            try:
                shutil.rmtree(chroma_dir)
                click.echo("‚úÖ –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É–¥–∞–ª–µ–Ω–∞")
            except Exception as e:
                click.echo(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
                return
            click.echo()

        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        click.echo("üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤...")
        click.echo()

        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
            from ..core.indexer import TwoLevelIndexer

            click.echo("üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞...")
            indexer = TwoLevelIndexer()
            click.echo("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤")
            click.echo()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            click.echo("üìö –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π...")

            sessions_data = []
            for json_file in json_files:
                try:
                    with open(json_file, encoding="utf-8") as f:
                        session_data = json.load(f)
                        sessions_data.append(session_data)
                except Exception as e:
                    click.echo(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {json_file.name}: {e}")
                    continue

            click.echo(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π: {len(sessions_data)}")
            click.echo()

            if not sessions_data:
                click.echo("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã")
                return

            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            click.echo("üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–π ChromaDB...")

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —á–∞—Ç–∞–º
            chats_data = {}
            for session in sessions_data:
                chat_name = session.get("meta", {}).get("chat_name", "Unknown")
                if chat_name not in chats_data:
                    chats_data[chat_name] = []
                chats_data[chat_name].append(session)

            click.echo(f"üìã –ù–∞–π–¥–µ–Ω–æ —á–∞—Ç–æ–≤: {len(chats_data)}")

            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Å–µ—Å—Å–∏—é —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
            total_sessions = len(sessions_data)
            indexed_sessions = 0
            indexed_messages = 0
            indexed_tasks = 0

            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
            from tqdm import tqdm

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            show_progress = not no_progress

            if show_progress:
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –≤—Å–µ—Ö —Å–µ—Å—Å–∏–π
                with tqdm(
                    total=total_sessions,
                    desc="–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã",
                    unit="—Å–µ—Å—Å–∏—è",
                ) as pbar:
                    for chat_name, chat_sessions in chats_data.items():
                        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
                        pbar.set_description(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç–∞: {chat_name}")

                        for session in chat_sessions:
                            try:
                                # L1: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏ —Å–µ—Å—Å–∏–∏
                                await indexer._index_session_l1(session)
                                indexed_sessions += 1

                                # L2: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
                                messages_count = await indexer._index_messages_l2(
                                    session
                                )
                                indexed_messages += messages_count

                                # L3: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–¥–∞—á
                                tasks_count = await indexer._index_tasks(session)
                                indexed_tasks += tasks_count

                            except Exception as e:
                                click.echo(
                                    f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏ {session.get('session_id', 'Unknown')}: {e}"
                                )
                                continue

                            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                            pbar.set_postfix(
                                {
                                    "—Å–µ—Å—Å–∏–π": indexed_sessions,
                                    "—Å–æ–æ–±—â–µ–Ω–∏–π": indexed_messages,
                                    "–∑–∞–¥–∞—á": indexed_tasks,
                                }
                            )
                            pbar.update(1)
            else:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
                for chat_name, chat_sessions in chats_data.items():
                    click.echo(
                        f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Ç–∞: {chat_name} ({len(chat_sessions)} —Å–µ—Å—Å–∏–π)"
                    )

                    for session in chat_sessions:
                        try:
                            # L1: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏ —Å–µ—Å—Å–∏–∏
                            await indexer._index_session_l1(session)
                            indexed_sessions += 1

                            # L2: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
                            messages_count = await indexer._index_messages_l2(session)
                            indexed_messages += messages_count

                            # L3: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–¥–∞—á
                            tasks_count = await indexer._index_tasks(session)
                            indexed_tasks += tasks_count

                        except Exception as e:
                            click.echo(
                                f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏ {session.get('session_id', 'Unknown')}: {e}"
                            )
                            continue

                    click.echo(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–µ—Å—Å–∏–π: {len(chat_sessions)}")

            click.echo()
            click.echo("=" * 80)
            click.echo("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞!")
            click.echo("=" * 80)
            click.echo()
            click.echo("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            click.echo(f"   - –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–æ —Å–µ—Å—Å–∏–π (L1): {indexed_sessions}")
            click.echo(f"   - –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π (L2): {indexed_messages}")
            click.echo(f"   - –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–æ –∑–∞–¥–∞—á (L3): {indexed_tasks}")
            click.echo()
            click.echo("üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
            click.echo("   - –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞: ./chroma_db/")
            click.echo("   - –ö–æ–ª–ª–µ–∫—Ü–∏–∏: chat_sessions, chat_messages, chat_tasks")
            if keep_reports:
                click.echo("   - Markdown –æ—Ç—á–µ—Ç—ã: —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./artifacts/reports/")
            click.echo()
            click.echo("üí° –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫: memory_mcp search")

        except Exception as e:
            click.echo()
            click.echo("=" * 80)
            click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã!")
            click.echo("=" * 80)
            click.echo(f"–û—à–∏–±–∫–∞: {e}")
            click.echo()
            import traceback

            traceback.print_exc()

    asyncio.run(_rebuild())


@cli.command("extract-messages")
@click.option("--dry-run", is_flag=True, help="–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤")
@click.option("--no-date-filter", is_flag=True, help="–û—Ç–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –¥–∞—Ç–µ")
@click.option("--chat", help="–§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —á–∞—Ç–∞")
@click.option("--input-dir", default="input", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
@click.option(
    "--chats-dir", default="chats", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π"
)
def extract_messages(dry_run, no_date_filter, chat, input_dir, chats_dir):
    """üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ input –≤ chats

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ input –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ chats,
    —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –¥–∞—Ç–µ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π.
    """

    async def _extract_messages():
        click.echo("üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π...")
        click.echo(f"   –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {input_dir}")
        click.echo(f"   –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {chats_dir}")
        click.echo(
            f"   –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ: {'‚ùå –û—Ç–∫–ª—é—á–µ–Ω' if no_date_filter else '‚úÖ –í–∫–ª—é—á–µ–Ω'}"
        )
        click.echo(f"   –§–∏–ª—å—Ç—Ä –ø–æ —á–∞—Ç—É: {chat or '–≤—Å–µ —á–∞—Ç—ã'}")
        click.echo(f"   –†–µ–∂–∏–º: {'üî∏ DRY RUN' if dry_run else '‚úÖ –†–µ–∞–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ'}")
        click.echo()

        # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
        extractor = MessageExtractor(input_dir=input_dir, chats_dir=chats_dir)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        extractor.extract_all_messages(
            dry_run=dry_run, filter_by_date=not no_date_filter, chat_filter=chat
        )

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        extractor.print_stats()

        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        click.echo("=" * 80)

    asyncio.run(_extract_messages())


@cli.command("deduplicate")
@click.option(
    "--chats-dir", default="chats", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"
)
def deduplicate(chats_dir):
    """üßπ –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π

    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –ø–æ–ª—é 'id' –≤–æ –≤—Å–µ—Ö —á–∞—Ç–∞—Ö.
    """

    async def _deduplicate():
        click.echo("üßπ –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        click.echo(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {chats_dir}")
        click.echo()

        # –°–æ–∑–¥–∞–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä
        deduplicator = MessageDeduplicator(chats_dir=chats_dir)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
        deduplicator.deduplicate_all_chats()

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        deduplicator.print_stats()

        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        click.echo("=" * 80)

    asyncio.run(_deduplicate())


@cli.command("sync-chromadb")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏",
)
@click.option(
    "--chroma-path",
    default="chroma_db",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ ChromaDB",
)
@click.option(
    "--chat",
    help="–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–π —á–∞—Ç",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="–†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π",
)
def sync_chromadb(db_path: Path, chroma_path: Path, chat: Optional[str], dry_run: bool):
    """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π –∏–∑ ChromaDB –≤ –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏.
    
    –≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –º–∏–≥—Ä–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏–π
    (chat_messages, chat_sessions, chat_tasks) –≤ –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏ TypedGraphMemory.
    –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–∞–∫–∂–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è.
    """
    import chromadb
    from ..memory.ingest import MemoryIngestor
    from ..indexing import MemoryRecord
    from ..utils.datetime_utils import parse_datetime_utc
    from datetime import datetime, timezone
    
    logger.info("üîÑ –ù–∞—á–∞–ª–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ ChromaDB ‚Üí –ì—Ä–∞—Ñ –ø–∞–º—è—Ç–∏")
    
    if dry_run:
        logger.info("üîç –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (dry-run), –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞
    graph = TypedGraphMemory(db_path=str(db_path))
    ingestor = MemoryIngestor(graph)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ Qdrant
    from ..memory.embeddings import build_embedding_service_from_env
    from ..memory.vector_store import build_vector_store_from_env
    
    embedding_service = build_embedding_service_from_env()
    vector_store = build_vector_store_from_env()
    
    if vector_store and embedding_service and embedding_service.dimension:
        vector_store.ensure_collection(embedding_service.dimension)
        logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
    else:
        logger.warning("‚ö†Ô∏è  –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Qdrant")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
    chroma_client = chromadb.PersistentClient(path=str(chroma_path))
    
    total_synced = 0
    total_errors = 0
    
    collections_to_sync = ["chat_messages", "chat_sessions", "chat_tasks"]
    
    for collection_name in collections_to_sync:
        try:
            collection = chroma_client.get_collection(collection_name)
            total_count = collection.count()
            
            if total_count == 0:
                logger.info(f"  –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}: –ø—É—Å—Ç–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
            
            logger.info(f"  –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}: {total_count} –∑–∞–ø–∏—Å–µ–π")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –±–∞—Ç—á–∞–º–∏
            offset = 0
            batch_size = 100
            synced_in_collection = 0
            
            while offset < total_count:
                try:
                    result = collection.get(
                        limit=batch_size,
                        offset=offset,
                        include=["documents", "metadatas", "embeddings"]
                    )
                    
                    ids = result.get("ids", [])
                    if not ids:
                        break
                    
                    documents = result.get("documents", [])
                    metadatas = result.get("metadatas", [])
                    embeddings = result.get("embeddings", [])
                    
                    records_to_ingest = []
                    
                    for idx, record_id in enumerate(ids):
                        try:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—å –≤ –≥—Ä–∞—Ñ–µ
                            if record_id in graph.graph:
                                continue
                            
                            # –§–∏–ª—å—Ç—Ä –ø–æ —á–∞—Ç—É, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                            metadata = metadatas[idx] if idx < len(metadatas) else {}
                            if chat and metadata.get("chat") != chat:
                                continue
                            
                            doc = documents[idx] if idx < len(documents) else ""
                            embedding = embeddings[idx] if idx < len(embeddings) else None
                            
                            # –ü–∞—Ä—Å–∏–º timestamp
                            date_utc = metadata.get("date_utc") or metadata.get("start_time_utc") or metadata.get("end_time_utc")
                            timestamp = None
                            if date_utc:
                                try:
                                    timestamp = parse_datetime_utc(date_utc, use_zoneinfo=True)
                                except Exception:
                                    timestamp = datetime.now(timezone.utc)
                            else:
                                timestamp = datetime.now(timezone.utc)
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä–∞
                            author = metadata.get("sender") or metadata.get("author") or metadata.get("username")
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–≥–∏ –∏ —Å—É—â–Ω–æ—Å—Ç–∏
                            tags = metadata.get("tags", [])
                            if isinstance(tags, str):
                                tags = [tags] if tags else []
                            
                            entities = metadata.get("entities", [])
                            if isinstance(entities, str):
                                entities = [entities] if entities else []
                            
                            # –°–æ–∑–¥–∞—ë–º MemoryRecord
                            record = MemoryRecord(
                                record_id=record_id,
                                source=metadata.get("chat", collection_name.replace("chat_", "")),
                                content=doc,
                                timestamp=timestamp,
                                author=author,
                                tags=tags if isinstance(tags, list) else [],
                                entities=entities if isinstance(entities, list) else [],
                                attachments=[],
                                metadata={
                                    "collection": collection_name,
                                    "chat": metadata.get("chat", ""),
                                    **metadata,
                                },
                            )
                            
                            records_to_ingest.append((record, embedding))
                            
                        except Exception as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∑–∞–ø–∏—Å–∏ {record_id}: {e}")
                            total_errors += 1
                            continue
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å–∏ –≤ –≥—Ä–∞—Ñ
                    if records_to_ingest and not dry_run:
                        try:
                            records_only = [r for r, _ in records_to_ingest]
                            ingestor.ingest(records_only)
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –≥—Ä–∞—Ñ –∏ Qdrant
                            for record, embedding in records_to_ingest:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –ø—É—Å—Ç–æ–π
                                if embedding is not None and len(embedding) > 0:
                                    try:
                                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –º–∞—Å—Å–∏–≤ –≤ —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                                        if hasattr(embedding, 'tolist'):
                                            embedding = embedding.tolist()
                                        elif not isinstance(embedding, list):
                                            embedding = list(embedding)
                                        
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –≥—Ä–∞—Ñ
                                        graph.update_node(record.record_id, embedding=embedding)
                                        
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ Qdrant
                                        if vector_store:
                                            payload_data = {
                                                "record_id": record.record_id,
                                                "source": record.source,
                                                "tags": record.tags,
                                                "timestamp": record.timestamp.timestamp(),
                                                "timestamp_iso": record.timestamp.isoformat(),
                                                "content_preview": record.content[:200],
                                            }
                                            chat_name = record.metadata.get("chat")
                                            if isinstance(chat_name, str):
                                                payload_data["chat"] = chat_name
                                            
                                            try:
                                                vector_store.upsert(record.record_id, embedding, payload_data)
                                            except Exception as e:
                                                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ Qdrant –¥–ª—è {record.record_id}: {e}")
                                    except Exception as e:
                                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è {record.record_id}: {e}")
                            
                            synced_in_collection += len(records_to_ingest)
                            total_synced += len(records_to_ingest)
                            
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∑–∞–ø–∏—Å–µ–π –≤ –≥—Ä–∞—Ñ: {e}")
                            total_errors += len(records_to_ingest)
                    elif records_to_ingest and dry_run:
                        synced_in_collection += len(records_to_ingest)
                        total_synced += len(records_to_ingest)
                    
                    offset += len(ids)
                    if len(ids) < batch_size:
                        break
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ (offset={offset}): {e}")
                    total_errors += batch_size
                    offset += batch_size
            
            if synced_in_collection > 0:
                logger.info(f"  ‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {synced_in_collection} –∑–∞–ø–∏—Å–µ–π –∏–∑ {collection_name}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}: {e}")
            total_errors += 1
    
    if dry_run:
        logger.info(f"üîç –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –±—ã–ª–æ –±—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {total_synced} –∑–∞–ø–∏—Å–µ–π")
    else:
        logger.info(f"‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {total_synced} –∑–∞–ø–∏—Å–µ–π, {total_errors} –æ—à–∏–±–æ–∫")
        if vector_store:
            logger.info("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Qdrant")
    
    graph.conn.close()
    if vector_store:
        vector_store.close()
    if embedding_service:
        embedding_service.close()


@cli.command("stop-indexing")
def stop_indexing():
    """üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

    –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ Ollama —Å–µ—Ä–≤–µ—Ä.
    """

    async def _stop_indexing():
        click.echo("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
        click.echo()

        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
        ProcessManager.stop_all_indexing()

        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        click.echo("=" * 80)

    asyncio.run(_stop_indexing())


@cli.command("review-summaries")
@click.option("--dry-run", is_flag=True, help="–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤")
@click.option("--chat", help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —á–∞—Ç")
@click.option("--limit", type=int, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
def review_summaries(dry_run, chat, limit):
    """üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–≤—å—é –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º -needs-review

    –ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã *-needs-review.md, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Ö —á–µ—Ä–µ–∑ LLM –∏ —Å–æ–∑–¥–∞–µ—Ç
    –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞ -needs-review.
    """
    import json

    from ..core.lmstudio_client import LMStudioEmbeddingClient
    from ..config import get_settings

    async def _review_summaries():
        click.echo("üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–≤—å—é –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π")
        click.echo()

        if dry_run:
            click.echo("üî∏ –†–µ–∂–∏–º DRY RUN - —Ñ–∞–π–ª—ã –Ω–µ –±—É–¥—É—Ç –∏–∑–º–µ–Ω–µ–Ω—ã")
            click.echo()

        reports_dir = Path("artifacts/reports")

        if not reports_dir.exists():
            click.echo("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è artifacts/reports –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã —Å -needs-review
        needs_review_files = []
        for md_file in reports_dir.rglob("*-needs-review.md"):
            json_file = md_file.with_suffix(".json")

            file_info = {
                "md_file": md_file,
                "json_file": json_file if json_file.exists() else None,
                "session_id": md_file.stem.replace("-needs-review", ""),
                "chat": md_file.parent.parent.name,
            }

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —á–∞—Ç—É –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if chat and chat.lower() not in file_info["chat"].lower():
                continue

            needs_review_files.append(file_info)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ª–∏–º–∏—Ç
        if limit:
            needs_review_files = needs_review_files[:limit]

        if not needs_review_files:
            click.echo("‚úÖ –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º -needs-review")
            return

        click.echo(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(needs_review_files)}")
        click.echo()

        # –°–æ–∑–¥–∞–µ–º LLM –∫–ª–∏–µ–Ω—Ç
        settings = get_settings()
        embedding_client = LMStudioEmbeddingClient(
            model_name=settings.lmstudio_model,
            base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
        )

        async def review_summary(md_content: str) -> dict:
            prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∏ —É–ª—É—á—à–µ–Ω–∏—é —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π —á–∞—Ç–æ–≤.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â—É—é —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –∏ —É–ª—É—á—à–∏ –µ—ë, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:

{md_content}

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –ø–æ–ª–Ω–æ—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
2. –ò—Å–ø—Ä–∞–≤–∏—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
3. –£–ª—É—á—à–∏—Ç—å —è—Å–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å
4. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
5. –î–æ–±–∞–≤–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â—É—é –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –æ—á–µ–≤–∏–¥–Ω–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–í–ê–ñ–ù–û:
- –°–æ—Ö—Ä–∞–Ω–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É markdown (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏, –∏ —Ç.–¥.)
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
- –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –¥–∞—Ç—ã, –∏–º–µ–Ω–∞ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- –ï—Å–ª–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ö–æ—Ä–æ—à–∞—è - –≤–µ—Ä–Ω–∏ –µ—ë –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —É–ª—É—á—à–µ–Ω–Ω—ã–π markdown-—Ç–µ–∫—Å—Ç –ë–ï–ó –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""

            try:
                async with embedding_client:
                    improved = await embedding_client.generate_summary(
                        prompt,
                        temperature=0.3,
                        max_tokens=131072,  # –î–ª—è gpt-oss-20b (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç)
                    )
                    improved = improved.strip()

                    if improved:

                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
                        issues_found = []
                        if (
                            "_(–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö)_" in md_content
                            or "_(–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)_" in md_content
                        ):
                            issues_found.append("–ï—Å—Ç—å –ø—É—Å—Ç—ã–µ —Å–µ–∫—Ü–∏–∏")
                        if len(md_content) < 200:
                            issues_found.append("–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
                        if md_content.count("##") < 2:
                            issues_found.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏—è")

                        improvements = []
                        if md_content != improved:
                            improvements.append("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –æ—à–∏–±–∫–∏")
                        if len(improved) > len(md_content) * 1.1:
                            improvements.append("–†–∞—Å—à–∏—Ä–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç")
                        if not improvements:
                            improvements.append("–ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")

                        return {
                            "improved_content": improved,
                            "issues_found": issues_found,
                            "improvements": improvements,
                            "success": True,
                        }
                    else:
                        return {
                            "improved_content": md_content,
                            "issues_found": [],
                            "improvements": [],
                            "success": False,
                            "error": "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM",
                        }

            except Exception as e:
                return {
                    "improved_content": md_content,
                    "issues_found": [],
                    "improvements": [],
                    "success": False,
                    "error": str(e),
                }

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª

        for file_info in needs_review_files:
            md_file = file_info["md_file"]
            json_file = file_info["json_file"]
            session_id = file_info["session_id"]

            click.echo(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {md_file.name}")

            # –ß–∏—Ç–∞–µ–º markdown
            try:
                with open(md_file, encoding="utf-8") as f:
                    md_content = f.read()
            except Exception as e:
                click.echo(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è MD: {e}")
                continue

            # –ü—Ä–æ–≤–æ–¥–∏–º —Ä–µ–≤—å—é
            click.echo("   üîç –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM...")
            review_result = await review_summary(md_content)

            if not review_result["success"]:
                click.echo(
                    f"   ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {review_result.get('error', 'Unknown')}"
                )
                continue

            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
            if review_result["issues_found"]:
                click.echo(
                    f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {', '.join(review_result['issues_found'])}"
                )

            if review_result["improvements"]:
                click.echo(
                    f"   ‚ú® –£–ª—É—á—à–µ–Ω–∏—è: {', '.join(review_result['improvements'])}"
                )

            if dry_run:
                click.echo("   üî∏ DRY RUN - —Ñ–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω—ë–Ω")
                continue

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
            new_md_file = md_file.parent / f"{session_id}.md"
            new_json_file = md_file.parent / f"{session_id}.json"

            try:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π markdown
                with open(new_md_file, "w", encoding="utf-8") as f:
                    f.write(review_result["improved_content"])

                # –û–±–Ω–æ–≤–ª—è–µ–º JSON –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if json_file:
                    try:
                        with open(json_file, encoding="utf-8") as f:
                            session_data = json.load(f)

                        session_data["session_id"] = session_id

                        with open(new_json_file, "w", encoding="utf-8") as f:
                            json.dump(session_data, f, ensure_ascii=False, indent=2)

                        if new_json_file != json_file:
                            json_file.unlink()
                            click.echo(f"   üóëÔ∏è  –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π JSON: {json_file.name}")
                    except Exception as e:
                        click.echo(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è JSON: {e}")

                if new_md_file != md_file:
                    md_file.unlink()
                    click.echo(f"   üóëÔ∏è  –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π MD: {md_file.name}")

                click.echo(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {new_md_file.name}")

            except Exception as e:
                click.echo(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1)

        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        click.echo("=" * 80)

    asyncio.run(_review_summaries())


@cli.command("backup-database")
@click.option(
    "--backup-path",
    type=click.Path(path_type=Path),
    help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è backup (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: backups/backup_YYYYMMDD_HHMMSS)",
)
@click.option(
    "--include-chromadb/--no-chromadb",
    default=True,
    help="–í–∫–ª—é—á–∏—Ç—å ChromaDB –≤ backup",
)
@click.option(
    "--include-reports/--no-reports",
    default=False,
    help="–í–∫–ª—é—á–∏—Ç—å markdown –æ—Ç—á–µ—Ç—ã –≤ backup",
)
@click.option(
    "--compress/--no-compress",
    default=True,
    help="–°–æ–∑–¥–∞—Ç—å —Å–∂–∞—Ç—ã–π .tar.gz –∞—Ä—Ö–∏–≤",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö",
)
@click.option(
    "--chroma-path",
    default="chroma_db",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ ChromaDB",
)
def backup_database(backup_path, include_chromadb, include_reports, compress, db_path, chroma_path):
    """üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (SQLite + ChromaDB)
    
    –°–æ–∑–¥–∞—ë—Ç –ø–æ–ª–Ω—É—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º—ã:
    - SQLite –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (memory_graph.db)
    - ChromaDB –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - Markdown –æ—Ç—á–µ—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    import shutil
    import tarfile
    
    click.echo("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
    click.echo()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –¥–ª—è backup
    if not backup_path:
        backups_dir = Path("backups")
        backups_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = backups_dir / f"backup_{timestamp}"
        if compress:
            backup_path = backup_path.with_suffix(".tar.gz")
    
    backup_path = Path(backup_path)
    backup_path.parent.mkdir(parents=True, exist_ok=True)
    
    includes = []
    temp_backup_dir = None
    
    try:
        if compress:
            # –î–ª—è —Å–∂–∞—Ç–æ–≥–æ –∞—Ä—Ö–∏–≤–∞ —Å–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            temp_backup_dir = Path(f"/tmp/memory_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            temp_backup_dir.mkdir(exist_ok=True)
            actual_backup_path = temp_backup_dir
        else:
            actual_backup_path = backup_path
            actual_backup_path.mkdir(exist_ok=True)
        
        # –ö–æ–ø–∏—Ä—É–µ–º SQLite –ë–î
        if db_path.exists():
            click.echo(f"üìÑ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ SQLite –ë–î: {db_path}")
            db_backup_path = actual_backup_path / "memory_graph.db"
            shutil.copy2(db_path, db_backup_path)
            includes.append("sqlite_database")
            click.echo(f"   ‚úÖ –†–∞–∑–º–µ—Ä: {db_backup_path.stat().st_size / 1024 / 1024:.2f} MB")
        else:
            click.echo(f"‚ö†Ô∏è  SQLite –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        
        # –ö–æ–ø–∏—Ä—É–µ–º ChromaDB
        if include_chromadb and chroma_path.exists():
            click.echo(f"üîç –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ ChromaDB: {chroma_path}")
            chroma_backup_path = actual_backup_path / "chroma_db"
            shutil.copytree(chroma_path, chroma_backup_path, dirs_exist_ok=True)
            includes.append("chromadb")
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            total_size = sum(f.stat().st_size for f in chroma_backup_path.rglob('*') if f.is_file())
            click.echo(f"   ‚úÖ –†–∞–∑–º–µ—Ä: {total_size / 1024 / 1024:.2f} MB")
        elif include_chromadb:
            click.echo(f"‚ö†Ô∏è  ChromaDB –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {chroma_path}")
        
        # –ö–æ–ø–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã
        if include_reports:
            reports_path = Path("artifacts/reports")
            if reports_path.exists():
                click.echo(f"üìä –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤: {reports_path}")
                reports_backup_path = actual_backup_path / "reports"
                shutil.copytree(reports_path, reports_backup_path, dirs_exist_ok=True)
                includes.append("reports")
                total_size = sum(f.stat().st_size for f in reports_backup_path.rglob('*') if f.is_file())
                click.echo(f"   ‚úÖ –†–∞–∑–º–µ—Ä: {total_size / 1024 / 1024:.2f} MB")
            else:
                click.echo(f"‚ö†Ô∏è  –û—Ç—á–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {reports_path}")
        
        # –°–æ–∑–¥–∞—ë–º –∞—Ä—Ö–∏–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if compress and temp_backup_dir:
            click.echo(f"üóúÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞: {backup_path}")
            with tarfile.open(backup_path, "w:gz") as tar:
                tar.add(temp_backup_dir, arcname=backup_path.stem)
            backup_size = backup_path.stat().st_size
            click.echo(f"   ‚úÖ –†–∞–∑–º–µ—Ä –∞—Ä—Ö–∏–≤–∞: {backup_size / 1024 / 1024:.2f} MB")
        
        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
        click.echo("=" * 80)
        click.echo(f"üìÅ –ü—É—Ç—å: {backup_path}")
        click.echo(f"üì¶ –í–∫–ª—é—á–µ–Ω–æ: {', '.join(includes)}")
        if compress:
            click.echo(f"üìä –†–∞–∑–º–µ—Ä: {backup_path.stat().st_size / 1024 / 1024:.2f} MB")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏!")
        click.echo("=" * 80)
        click.echo(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()
    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        if temp_backup_dir and temp_backup_dir.exists():
            shutil.rmtree(temp_backup_dir)


@cli.command("restore-database")
@click.option(
    "--backup-path",
    type=click.Path(exists=True, path_type=Path),
    required=True,
    help="–ü—É—Ç—å –∫ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ (—Ñ–∞–π–ª .tar.gz –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è)",
)
@click.option(
    "--confirm",
    is_flag=True,
    help="–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ (—É–¥–∞–ª–∏—Ç —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ)",
)
@click.option(
    "--restore-chromadb/--no-chromadb",
    default=True,
    help="–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å ChromaDB",
)
@click.option(
    "--restore-reports/--no-reports",
    default=False,
    help="–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å markdown –æ—Ç—á–µ—Ç—ã",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö",
)
@click.option(
    "--chroma-path",
    default="chroma_db",
    type=click.Path(file_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ ChromaDB",
)
def restore_database(backup_path, confirm, restore_chromadb, restore_reports, db_path, chroma_path):
    """üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
    
    –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è —É–¥–∞–ª–∏—Ç —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –∑–∞–º–µ–Ω–∏—Ç –∏—Ö –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ backup!
    """
    import shutil
    import tarfile
    
    click.echo("üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏")
    click.echo()
    
    if not confirm:
        click.echo("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è —É–¥–∞–ª–∏—Ç —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ!")
        click.echo(f"   –ë—É–¥–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑: {backup_path}")
        if not click.confirm("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?"):
            click.echo("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
            return
    
    backup_path = Path(backup_path)
    temp_extract_dir = None
    
    try:
        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if backup_path.suffix == ".gz" or backup_path.suffixes == [".tar", ".gz"]:
            click.echo(f"üì¶ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–∞: {backup_path}")
            temp_extract_dir = Path(f"/tmp/memory_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            temp_extract_dir.mkdir(exist_ok=True)
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(temp_extract_dir)
            # –ù–∞—Ö–æ–¥–∏–º —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            extracted_dirs = [d for d in temp_extract_dir.iterdir() if d.is_dir()]
            if extracted_dirs:
                source_dir = extracted_dirs[0]
            else:
                source_dir = temp_extract_dir
        else:
            source_dir = backup_path
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º SQLite –ë–î
        db_backup = source_dir / "memory_graph.db"
        if db_backup.exists():
            click.echo(f"üìÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ SQLite –ë–î: {db_path}")
            if db_path.exists():
                # –°–æ–∑–¥–∞—ë–º backup —Ç–µ–∫—É—â–µ–π –ë–î
                old_db_backup = Path(f"{db_path}.old_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
                shutil.copy2(db_path, old_db_backup)
                click.echo(f"   üíæ –¢–µ–∫—É—â–∞—è –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫: {old_db_backup}")
            db_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(db_backup, db_path)
            click.echo("   ‚úÖ SQLite –ë–î –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        else:
            click.echo(f"‚ö†Ô∏è  SQLite –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ backup: {db_backup}")
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ChromaDB
        if restore_chromadb:
            chroma_backup = source_dir / "chroma_db"
            if chroma_backup.exists() and chroma_backup.is_dir():
                click.echo(f"üîç –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ ChromaDB: {chroma_path}")
                if chroma_path.exists():
                    shutil.rmtree(chroma_path)
                chroma_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copytree(chroma_backup, chroma_path)
                click.echo("   ‚úÖ ChromaDB –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            else:
                click.echo(f"‚ö†Ô∏è  ChromaDB –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ backup: {chroma_backup}")
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ç—á–µ—Ç—ã
        if restore_reports:
            reports_backup = source_dir / "reports"
            if reports_backup.exists() and reports_backup.is_dir():
                reports_path = Path("artifacts/reports")
                click.echo(f"üìä –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤: {reports_path}")
                if reports_path.exists():
                    shutil.rmtree(reports_path)
                reports_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copytree(reports_backup, reports_path)
                click.echo("   ‚úÖ –û—Ç—á–µ—Ç—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
            else:
                click.echo(f"‚ö†Ô∏è  –û—Ç—á–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ backup: {reports_backup}")
        
        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
        click.echo("=" * 80)
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö!")
        click.echo("=" * 80)
        click.echo(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()
    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        if temp_extract_dir and temp_extract_dir.exists():
            shutil.rmtree(temp_extract_dir)


@cli.command("optimize-database")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö",
)
@click.option(
    "--vacuum/--no-vacuum",
    default=True,
    help="–í—ã–ø–æ–ª–Ω–∏—Ç—å VACUUM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –º–µ—Å—Ç–∞",
)
@click.option(
    "--analyze/--no-analyze",
    default=True,
    help="–í—ã–ø–æ–ª–Ω–∏—Ç—å ANALYZE –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏",
)
@click.option(
    "--reindex/--no-reindex",
    default=False,
    help="–í—ã–ø–æ–ª–Ω–∏—Ç—å REINDEX –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤",
)
@click.option(
    "--optimize-fts/--no-optimize-fts",
    default=True,
    help="–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å FTS5 –∏–Ω–¥–µ–∫—Å",
)
def optimize_database(db_path, vacuum, analyze, reindex, optimize_fts):
    """‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
    - VACUUM: –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç –º–µ—Å—Ç–æ, —É–¥–∞–ª—è—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    - ANALYZE: –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    - REINDEX: –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç –∏–Ω–¥–µ–∫—Å—ã
    - FTS5 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
    """
    import sqlite3
    import time
    
    click.echo("‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
    click.echo()
    
    if not db_path.exists():
        click.echo(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        raise click.Abort()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    size_before = db_path.stat().st_size
    
    operations_performed = []
    start_time = time.time()
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # VACUUM
        if vacuum:
            click.echo("üßπ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ VACUUM...")
            cursor.execute("VACUUM")
            conn.commit()
            operations_performed.append("VACUUM")
            click.echo("   ‚úÖ VACUUM –≤—ã–ø–æ–ª–Ω–µ–Ω")
        
        # ANALYZE
        if analyze:
            click.echo("üìä –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ ANALYZE...")
            cursor.execute("ANALYZE")
            conn.commit()
            operations_performed.append("ANALYZE")
            click.echo("   ‚úÖ ANALYZE –≤—ã–ø–æ–ª–Ω–µ–Ω")
        
        # REINDEX
        if reindex:
            click.echo("üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ REINDEX...")
            cursor.execute("REINDEX")
            conn.commit()
            operations_performed.append("REINDEX")
            click.echo("   ‚úÖ REINDEX –≤—ã–ø–æ–ª–Ω–µ–Ω")
        
        # FTS5 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        if optimize_fts:
            click.echo("üîç –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è FTS5 –∏–Ω–¥–µ–∫—Å–∞...")
            try:
                cursor.execute("INSERT INTO node_search(node_search) VALUES('optimize')")
                conn.commit()
                operations_performed.append("FTS5_optimize")
                click.echo("   ‚úÖ FTS5 –∏–Ω–¥–µ–∫—Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except sqlite3.OperationalError as e:
                if "no such table" not in str(e).lower():
                    raise
                click.echo("   ‚ö†Ô∏è  FTS5 —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        conn.close()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        size_after = db_path.stat().st_size
        space_freed = size_before - size_after
        duration = time.time() - start_time
        
        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        click.echo("=" * 80)
        click.echo(f"üìä –û–ø–µ—Ä–∞—Ü–∏–∏: {', '.join(operations_performed)}")
        click.echo(f"üì¶ –†–∞–∑–º–µ—Ä –¥–æ: {size_before / 1024 / 1024:.2f} MB")
        click.echo(f"üì¶ –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ: {size_after / 1024 / 1024:.2f} MB")
        if space_freed > 0:
            click.echo(f"üíæ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ: {space_freed / 1024 / 1024:.2f} MB")
        click.echo(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö!")
        click.echo("=" * 80)
        click.echo(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("validate-database")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö",
)
@click.option(
    "--check-integrity/--no-check-integrity",
    default=True,
    help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å SQLite (PRAGMA integrity_check)",
)
@click.option(
    "--check-foreign-keys/--no-check-foreign-keys",
    default=True,
    help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏ (PRAGMA foreign_key_check)",
)
@click.option(
    "--check-orphaned-nodes/--no-check-orphaned-nodes",
    default=True,
    help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É–∑–ª—ã –±–µ–∑ —Å–≤—è–∑–µ–π",
)
@click.option(
    "--check-orphaned-edges/--no-check-orphaned-edges",
    default=True,
    help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä—ë–±—Ä–∞ —Å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —É–∑–ª–∞–º–∏",
)
def validate_database(db_path, check_integrity, check_foreign_keys, check_orphaned_nodes, check_orphaned_edges):
    """üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö:
    - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ SQLite
    - –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π
    - –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π (—Å–∏—Ä–æ—Ç—Å–∫–∏–µ —É–∑–ª—ã –∏ —Ä—ë–±—Ä–∞)
    """
    import sqlite3
    
    click.echo("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
    click.echo()
    
    if not db_path.exists():
        click.echo(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        raise click.Abort()
    
    issues = []
    checks_performed = []
    
    try:
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ SQLite
        if check_integrity:
            click.echo("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ SQLite...")
            cursor.execute("PRAGMA integrity_check")
            result = cursor.fetchone()[0]
            checks_performed.append("integrity_check")
            if result == "ok":
                click.echo("   ‚úÖ –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å SQLite: OK")
            else:
                click.echo(f"   ‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å—é: {result}")
                issues.append({
                    "type": "integrity",
                    "severity": "error",
                    "message": f"SQLite integrity check failed: {result}",
                    "details": {"result": result}
                })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π
        if check_foreign_keys:
            click.echo("üîó –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π...")
            cursor.execute("PRAGMA foreign_key_check")
            foreign_key_issues = cursor.fetchall()
            checks_performed.append("foreign_key_check")
            if not foreign_key_issues:
                click.echo("   ‚úÖ –í–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏: OK")
            else:
                click.echo(f"   ‚ùå –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∫–ª—é—á–∞–º–∏: {len(foreign_key_issues)}")
                for issue in foreign_key_issues:
                    issues.append({
                        "type": "foreign_key",
                        "severity": "error",
                        "message": f"Foreign key violation: {dict(issue)}",
                        "details": dict(issue)
                    })
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        if check_orphaned_nodes or check_orphaned_edges:
            click.echo("üï∏Ô∏è  –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π...")
            from ..memory.typed_graph import TypedGraphMemory
            graph = TypedGraphMemory(db_path=str(db_path))
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Ä–æ—Ç—Å–∫–∏—Ö —É–∑–ª–æ–≤
            if check_orphaned_nodes:
                cursor.execute("""
                    SELECT id, type 
                    FROM nodes 
                    WHERE id NOT IN (
                        SELECT DISTINCT source_id FROM edges
                        UNION
                        SELECT DISTINCT target_id FROM edges
                    )
                """)
                orphaned_nodes = cursor.fetchall()
                checks_performed.append("orphaned_nodes")
                if not orphaned_nodes:
                    click.echo("   ‚úÖ –°–∏—Ä–æ—Ç—Å–∫–∏–µ —É–∑–ª—ã: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                else:
                    click.echo(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ —Å–∏—Ä–æ—Ç—Å–∫–∏—Ö —É–∑–ª–æ–≤: {len(orphaned_nodes)}")
                    for node in orphaned_nodes[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                        issues.append({
                            "type": "orphaned_node",
                            "severity": "warning",
                            "message": f"Node '{node['id']}' has no connections",
                            "details": {"node_id": node["id"], "node_type": node["type"]}
                        })
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Ä–æ—Ç—Å–∫–∏—Ö —Ä—ë–±–µ—Ä
            if check_orphaned_edges:
                cursor.execute("""
                    SELECT e.id, e.source_id, e.target_id, e.type
                    FROM edges e
                    LEFT JOIN nodes n1 ON e.source_id = n1.id
                    LEFT JOIN nodes n2 ON e.target_id = n2.id
                    WHERE n1.id IS NULL OR n2.id IS NULL
                """)
                orphaned_edges = cursor.fetchall()
                checks_performed.append("orphaned_edges")
                if not orphaned_edges:
                    click.echo("   ‚úÖ –°–∏—Ä–æ—Ç—Å–∫–∏–µ —Ä—ë–±—Ä–∞: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                else:
                    click.echo(f"   ‚ùå –ù–∞–π–¥–µ–Ω–æ —Å–∏—Ä–æ—Ç—Å–∫–∏—Ö —Ä—ë–±–µ—Ä: {len(orphaned_edges)}")
                    for edge in orphaned_edges[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                        issues.append({
                            "type": "orphaned_edge",
                            "severity": "error",
                            "message": f"Edge '{edge['id']}' references non-existent node",
                            "details": {
                                "edge_id": edge["id"],
                                "source_id": edge["source_id"],
                                "target_id": edge["target_id"],
                                "edge_type": edge["type"]
                            }
                        })
        
        conn.close()
        
        click.echo()
        click.echo("=" * 80)
        if not issues:
            click.echo("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤–∞–ª–∏–¥–Ω–∞! –ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")
        else:
            click.echo(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(issues)}")
            click.echo()
            for issue in issues[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
                severity_icon = "‚ùå" if issue["severity"] == "error" else "‚ö†Ô∏è"
                click.echo(f"{severity_icon} [{issue['type']}] {issue['message']}")
        click.echo("=" * 80)
        click.echo(f"üìä –í—ã–ø–æ–ª–Ω–µ–Ω–æ –ø—Ä–æ–≤–µ—Ä–æ–∫: {', '.join(checks_performed)}")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö!")
        click.echo("=" * 80)
        click.echo(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("calculate-importance")
@click.option(
    "--record-id",
    required=True,
    help="ID –∑–∞–ø–∏—Å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö",
)
@click.option(
    "--entity-weight",
    type=float,
    default=0.1,
    help="–í–µ—Å –∑–∞ –∫–∞–∂–¥—É—é —Å—É—â–Ω–æ—Å—Ç—å",
)
@click.option(
    "--task-weight",
    type=float,
    default=0.3,
    help="–í–µ—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∑–∞–¥–∞—á–∏",
)
@click.option(
    "--length-weight",
    type=float,
    default=0.2,
    help="–í–µ—Å –∑–∞ –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏—è",
)
@click.option(
    "--search-hits-weight",
    type=float,
    default=0.4,
    help="–í–µ—Å –∑–∞ —á–∞—Å—Ç–æ—Ç—É –ø–æ–∏—Å–∫–∞",
)
def calculate_importance(record_id, db_path, entity_weight, task_weight, length_weight, search_hits_weight):
    """üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏
    
    –í—ã—á–∏—Å–ª—è–µ—Ç importance score (0.0-1.0) –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
    - –ù–∞–ª–∏—á–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
    - –ù–∞–ª–∏—á–∏—è –∑–∞–¥–∞—á/action items
    - –î–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    - –ß–∞—Å—Ç–æ—Ç—ã –ø–æ–∏—Å–∫–∞
    """
    from ..memory.importance_scoring import ImportanceScorer
    from ..memory.typed_graph import TypedGraphMemory
    import sqlite3
    
    click.echo(f"üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏: {record_id}")
    click.echo()
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–∞—Ñ –∏ scorer
        graph = TypedGraphMemory(db_path=str(db_path))
        scorer = ImportanceScorer(
            entity_weight=entity_weight,
            task_weight=task_weight,
            length_weight=length_weight,
            search_hits_weight=search_hits_weight
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø–∏—Å—å –∏–∑ –ë–î
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM nodes WHERE id = ?", (record_id,))
        node = cursor.fetchone()
        
        if not node:
            click.echo(f"‚ùå –ó–∞–ø–∏—Å—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {record_id}")
            raise click.Abort()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —É–∑–µ–ª –≤ —Å–ª–æ–≤–∞—Ä—å
        node_dict = dict(node)
        properties = json.loads(node_dict.get("properties", "{}") or "{}")
        node_dict.update(properties)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—á–∞—Å—Ç–æ—Ç–∞ –ø–æ–∏—Å–∫–∞ –∏ —Ç.–¥.)
        metadata = {
            "_search_hits": properties.get("_search_hits", 0)
        }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
        importance_score = scorer.compute_importance(node_dict, metadata)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∞–∫—Ç–æ—Ä—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        factors = {}
        entities = node_dict.get("entities") or properties.get("entities", [])
        if entities:
            factors["entities"] = min(len(entities) * entity_weight, 0.5)
        if node_dict.get("has_task") or node_dict.get("is_action_item") or properties.get("has_task") or properties.get("is_action_item"):
            factors["task"] = task_weight
        text = node_dict.get("text", "") or node_dict.get("content", "") or properties.get("content", "")
        if len(text) > 500:
            factors["length"] = length_weight
        elif len(text) > 200:
            factors["length"] = length_weight * 0.5
        if metadata.get("_search_hits", 0) > 0:
            factors["search_hits"] = min(metadata["_search_hits"] / 10.0, 1.0) * search_hits_weight
        
        conn.close()
        
        click.echo("=" * 80)
        click.echo("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏")
        click.echo("=" * 80)
        click.echo(f"üìù –ó–∞–ø–∏—Å—å: {record_id}")
        click.echo(f"‚≠ê Importance Score: {importance_score:.3f} (0.0 - 1.0)")
        click.echo()
        click.echo("üìà –§–∞–∫—Ç–æ—Ä—ã:")
        for factor, value in factors.items():
            click.echo(f"   ‚Ä¢ {factor}: {value:.3f}")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏!")
        click.echo("=" * 80)
        click.echo(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("prune-memory")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö",
)
@click.option(
    "--max-records",
    type=int,
    default=100000,
    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π",
)
@click.option(
    "--eviction-threshold",
    type=float,
    default=0.7,
    help="–ü–æ—Ä–æ–≥ eviction score –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (0.0-1.0)",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑, –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è",
)
@click.option(
    "--source",
    help="–§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
)
def prune_memory(db_path, max_records, eviction_threshold, dry_run, source):
    """üßπ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–µ–≤–∞–∂–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    
    –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å –Ω–∏–∑–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º –ë–î.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ (Importance Scoring).
    """
    from ..memory.importance_scoring import MemoryPruner, EvictionScorer
    from ..memory.typed_graph import TypedGraphMemory
    import sqlite3
    
    click.echo("üßπ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏")
    click.echo()
    
    if dry_run:
        click.echo("üî∏ –†–µ–∂–∏–º DRY RUN - –∑–∞–ø–∏—Å–∏ –Ω–µ –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã")
        click.echo()
    
    try:
        graph = TypedGraphMemory(db_path=str(db_path))
        eviction_scorer = EvictionScorer()
        pruner = MemoryPruner(
            eviction_scorer=eviction_scorer,
            max_messages=max_records,
            eviction_threshold=eviction_threshold
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        query = "SELECT * FROM nodes"
        params = []
        if source:
            query += " WHERE properties LIKE ?"
            params.append(f'%"source": "{source}"%')
        
        cursor.execute(query, params)
        nodes = cursor.fetchall()
        
        current_count = len(nodes)
        click.echo(f"üìä –¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {current_count}")
        
        if not pruner.should_prune(current_count):
            click.echo("‚úÖ –û—á–∏—Å—Ç–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞)")
            conn.close()
            return
        
        click.echo(f"‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç ({max_records}), —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ—á–∏—Å—Ç–∫–∞")
        click.echo()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —É–∑–ª—ã –≤ —Å–ª–æ–≤–∞—Ä–∏
        messages = []
        for node in nodes:
            node_dict = dict(node)
            properties = json.loads(node_dict.get("properties", "{}") or "{}")
            node_dict.update(properties)
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –µ—Å—Ç—å –ø–æ–ª–µ id –∏–ª–∏ msg_id –¥–ª—è get_eviction_candidates
            if "id" not in node_dict and "msg_id" not in node_dict:
                node_dict["id"] = node_dict.get("node_id") or node["id"]
            messages.append(node_dict)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ
        candidates = pruner.get_eviction_candidates(
            messages,
            threshold=eviction_threshold
        )
        
        click.echo(f"üéØ –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ: {len(candidates)}")
        click.echo()
        
        if not dry_run and candidates:
            click.echo("üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π...")
            removed_count = 0
            for candidate in candidates:
                try:
                    # get_eviction_candidates –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç msg_id
                    node_id = candidate.get("msg_id") or candidate.get("message", {}).get("id")
                    if node_id:
                        graph.delete_node(node_id)
                        removed_count += 1
                except Exception as e:
                    click.echo(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {node_id}: {e}")
            
            click.echo(f"   ‚úÖ –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {removed_count}")
        
        conn.close()
        
        click.echo()
        click.echo("=" * 80)
        if dry_run:
            click.echo("üî∏ DRY RUN –∑–∞–≤–µ—Ä—à—ë–Ω")
            click.echo(f"üìä –ë—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(candidates)}")
        else:
            click.echo("‚úÖ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        click.echo("=" * 80)
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –ø–∞–º—è—Ç–∏!")
        click.echo("=" * 80)
        click.echo(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("update-importance-scores")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="–ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö",
)
@click.option(
    "--source",
    help="–û–±–Ω–æ–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞",
)
@click.option(
    "--batch-size",
    type=int,
    default=1000,
    help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏",
)
def update_importance_scores(db_path, source, batch_size):
    """üîÑ –ú–∞—Å—Å–æ–≤—ã–π –ø–µ—Ä–µ—Å—á—ë—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–µ–π
    
    –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç importance scores –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.
    –ü–æ–ª–µ–∑–Ω–æ –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏.
    """
    from ..memory.importance_scoring import ImportanceScorer
    from ..memory.typed_graph import TypedGraphMemory
    import sqlite3
    
    click.echo("üîÑ –ú–∞—Å—Å–æ–≤—ã–π –ø–µ—Ä–µ—Å—á—ë—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–µ–π")
    click.echo()
    
    try:
        graph = TypedGraphMemory(db_path=str(db_path))
        scorer = ImportanceScorer()
        
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏
        query = "SELECT * FROM nodes"
        params = []
        if source:
            query += " WHERE properties LIKE ?"
            params.append(f'%"source": "{source}"%')
        
        cursor.execute(query, params)
        nodes = cursor.fetchall()
        
        total_nodes = len(nodes)
        click.echo(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_nodes}")
        click.echo()
        
        updated_count = 0
        importance_scores = []
        
        for i, node in enumerate(nodes, 1):
            try:
                node_dict = dict(node)
                properties = json.loads(node_dict.get("properties", "{}"))
                node_dict.update(properties)
                
                metadata = {
                    "_search_hits": properties.get("_search_hits", 0)
                }
                
                importance_score = scorer.compute_importance(node_dict, metadata)
                importance_scores.append(importance_score)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º properties —Å –Ω–æ–≤—ã–º importance_score
                properties["_importance_score"] = importance_score
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —É–∑–µ–ª –≤ –≥—Ä–∞—Ñ–µ
                graph.update_node(
                    node_id=node_dict["id"],
                    properties=properties
                )
                
                updated_count += 1
                
                if i % batch_size == 0:
                    click.echo(f"   ‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{total_nodes} ({i*100//total_nodes}%)")
            
            except Exception as e:
                node_id_str = node_dict.get("id", "unknown")
                click.echo(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {node_id_str}: {e}")
        
        conn.close()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        avg_importance = sum(importance_scores) / len(importance_scores) if importance_scores else 0
        min_importance = min(importance_scores) if importance_scores else 0
        max_importance = max(importance_scores) if importance_scores else 0
        
        click.echo()
        click.echo("=" * 80)
        click.echo("‚úÖ –ü–µ—Ä–µ—Å—á—ë—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à—ë–Ω")
        click.echo("=" * 80)
        click.echo(f"üìä –û–±–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {updated_count}")
        click.echo(f"‚≠ê –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å: {avg_importance:.3f}")
        click.echo(f"üìâ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å: {min_importance:.3f}")
        click.echo(f"üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å: {max_importance:.3f}")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å—á—ë—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç–∏!")
        click.echo("=" * 80)
        click.echo(f"–û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    cli()


if __name__ == "__main__":
    main()
