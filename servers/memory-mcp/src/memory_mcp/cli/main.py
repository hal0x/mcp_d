#!/usr/bin/env python3
"""CLI Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ Ð´Ð»Ñ Telegram Dump Manager v2.0."""

import asyncio
import json
import logging
import math
import os
import re
import signal
import subprocess
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import click

from ..utils.russian_tokenizer import tokenize_text as enhanced_tokenize

# ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ñ‚ÐµÐ»ÐµÐ¼ÐµÑ‚Ñ€Ð¸ÑŽ ChromaDB
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY_IMPL"] = ""

from ..analysis.insight_graph import SummaryInsightAnalyzer
from ..analysis.instruction_manager import InstructionManager
from ..core.indexer import TwoLevelIndexer
from ..indexing import TelegramIndexer
from ..memory.ingest import MemoryIngestor
from ..memory.typed_graph import TypedGraphMemory
from ..utils.message_extractor import MessageExtractor

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MessageDeduplicator:
    """ÐšÐ»Ð°ÑÑ Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ð¿Ð¾Ð»ÑŽ 'id'."""

    def __init__(self, chats_dir: str = "chats"):
        self.chats_dir = Path(chats_dir)
        self.stats = {
            "total_chats": 0,
            "processed_chats": 0,
            "total_messages": 0,
            "duplicates_removed": 0,
            "unique_messages": 0,
            "errors": 0,
        }

    def deduplicate_chat(self, chat_dir: Path) -> Dict[str, int]:
        """Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ð¾Ð´Ð½Ð¾Ð¼ Ñ‡Ð°Ñ‚Ðµ."""
        chat_stats = {
            "total_messages": 0,
            "duplicates_removed": 0,
            "unique_messages": 0,
            "errors": 0,
        }

        if not chat_dir.exists():
            return chat_stats

        all_messages = []
        for json_file in chat_dir.glob("*.json"):
            try:
                with open(json_file, encoding="utf-8") as f:
                    for line in f:
                        try:
                            message = json.loads(line.strip())
                            if isinstance(message, dict):
                                all_messages.append(message)
                        except json.JSONDecodeError:
                            continue
            except Exception as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð° {json_file}: {e}")
                chat_stats["errors"] += 1

        chat_stats["total_messages"] = len(all_messages)

        from ..utils.deduplication import deduplicate_by_id

        unique_messages = deduplicate_by_id(all_messages)
        chat_stats["duplicates_removed"] = len(all_messages) - len(unique_messages)
        chat_stats["unique_messages"] = len(unique_messages)

        if unique_messages != all_messages:
            temp_file = chat_dir / "temp_dedup.json"
            try:
                with open(temp_file, "w", encoding="utf-8") as f:
                    for message in unique_messages:
                        f.write(json.dumps(message, ensure_ascii=False) + "\n")

                for json_file in chat_dir.glob("*.json"):
                    if json_file.name != "temp_dedup.json":
                        json_file.unlink()

                final_file = chat_dir / "messages.json"
                temp_file.rename(final_file)

            except Exception as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ð¸ÑÐ¸ Ñ„Ð°Ð¹Ð»Ð° Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° {chat_dir.name}: {e}")
                chat_stats["errors"] += 1
                if temp_file.exists():
                    temp_file.unlink()

        return chat_stats

    def deduplicate_all_chats(self) -> Dict[str, int]:
        """Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð²Ð¾ Ð²ÑÐµÑ… Ñ‡Ð°Ñ‚Ð°Ñ…."""
        if not self.chats_dir.exists():
            logger.error(f"Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ {self.chats_dir} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
            return self.stats

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ñ‡Ð°Ñ‚Ð¾Ð²
        chat_dirs = [d for d in self.chats_dir.iterdir() if d.is_dir()]
        self.stats["total_chats"] = len(chat_dirs)

        for chat_dir in chat_dirs:
            logger.info(f"Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ñ‡Ð°Ñ‚Ð°: {chat_dir.name}")

            # Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð² Ñ‡Ð°Ñ‚Ðµ
            chat_stats = self.deduplicate_chat(chat_dir)

            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¾Ð±Ñ‰ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
            for key, value in chat_stats.items():
                self.stats[key] += value

            self.stats["processed_chats"] += 1

            logger.info(
                f"Ð§Ð°Ñ‚Ð° {chat_dir.name}: {chat_stats['duplicates_removed']} Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾"
            )

        return self.stats

    def print_stats(self):
        """Ð’Ñ‹Ð²Ð¾Ð´ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸."""
        print("\n" + "=" * 60)
        print("ðŸ“Š Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ Ð”Ð•Ð”Ð£ÐŸÐ›Ð˜ÐšÐÐ¦Ð˜Ð˜")
        print("=" * 60)
        print(f"ðŸ“ Ð’ÑÐµÐ³Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²: {self.stats['total_chats']}")
        print(f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²: {self.stats['processed_chats']}")
        print(f"ðŸ“¨ Ð’ÑÐµÐ³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹: {self.stats['total_messages']}")
        print(f"ðŸ”„ Ð”ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾: {self.stats['duplicates_removed']}")
        print(f"âœ¨ Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹: {self.stats['unique_messages']}")
        print(f"âŒ ÐžÑˆÐ¸Ð±Ð¾Ðº: {self.stats['errors']}")
        print("=" * 60)


class ProcessManager:
    """ÐšÐ»Ð°ÑÑ Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼Ð¸ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸."""

    @staticmethod
    def kill_processes_by_name(pattern: str) -> int:
        """ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð¿Ð¾ Ð¸Ð¼ÐµÐ½Ð¸."""
        killed_count = 0
        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)
            lines = result.stdout.split("\n")

            for line in lines:
                if pattern in line and "grep" not in line:
                    parts = line.split()
                    if len(parts) >= 2:
                        pid = parts[1]
                        try:
                            os.kill(int(pid), signal.SIGTERM)
                            killed_count += 1
                            logger.info(f"ÐŸÑ€Ð¾Ñ†ÐµÑÑ {pid} ({pattern}) Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½")
                        except (ValueError, ProcessLookupError):
                            continue
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² {pattern}: {e}")

        return killed_count

    @staticmethod
    def stop_ollama():
        """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama ÑÐµÑ€Ð²ÐµÑ€Ð°."""
        logger.info("ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ollama ÑÐµÑ€Ð²ÐµÑ€Ð°...")

        try:
            result = subprocess.run(
                ["ollama", "stop"], capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                logger.info("âœ… Ollama ÑÐµÑ€Ð²ÐµÑ€ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½")
            else:
                logger.warning("âš ï¸ Ollama stop Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð», Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ kill")
                ProcessManager.kill_processes_by_name("ollama")
        except subprocess.TimeoutExpired:
            logger.warning("âš ï¸ Timeout Ð¿Ñ€Ð¸ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ Ollama, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ kill")
            ProcessManager.kill_processes_by_name("ollama")
        except FileNotFoundError:
            logger.warning("âš ï¸ Ollama Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² PATH, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ kill")
            ProcessManager.kill_processes_by_name("ollama")
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ Ollama: {e}")

    @staticmethod
    def stop_indexing_processes():
        """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸."""
        logger.info("ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸...")

        patterns = [
            "tg_dump.py",
            "index_messages.py",
            "summarize_chats.py",
            "index_summaries.py",
            "cross_analyze.py",
            "ollama",
        ]

        total_killed = 0
        for pattern in patterns:
            killed = ProcessManager.kill_processes_by_name(pattern)
            total_killed += killed

        if total_killed > 0:
            logger.info(f"âœ… ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²: {total_killed}")
        else:
            logger.info("â„¹ï¸ ÐŸÑ€Ð¾Ñ†ÐµÑÑÑ‹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹")

    @staticmethod
    def check_remaining_processes():
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸Ñ…ÑÑ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²."""
        logger.info("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸Ñ…ÑÑ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²...")

        patterns = [
            "tg_dump.py",
            "index_messages.py",
            "summarize_chats.py",
            "index_summaries.py",
            "cross_analyze.py",
            "ollama",
        ]

        remaining = []
        try:
            result = subprocess.run(["ps", "aux"], capture_output=True, text=True)
            lines = result.stdout.split("\n")

            for line in lines:
                for pattern in patterns:
                    if pattern in line and "grep" not in line:
                        remaining.append(line.strip())
                        break
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²: {e}")

        if remaining:
            logger.warning(f"âš ï¸ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(remaining)} Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸Ñ…ÑÑ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²:")
            for proc in remaining:
                logger.warning(f"   {proc}")
        else:
            logger.info("âœ… Ð’ÑÐµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹")

    @staticmethod
    def stop_all_indexing():
        """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸."""
        logger.info("ðŸ›‘ ÐžÐ¡Ð¢ÐÐÐžÐ’ÐšÐ Ð’Ð¡Ð•Ð¥ ÐŸÐ ÐžÐ¦Ð•Ð¡Ð¡ÐžÐ’ Ð˜ÐÐ”Ð•ÐšÐ¡ÐÐ¦Ð˜Ð˜")
        logger.info("=" * 50)

        ProcessManager.stop_indexing_processes()
        ProcessManager.stop_ollama()

        import time
        time.sleep(2)

        ProcessManager.check_remaining_processes()

        logger.info("=" * 50)
        logger.info("âœ… ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°")


@click.group()
@click.version_option(version="2.0.0", prog_name="memory_mcp")
@click.option("--verbose", "-v", is_flag=True, help="ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´")
@click.option("--quiet", "-q", is_flag=True, help="Ð¢Ð¸Ñ…Ð¸Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼")
def cli(verbose, quiet):
    """ðŸš€ Telegram Dump Manager v2.0 - Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð¼Ð¿Ð°Ð¼Ð¸ Telegram Ñ‡Ð°Ñ‚Ð¾Ð²

    Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ CLI Ð´Ð»Ñ Ð´Ð²ÑƒÑ…ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Telegram Ñ‡Ð°Ñ‚Ð¾Ð².

    ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹:
      â€¢ index              - Ð”Ð²ÑƒÑ…ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ð°Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ (ÑÐµÑÑÐ¸Ð¸ + ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ + Ð·Ð°Ð´Ð°Ñ‡Ð¸)
      â€¢ ingest-telegram    - ÐŸÑ€ÑÐ¼Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ‡Ð°Ñ‚Ð¾Ð² Ð² Ð³Ñ€Ð°Ñ„ Ð¿Ð°Ð¼ÑÑ‚Ð¸
      â€¢ indexing-progress  - Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ¾Ð¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸
      â€¢ update-summaries   - ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ markdown-Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð² Ð±ÐµÐ· Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸
      â€¢ review-summaries   - ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ñ€ÐµÐ²ÑŒÑŽ Ð¸ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹
      â€¢ rebuild-vector-db  - ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð²
      â€¢ search             - ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ð¼
      â€¢ insight-graph      - ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹
      â€¢ stats              - Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
      â€¢ check              - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
      â€¢ extract-messages   - Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð¾Ð²Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¸Ð· input Ð² chats
      â€¢ deduplicate        - Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
      â€¢ stop-indexing      - ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸
      
    Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸:
      â€¢ backup-database    - Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸ (SQLite + ChromaDB)
      â€¢ restore-database   - Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð· Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸
      â€¢ optimize-database  - ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ SQLite (VACUUM, ANALYZE, REINDEX)
      â€¢ validate-database  - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…
      
    Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸:
      â€¢ calculate-importance    - Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸
      â€¢ prune-memory            - ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð½ÐµÐ²Ð°Ð¶Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
      â€¢ update-importance-scores - ÐœÐ°ÑÑÐ¾Ð²Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸
    """
    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    elif quiet:
        logging.getLogger().setLevel(logging.WARNING)
    else:
        logging.getLogger().setLevel(logging.INFO)


@cli.command("ingest-telegram")
@click.option(
    "--chats-dir",
    default="chats",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°Ð¼Ð¸ Telegram",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ñ‚Ð¸Ð¿Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚Ð¸",
)
@click.option(
    "--chat",
    "selected_chats",
    multiple=True,
    help="Ð˜Ð¼Ñ Ñ‡Ð°Ñ‚Ð° Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ñ‡Ð½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ (Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾)",
)
def ingest_telegram(chats_dir: Path, db_path: Path, selected_chats: tuple[str, ...]):
    """ðŸ“š Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Telegram Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð² Ð³Ñ€Ð°Ñ„ Ð¿Ð°Ð¼ÑÑ‚Ð¸."""

    chosen = [chat for chat in selected_chats if chat] or None
    indexer = TelegramIndexer(chats_dir=str(chats_dir), selected_chats=chosen)
    graph: TypedGraphMemory | None = None

    try:
        indexer.prepare()
        graph = TypedGraphMemory(db_path=str(db_path))
        ingestor = MemoryIngestor(graph)
        ingest_stats = ingestor.ingest(indexer.iter_records())
        index_stats = indexer.finalize()
    except Exception as exc:
        raise click.ClickException(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸ÑŽ: {exc}") from exc
    try:
        indexer.close()
    except Exception:  # pragma: no cover - best effort
        logger.debug("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ð·Ð°ÐºÑ€Ñ‹Ñ‚ÑŒ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ‚Ð¾Ñ€", exc_info=True)

    try:
        if graph is not None:
            graph.conn.close()
    except Exception:  # pragma: no cover - best effort
        logger.debug("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°ÐºÑ€Ñ‹Ñ‚ÑŒ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ñ Ð‘Ð” Ð³Ñ€Ð°Ñ„Ð°", exc_info=True)

    skipped = max(0, index_stats.records_indexed - ingest_stats.records_ingested)

    click.echo("")
    click.echo("ðŸ“¥ Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Telegram Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°")
    click.echo(f"â€¢ Ð§Ð°Ñ‚Ð¾Ð² Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾: {index_stats.sources_processed}")
    click.echo(
        f"â€¢ Ð—Ð°Ð¿Ð¸ÑÐµÐ¹ ÑÐ¾Ð·Ð´Ð°Ð½Ð¾: {ingest_stats.records_ingested} "
        f"(Ð²Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ: {ingest_stats.attachments_ingested})"
    )
    if skipped:
        click.echo(f"â€¢ ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾ Ð¸Ð·-Ð·Ð° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²: {skipped}")
    if index_stats.warnings:
        click.echo("")
        click.echo("âš ï¸  ÐŸÑ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ñ:")
        for warning in index_stats.warnings:
            click.echo(f"  - {warning}")


@cli.command()
@click.option(
    "--embedding-model", default="text-embedding-qwen3-embedding-0.6b", help="ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²"
)
def check(embedding_model):
    """ðŸ”§ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð¸ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¹"""

    async def _check():
        import chromadb

        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings

        click.echo("ðŸ”§ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹...")

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ LM Studio Server
        try:
            settings = get_settings()
            lmstudio_client = LMStudioEmbeddingClient(
                model_name=embedding_model or settings.lmstudio_model,
                llm_model_name=settings.lmstudio_llm_model,
                base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
            )
            async with lmstudio_client:
                available = await lmstudio_client.test_connection()
                if not available or not available.get("lmstudio_available", False):
                    click.echo("âŒ LM Studio Server Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")
                    click.echo(f"Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ LM Studio Server Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ð½Ð° {settings.lmstudio_host}:{settings.lmstudio_port}")
                    return False

                if not available.get("model_available", False):
                    click.echo("âŒ ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
                    click.echo(f"Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ {embedding_model or settings.lmstudio_model} Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð² LM Studio Server")
                    return False

                click.echo("âœ… Ollama Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")
        except Exception as e:
            click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐµ Ollama: {e}")
            return False

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ChromaDB ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
            collections_status = []
            try:
                sessions_collection = chroma_client.get_collection("chat_sessions")
                click.echo(
                    f"âœ… ChromaDB chat_sessions: {sessions_collection.count()} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                )
                collections_status.append(True)
            except:
                click.echo("âš ï¸  ChromaDB ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ chat_sessions Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
                collections_status.append(False)

            try:
                messages_collection = chroma_client.get_collection("chat_messages")
                click.echo(
                    f"âœ… ChromaDB chat_messages: {messages_collection.count()} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                )
                collections_status.append(True)
            except:
                click.echo("âš ï¸  ChromaDB ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ chat_messages Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
                collections_status.append(False)

            try:
                tasks_collection = chroma_client.get_collection("chat_tasks")
                click.echo(f"âœ… ChromaDB chat_tasks: {tasks_collection.count()} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
                collections_status.append(True)
            except:
                click.echo("âš ï¸  ChromaDB ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ chat_tasks Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
                collections_status.append(False)

            if not any(collections_status):
                click.echo(
                    "\nðŸ’¡ ÐŸÐ¾Ð´ÑÐºÐ°Ð·ÐºÐ°: Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ 'memory_mcp index' Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²"
                )

        except Exception as e:
            click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐµ ChromaDB: {e}")

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ„Ð°Ð¹Ð»Ñ‹
        chats_path = Path("chats")
        if chats_path.exists():
            json_files = list(chats_path.glob("**/*.json"))
            click.echo(f"âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ JSON Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(json_files)}")
        else:
            click.echo("âŒ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ chats Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        summaries_path = Path("artifacts/reports")
        if summaries_path.exists():
            md_files = list(summaries_path.glob("**/*.md"))
            click.echo(f"âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ MD Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(md_files)}")
        else:
            click.echo(
                "âš ï¸  Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ artifacts/reports Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° (Ð±ÑƒÐ´ÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ð¿Ñ€Ð¸ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸)"
            )

        click.echo("\nðŸŽ‰ Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ðµ!")
        return True

    asyncio.run(_check())


@cli.command()
@click.option(
    "--scope",
    default="all",
    type=click.Choice(["all", "chat"]),
    help="ÐžÐ±Ð»Ð°ÑÑ‚ÑŒ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸: all (Ð²ÑÐµ Ñ‡Ð°Ñ‚Ñ‹) Ð¸Ð»Ð¸ chat (Ð¾Ð´Ð¸Ð½ Ñ‡Ð°Ñ‚)",
)
@click.option("--chat", help="ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð° Ð´Ð»Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ scope=chat)")
@click.option("--force-full", is_flag=True, help="ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¿ÐµÑ€ÐµÑÐ±Ð¾Ñ€ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ°")
@click.option(
    "--recent-days", default=7, type=int, help="ÐŸÐµÑ€ÐµÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ N Ð´Ð½ÐµÐ¹"
)
@click.option(
    "--no-quality-check",
    is_flag=True,
    help="ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ (Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ)",
)
@click.option(
    "--no-improvement",
    is_flag=True,
    help="ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸",
)
@click.option(
    "--min-quality", default=90.0, type=float, help="ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð±Ð°Ð»Ð» ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° (0-100)"
)
@click.option(
    "--enable-clustering",
    is_flag=True,
    help="Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸ÑŽ ÑÐµÑÑÐ¸Ð¹ Ð´Ð»Ñ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸",
)
@click.option(
    "--clustering-threshold",
    default=0.8,
    type=float,
    help="ÐŸÐ¾Ñ€Ð¾Ð³ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð° Ð´Ð»Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ (0.0-1.0)",
)
@click.option(
    "--min-cluster-size", default=2, type=int, help="ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° ÑÐµÑÑÐ¸Ð¹"
)
@click.option(
    "--max-messages-per-group",
    default=200,
    type=int,
    help="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ð³Ñ€ÑƒÐ¿Ð¿Ðµ (Ð±Ð¾Ð»ÑŒÑˆÐµ = Ð¼ÐµÐ½ÑŒÑˆÐµ ÑÐµÑÑÐ¸Ð¹)",
)
@click.option(
    "--max-session-hours",
    default=12,
    type=int,
    help="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÑÐµÑÑÐ¸Ð¸ Ð² Ñ‡Ð°ÑÐ°Ñ… (Ð±Ð¾Ð»ÑŒÑˆÐµ = Ð¼ÐµÐ½ÑŒÑˆÐµ ÑÐµÑÑÐ¸Ð¹)",
)
@click.option(
    "--gap-minutes",
    default=120,
    type=int,
    help="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ñ€Ñ‹Ð² Ð¼ÐµÐ¶Ð´Ñƒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð² Ð¼Ð¸Ð½ÑƒÑ‚Ð°Ñ… (Ð±Ð¾Ð»ÑŒÑˆÐµ = Ð¼ÐµÐ½ÑŒÑˆÐµ ÑÐµÑÑÐ¸Ð¹)",
)
@click.option(
    "--enable-smart-aggregation",
    is_flag=True,
    help="Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ ÑƒÐ¼Ð½ÑƒÑŽ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÑƒ Ñ ÑÐºÐ¾Ð»ÑŒÐ·ÑÑ‰Ð¸Ð¼Ð¸ Ð¾ÐºÐ½Ð°Ð¼Ð¸ (NOW/FRESH/RECENT/OLD)",
)
@click.option(
    "--aggregation-strategy",
    default="smart",
    type=click.Choice(["smart", "channel", "legacy"]),
    help="Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸: smart (ÑƒÐ¼Ð½Ð°Ñ), channel (Ð´Ð»Ñ ÐºÐ°Ð½Ð°Ð»Ð¾Ð²), legacy (ÑÑ‚Ð°Ñ€Ð°Ñ)",
)
@click.option(
    "--now-window-hours",
    default=24,
    type=int,
    help="Ð Ð°Ð·Ð¼ÐµÑ€ NOW Ð¾ÐºÐ½Ð° Ð² Ñ‡Ð°ÑÐ°Ñ… (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: 24)",
)
@click.option(
    "--fresh-window-days",
    default=14,
    type=int,
    help="Ð Ð°Ð·Ð¼ÐµÑ€ FRESH Ð¾ÐºÐ½Ð° Ð² Ð´Ð½ÑÑ… (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: 14)",
)
@click.option(
    "--recent-window-days",
    default=30,
    type=int,
    help="Ð Ð°Ð·Ð¼ÐµÑ€ RECENT Ð¾ÐºÐ½Ð° Ð² Ð´Ð½ÑÑ… (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: 30)",
)
@click.option(
    "--strategy-threshold",
    default=1000,
    type=int,
    help="ÐŸÐ¾Ñ€Ð¾Ð³ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð° Ð¼ÐµÐ¶Ð´Ñƒ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑÐ¼Ð¸ (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: 1000)",
)
@click.option(
    "--force",
    is_flag=True,
    help="ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ñ‹",
)
@click.option(
    "--embedding-model", 
    default="text-embedding-qwen3-embedding-0.6b", 
    help="ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²"
)
def index(
    scope,
    chat,
    force_full,
    recent_days,
    no_quality_check,
    no_improvement,
    min_quality,
    enable_clustering,
    clustering_threshold,
    min_cluster_size,
    max_messages_per_group,
    max_session_hours,
    gap_minutes,
    enable_smart_aggregation,
    aggregation_strategy,
    now_window_hours,
    fresh_window_days,
    recent_window_days,
    strategy_threshold,
    force,
    embedding_model,
):
    """ðŸ“š Ð”Ð²ÑƒÑ…ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ð°Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ñ‡Ð°Ñ‚Ð¾Ð² (L1: ÑÐµÑÑÐ¸Ð¸ + ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸, L2: ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ, L3: Ð·Ð°Ð´Ð°Ñ‡Ð¸)

    ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ð°Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ñ ÑƒÐ¼Ð½Ð¾Ð¹ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¾Ð¹ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² ÑÐµÑÑÐ¸Ð¸,
    Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸ÐµÐ¼ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¸ Ð·Ð°Ð´Ð°Ñ‡, ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÐµÐ¼ Markdown Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð¾Ð².
    """

    async def _index():
        click.echo("=" * 80)
        click.echo("ðŸš€ Telegram Dump Manager - Ð”Ð²ÑƒÑ…ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ð°Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ v2.0")
        click.echo("=" * 80)
        click.echo()

        if scope == "chat" and not chat:
            click.echo("âŒ Ð”Ð»Ñ scope='chat' Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ ÑƒÐºÐ°Ð·Ð°Ñ‚ÑŒ --chat")
            return

        click.echo("ðŸ“¦ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ‚Ð¾Ñ€Ð°...")
        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings
        
        settings = get_settings()
        embedding_client = LMStudioEmbeddingClient(
            model_name=embedding_model or settings.lmstudio_model,
            llm_model_name=settings.lmstudio_llm_model,
            base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
        )
        chroma_path = os.getenv("MEMORY_MCP_CHROMA_PATH") or settings.chroma_path
        indexer = TwoLevelIndexer(
            chroma_path=chroma_path,
            artifacts_path=settings.artifacts_path,
            embedding_client=embedding_client,
            enable_quality_check=not no_quality_check,
            enable_iterative_refinement=not no_improvement,
            min_quality_score=min_quality,
            enable_clustering=enable_clustering,
            clustering_threshold=clustering_threshold,
            min_cluster_size=min_cluster_size,
            max_messages_per_group=max_messages_per_group,
            max_session_hours=max_session_hours,
            gap_minutes=gap_minutes,
            enable_smart_aggregation=enable_smart_aggregation,
            aggregation_strategy=aggregation_strategy,
            now_window_hours=now_window_hours,
            fresh_window_days=fresh_window_days,
            recent_window_days=recent_window_days,
            strategy_threshold=strategy_threshold,
            force=force,
        )
        click.echo("âœ… Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ‚Ð¾Ñ€ Ð³Ð¾Ñ‚Ð¾Ð²")
        click.echo()

        click.echo("âš™ï¸  ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸:")
        click.echo(f"   - Scope: {scope}")
        click.echo(f"   - Chat: {chat or 'Ð²ÑÐµ Ñ‡Ð°Ñ‚Ñ‹'}")
        click.echo(f"   - Force full rebuild: {force_full}")
        click.echo(f"   - Force artifacts: {force}")
        click.echo(f"   - Recent days resummary: {recent_days}")
        click.echo()
        click.echo("ðŸŽ¯ ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸:")
        click.echo(
            f"   - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°: {'âŒ ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°' if no_quality_check else 'âœ… Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°'}"
        )
        click.echo(
            f"   - ÐÐ²Ñ‚Ð¾ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ: {'âŒ ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾' if no_improvement else 'âœ… Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾'}"
        )
        click.echo(
            f"   - ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð±Ð°Ð»Ð»: {min_quality}/100 {'(ÑÑ‚Ñ€Ð¾Ð³Ð¸Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼)' if min_quality >= 80 else '(ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼)' if min_quality >= 60 else '(Ð¼ÑÐ³ÐºÐ¸Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼)'}"
        )
        click.echo()
        click.echo("ðŸ”— ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐµÑÑÐ¸Ð¹:")
        click.echo(
            f"   - ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ: {'âœ… Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°' if enable_clustering else 'âŒ ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°'}"
        )
        if enable_clustering:
            click.echo(f"   - ÐŸÐ¾Ñ€Ð¾Ð³ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð°: {clustering_threshold}")
            click.echo(f"   - ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°: {min_cluster_size}")
        click.echo()
        click.echo("ðŸ“Š ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸ ÑÐµÑÑÐ¸Ð¹:")
        click.echo(f"   - ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ð³Ñ€ÑƒÐ¿Ð¿Ðµ: {max_messages_per_group}")
        click.echo(f"   - ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÑÐµÑÑÐ¸Ð¸: {max_session_hours} Ñ‡Ð°ÑÐ¾Ð²")
        click.echo(f"   - ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ñ€Ñ‹Ð² Ð¼ÐµÐ¶Ð´Ñƒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸ÑÐ¼Ð¸: {gap_minutes} Ð¼Ð¸Ð½ÑƒÑ‚")
        click.echo()

        if enable_smart_aggregation:
            click.echo("ðŸ§  Ð£Ð¼Ð½Ð°Ñ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ñ ÑÐºÐ¾Ð»ÑŒÐ·ÑÑ‰Ð¸Ð¼Ð¸ Ð¾ÐºÐ½Ð°Ð¼Ð¸:")
            click.echo(f"   - Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ: {aggregation_strategy}")
            click.echo(f"   - NOW Ð¾ÐºÐ½Ð¾: {now_window_hours} Ñ‡Ð°ÑÐ¾Ð² (ÑÐµÐ³Ð¾Ð´Ð½Ñ)")
            click.echo(f"   - FRESH Ð¾ÐºÐ½Ð¾: {fresh_window_days} Ð´Ð½ÐµÐ¹ (Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾)")
            click.echo(f"   - RECENT Ð¾ÐºÐ½Ð¾: {recent_window_days} Ð´Ð½ÐµÐ¹ (Ð¿Ð¾ Ð½ÐµÐ´ÐµÐ»ÑÐ¼)")
            click.echo(f"   - OLD Ð¾ÐºÐ½Ð¾: >{recent_window_days} Ð´Ð½ÐµÐ¹ (Ð¿Ð¾ Ð¼ÐµÑÑÑ†Ð°Ð¼)")
            click.echo(f"   - ÐŸÐ¾Ñ€Ð¾Ð³ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð° ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹: {strategy_threshold} ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹")
            click.echo("   - ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð°Ñ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ NOW Ð¾ÐºÐ½Ð°")
            click.echo("   - ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ðº Ollama")
        else:
            click.echo("ðŸ“Š ÐšÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ¸:")
            click.echo("   - ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ Ð´Ð½ÑÐ¼")
            click.echo("   - 10-100 ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ð³Ñ€ÑƒÐ¿Ð¿Ðµ")
            click.echo("   - Ð•ÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ñ€Ð°Ð·Ñ€Ñ‹Ð²Ñ‹ Ð² Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑÑ… (>4 Ñ‡Ð°ÑÐ¾Ð²)")
            click.echo("   - Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿ÑƒÑÑ‚Ñ‹Ñ… Ð¸ ÑÐµÑ€Ð²Ð¸ÑÐ½Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹")
            click.echo("   - Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹")
            click.echo("   - ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ñ… Ð³Ñ€ÑƒÐ¿Ð¿")
        click.echo()

        click.echo("ðŸ”„ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸...")
        click.echo()

        try:
            stats = await indexer.build_index(
                scope=scope, chat=chat, force_full=force_full, recent_days=recent_days
            )

            click.echo()
            click.echo("=" * 80)
            click.echo("âœ… Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")
            click.echo("=" * 80)
            click.echo()
            click.echo("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:")
            click.echo(f"   - ÐŸÑ€Ð¾Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²: {len(stats['indexed_chats'])}")
            click.echo(f"   - Ð¡ÐµÑÑÐ¸Ð¹ (L1): {stats['sessions_indexed']}")
            click.echo(f"   - Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ (L2): {stats['messages_indexed']}")
            click.echo(f"   - Ð—Ð°Ð´Ð°Ñ‡ (L3): {stats['tasks_indexed']}")
            click.echo()

            if stats["indexed_chats"]:
                click.echo("ðŸ“ ÐŸÑ€Ð¾Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ñ‡Ð°Ñ‚Ñ‹:")
                for chat_name in stats["indexed_chats"]:
                    click.echo(f"   - {chat_name}")
                click.echo()

            click.echo("ðŸ“‚ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²:")
            click.echo("   - Markdown Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹: ./artifacts/reports/")
            click.echo("   - Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð±Ð°Ð·Ð°: ./chroma_db/")
            click.echo("   - ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸: chat_sessions, chat_messages, chat_tasks")
            click.echo()

        except Exception as e:
            click.echo()
            click.echo("=" * 80)
            click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸!")
            click.echo("=" * 80)
            click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
            click.echo()
            import traceback

            traceback.print_exc()

    asyncio.run(_index())


@cli.command("set-instruction")
@click.option(
    "--chat", help="ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð° (ÐºÐ°Ðº Ð¿Ð°Ð¿ÐºÐ° Ð² chats/) Ð´Ð»Ñ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸"
)
@click.option(
    "--mode",
    type=click.Choice(["group", "channel"]),
    help="ÐžÐ±Ñ‰Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð²ÑÐµÑ… Ñ‡Ð°Ñ‚Ð¾Ð² Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ‚Ð¸Ð¿Ð°",
)
@click.option("--text", help="Ð¢ÐµÐºÑÑ‚ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð¿Ñ€ÑÐ¼Ð¾ Ð² Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ðµ")
@click.option(
    "--file",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÐµÐ¹",
)
@click.option(
    "--clear",
    is_flag=True,
    help="Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½Ð½ÑƒÑŽ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑŽ Ð´Ð»Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð° Ð¸Ð»Ð¸ Ñ‚Ð¸Ð¿Ð°",
)
def set_instruction(chat, mode, text, file, clear):
    """ðŸ“ Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¸Ð»Ð¸ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑŽ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸."""
    target_count = sum(1 for value in (chat, mode) if value)
    if target_count != 1:
        raise click.UsageError(
            "ÐÑƒÐ¶Ð½Ð¾ ÑƒÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ€Ð¾Ð²Ð½Ð¾ Ð¾Ð´Ð¸Ð½ Ð¸Ð· Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²: --chat Ð¸Ð»Ð¸ --mode"
        )

    manager = InstructionManager()

    if clear:
        if chat:
            manager.clear_chat_instruction(chat)
            click.echo(f"ðŸ—‘ï¸ Ð˜Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ '{chat}' ÑƒÐ´Ð°Ð»ÐµÐ½Ð°")
        else:
            manager.clear_mode_instruction(mode)
            click.echo(f"ðŸ—‘ï¸ ÐžÐ±Ñ‰Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ñ‚Ð¸Ð¿Ð° '{mode}' Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð°")
        return

    instruction_text = text or ""
    if file:
        instruction_text = file.read_text(encoding="utf-8")
    if not instruction_text.strip():
        raise click.UsageError(
            "ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· --text Ð¸Ð»Ð¸ --file (Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ --clear)."
        )

    if chat:
        manager.set_chat_instruction(chat, instruction_text)
        click.echo(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° '{chat}'")
    else:
        manager.set_mode_instruction(mode, instruction_text)
        click.echo(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° Ð¾Ð±Ñ‰Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ñ‚Ð¸Ð¿Ð° '{mode}'")


@cli.command("list-instructions")
def list_instructions():
    """ðŸ“‹ ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½Ð½Ñ‹Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸."""
    manager = InstructionManager()
    data = manager.export()

    click.echo("ðŸ“Œ Ð˜Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð¿Ð¾ Ñ‡Ð°Ñ‚Ð°Ð¼:")
    if data["chats"]:
        for name, instruction in sorted(data["chats"].items()):
            preview = instruction.strip().replace("\n", " ")
            if len(preview) > 120:
                preview = preview[:117] + "..."
            click.echo(f"  â€¢ {name}: {preview}")
    else:
        click.echo("  (ÐÐµÑ‚ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¹)")

    click.echo("\nðŸ“Œ Ð˜Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð¿Ð¾ Ñ‚Ð¸Ð¿Ð°Ð¼ Ñ‡Ð°Ñ‚Ð¾Ð²:")
    for mode in ("group", "channel"):
        instruction = data["modes"].get(mode, "").strip()
        if instruction:
            preview = instruction.replace("\n", " ")
            if len(preview) > 120:
                preview = preview[:117] + "..."
            click.echo(f"  â€¢ {mode}: {preview}")
        else:
            click.echo(f"  â€¢ {mode}: (Ð½Ðµ Ð·Ð°Ð´Ð°Ð½Ð¾)")


def highlight_text(text: str, query: str) -> str:
    """ÐŸÐ¾Ð´ÑÐ²ÐµÑ‚ÐºÐ° Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð¾Ð² Ð² Ñ‚ÐµÐºÑÑ‚Ðµ."""
    keywords = [
        word.strip().lower() for word in query.split() if len(word.strip()) >= 3
    ]

    if not keywords:
        return text

    result = text
    for keyword in keywords:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        result = pattern.sub(
            lambda m: click.style(m.group(0), fg="yellow", bold=True), result
        )

    return result


TOKEN_PATTERN = re.compile(r"\w+", re.UNICODE)
MIN_TOKEN_LENGTH = 3

HYBRID_WEIGHTS = {
    "messages": (0.65, 0.35),
    "sessions": (0.6, 0.4),
    "tasks": (0.6, 0.4),
}

RELEVANCE_THRESHOLDS = {
    "messages": 0.32,
    "sessions": 0.30,
    "tasks": 0.28,
}


def _tokenize(text: str) -> list[str]:
    """Ð¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ñ fallback Ð½Ð° Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ."""
    if not text:
        return []

    try:
        return enhanced_tokenize(text)
    except Exception as e:
        logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð¾Ð¹ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ fallback: {e}")
        return [
            token
            for token in TOKEN_PATTERN.findall(text.lower())
            if len(token) >= MIN_TOKEN_LENGTH
        ]


def _bm25_scores(
    query_tokens: list[str], documents_tokens: list[list[str]]
) -> list[float]:
    """Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ BM25 Ð´Ð»Ñ ÐºÐ¾Ñ€Ð¿ÑƒÑÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²."""
    if not query_tokens or not documents_tokens:
        return [0.0] * len(documents_tokens)

    num_docs = len(documents_tokens)
    doc_freq = Counter()
    doc_lengths = []
    for tokens in documents_tokens:
        unique_tokens = set(tokens)
        if unique_tokens:
            doc_freq.update(unique_tokens)
        doc_lengths.append(len(tokens))

    avgdl = sum(doc_lengths) / num_docs if num_docs else 0
    if avgdl == 0:
        return [0.0] * len(documents_tokens)

    idf = {}
    for token, freq in doc_freq.items():
        idf[token] = math.log(((num_docs - freq + 0.5) / (freq + 0.5)) + 1.0)

    scores = []
    k1, b = 1.5, 0.75  # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ BM25
    for tokens, doc_len in zip(documents_tokens, doc_lengths):
        if not tokens:
            scores.append(0.0)
            continue

        term_freq = Counter(tokens)
        score = 0.0
        for token in query_tokens:
            token_idf = idf.get(token)
            tf = term_freq.get(token)
            if not token_idf or not tf:
                continue
            denom = tf + k1 * (1 - b + b * (doc_len / avgdl))
            score += token_idf * (tf * (k1 + 1) / denom)

        scores.append(score)

    return scores


@cli.command()
@click.argument("query")
@click.option("--limit", "-l", default=10, help="Ð›Ð¸Ð¼Ð¸Ñ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²")
@click.option(
    "--collection",
    "-c",
    type=click.Choice(["messages", "sessions", "tasks"]),
    default="messages",
    help="ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°",
)
@click.option("--chat", help="Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ñ‡Ð°Ñ‚Ñƒ (Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ñ‡Ð°Ñ‚Ð°)")
@click.option(
    "--highlight/--no-highlight", default=True, help="ÐŸÐ¾Ð´ÑÐ²ÐµÑ‚ÐºÐ° Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð¾Ð²"
)
@click.option(
    "--embedding-model", 
    default="text-embedding-qwen3-embedding-0.6b", 
    help="ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²"
)
def search(query, limit, collection, chat, highlight, embedding_model):
    """ðŸ” ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ð¼

    ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ñ‚Ñ€Ñ‘Ð¼ ÑƒÑ€Ð¾Ð²Ð½ÑÐ¼:
    - messages: ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸ÑÐ¼
    - sessions: ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼ ÑÐµÑÑÐ¸Ð¹
    - tasks: ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð·Ð°Ð´Ð°Ñ‡Ð°Ð¼ (Action Items)
    """

    async def _search():
        import chromadb

        from ..core.lmstudio_client import LMStudioEmbeddingClient
        from ..config import get_settings

        click.echo(f"ðŸ” ÐŸÐ¾Ð¸ÑÐº Ð² ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ '{collection}': '{query}'")
        if chat:
            click.echo(f"ðŸ“‹ Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ñ‡Ð°Ñ‚Ñƒ: '{chat}'")

        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")
            settings = get_settings()
            embedding_client = LMStudioEmbeddingClient(
                model_name=embedding_model or settings.lmstudio_model,
                base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
            )

            collection_name = f"chat_{collection}"
            try:
                coll = chroma_client.get_collection(collection_name)
            except:
                click.echo(f"âŒ ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ {collection_name} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
                click.echo("ðŸ’¡ Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ 'memory_mcp index' Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²")
                return

            async with embedding_client:
                query_embedding = await embedding_client._generate_single_embedding(query)

                if not query_embedding:
                    click.echo("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°")
                    return

                where_filter = {"chat": chat} if chat else None
                vector_limit = max(limit * 4, 20)
                results = coll.query(
                    query_embeddings=[query_embedding],
                    n_results=vector_limit,
                    where=where_filter,
                )

                documents = results.get("documents")
                if not documents or not documents[0]:
                    click.echo("âŒ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹")
                    return

                raw_ids = results.get("ids") or [[]]
                raw_ids = raw_ids[0] if raw_ids else []
                metadatas = results.get("metadatas", [[]])[0]
                distances = results.get("distances", [[]])[0]

                def resolve_doc_id(raw_id, metadata, doc_text):
                    if raw_id:
                        return raw_id
                    metadata = metadata or {}
                    for key in ("msg_id", "session_id", "task_id", "id"):
                        value = metadata.get(key)
                        if value:
                            return value
                    return f"doc-{abs(hash((doc_text or '')[:80]))}"

                vector_scores: dict[str, float] = {}
                vector_distances: dict[str, float] = {}
                vector_candidates = []

                for doc, metadata, distance, raw_id in zip(
                    documents[0], metadatas, distances, raw_ids
                ):
                    if not doc:
                        continue
                    doc_id = resolve_doc_id(raw_id, metadata, doc)
                    vector_candidates.append(
                        {
                            "id": doc_id,
                            "doc": doc,
                            "metadata": metadata or {},
                            "distance": distance,
                        }
                    )
                    vector_distances[doc_id] = distance

                if not vector_candidates:
                    click.echo("âŒ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹")
                    return

                available_distances = [
                    item["distance"]
                    for item in vector_candidates
                    if item.get("distance") is not None
                ]
                if available_distances:
                    min_distance = min(available_distances)
                    max_distance = max(available_distances)
                    denominator = max(max_distance - min_distance, 1e-6)
                    for item in vector_candidates:
                        doc_id = item["id"]
                        distance = item.get("distance")
                        if distance is None:
                            continue
                        if max_distance == min_distance:
                            vector_scores[doc_id] = 1.0
                        else:
                            score = (max_distance - distance) / denominator
                            vector_scores[doc_id] = max(score, 0.0)

                get_kwargs = {"include": ["documents", "metadatas"]}
                if where_filter:
                    get_kwargs["where"] = where_filter

                corpus = coll.get(**get_kwargs)
                corpus_docs = corpus.get("documents", [])
                corpus_meta = corpus.get("metadatas", [])

                doc_store: dict[str, dict[str, object]] = {}
                lexical_entries: list[str] = []
                lexical_tokens: list[list[str]] = []

                for idx, (doc_text, metadata) in enumerate(
                    zip(corpus_docs, corpus_meta)
                ):
                    raw_id = f"doc_{idx}_{hash(doc_text or '')}"
                    resolved_id = resolve_doc_id(raw_id, metadata, doc_text)
                    doc_store[resolved_id] = {
                        "doc": doc_text or "",
                        "metadata": metadata or {},
                    }
                    lexical_entries.append(resolved_id)
                    lexical_tokens.append(_tokenize(doc_text or ""))

                query_tokens = _tokenize(query)
                lexical_scores_list = _bm25_scores(query_tokens, lexical_tokens)
                lexical_scores = dict(zip(lexical_entries, lexical_scores_list))
                max_lexical_score = (
                    max(lexical_scores.values()) if lexical_scores else 0.0
                )
                lexical_norm = {
                    doc_id: (score / max_lexical_score)
                    if max_lexical_score > 0
                    else 0.0
                    for doc_id, score in lexical_scores.items()
                }

                weight_vector, weight_lexical = HYBRID_WEIGHTS.get(
                    collection, (0.6, 0.4)
                )
                if not query_tokens or max_lexical_score == 0:
                    weight_vector, weight_lexical = 1.0, 0.0
                weight_sum = weight_vector + weight_lexical
                if weight_sum == 0:
                    weight_vector, weight_lexical = 1.0, 0.0
                else:
                    weight_vector /= weight_sum
                    weight_lexical /= weight_sum

                candidate_ids = set(vector_scores.keys())
                if lexical_scores:
                    sorted_lexical = sorted(
                        lexical_scores.items(), key=lambda item: item[1], reverse=True
                    )
                    top_lexical = [
                        doc_id for doc_id, score in sorted_lexical if score > 0
                    ][: max(limit * 3, 15)]
                    candidate_ids.update(top_lexical)

                final_candidates = []
                for doc_id in candidate_ids:
                    payload = doc_store.get(doc_id)
                    if not payload:
                        continue
                    vector_component = vector_scores.get(doc_id, 0.0)
                    lexical_component = lexical_norm.get(doc_id, 0.0)
                    hybrid_score = (
                        vector_component * weight_vector
                        + lexical_component * weight_lexical
                    )
                    final_candidates.append(
                        {
                            "id": doc_id,
                            "doc": payload["doc"],
                            "metadata": payload["metadata"],
                            "score": hybrid_score,
                            "vector_component": vector_component,
                            "lexical_component": lexical_component,
                            "vector_distance": vector_distances.get(doc_id),
                        }
                    )

                if not final_candidates:
                    click.echo("âŒ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹")
                    return

                final_candidates.sort(key=lambda item: item["score"], reverse=True)

                threshold = RELEVANCE_THRESHOLDS.get(collection, 0.0)
                filtered_candidates = [
                    candidate
                    for candidate in final_candidates
                    if candidate["score"] >= threshold
                ]
                filtered_out = len(final_candidates) - len(filtered_candidates)

                if not filtered_candidates:
                    filtered_candidates = final_candidates[:limit]
                    filtered_out = 0
                else:
                    filtered_candidates = filtered_candidates[:limit]

                click.echo(f"âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²: {len(filtered_candidates)}")
                if filtered_out > 0:
                    click.echo(f"   (Ð¾Ñ‚ÑÐµÑ‡ÐµÐ½Ð¾ Ð¿Ð¾ Ð¿Ð¾Ñ€Ð¾Ð³Ñƒ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚Ð¸: {filtered_out})")
                click.echo()

                for index, candidate in enumerate(filtered_candidates, 1):
                    metadata = candidate.get("metadata") or {}
                    chat_name = metadata.get(
                        "chat", metadata.get("chat_name", "Unknown")
                    )
                    signal_parts = []
                    if candidate.get("vector_component", 0) > 0:
                        signal_parts.append("vec")
                    if candidate.get("lexical_component", 0) > 0:
                        signal_parts.append("lex")
                    signals = "+".join(signal_parts) if signal_parts else "-"
                    header = f"{index}. {chat_name} (score: {candidate['score'] * 100:.1f} | signals: {signals}"
                    distance = candidate.get("vector_distance")
                    if distance is not None:
                        header += f" | distance: {distance:.1f}"
                    header += ")"
                    click.echo(header)

                    doc_text = candidate.get("doc") or ""

                    if collection == "messages":
                        text = (
                            doc_text[:200] + "..." if len(doc_text) > 200 else doc_text
                        )
                        if highlight:
                            text = highlight_text(text, query)
                        click.echo(f"   {text}")
                    elif collection == "sessions":
                        session_id = metadata.get("session_id", "N/A")
                        time_range = metadata.get("time_span", "N/A")
                        click.echo(f"   Session: {session_id}")
                        click.echo(f"   Time: {time_range}")
                        summary = (
                            doc_text[:150] + "..." if len(doc_text) > 150 else doc_text
                        )
                        if highlight:
                            summary = highlight_text(summary, query)
                        click.echo(f"   Summary: {summary}")
                    elif collection == "tasks":
                        task_text = (
                            doc_text[:200] + "..." if len(doc_text) > 200 else doc_text
                        )
                        if highlight:
                            task_text = highlight_text(task_text, query)
                        owner = metadata.get("owner", "N/A")
                        due_date = metadata.get("due", "N/A")
                        priority = metadata.get("priority", "N/A")
                        click.echo(f"   Task: {task_text}")
                        click.echo(
                            f"   Owner: {owner} | Due: {due_date} | Priority: {priority}"
                        )

                    click.echo()

        except Exception as e:
            click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¸ÑÐºÐµ: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(_search())


@cli.command()
@click.option(
    "--threshold", default=0.76, type=float, help="ÐŸÐ¾Ñ€Ð¾Ð³ ÑÑ…Ð¾Ð¶ÐµÑÑ‚Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ Ñ‡Ð°Ñ‚Ð°Ð¼Ð¸"
)
@click.option("--graphml", type=click.Path(), help="ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ GraphML-Ñ„Ð°Ð¹Ð»Ð°")
def insight_graph(threshold, graphml):
    """ðŸ§  ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹

    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð³Ñ€Ð°Ñ„ ÑÐ²ÑÐ·ÐµÐ¹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹, Ð²Ñ‹Ð´ÐµÐ»ÑÑ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¸Ð½ÑÐ°Ð¹Ñ‚Ñ‹
    Ð¸ ÑÐ²ÑÐ·Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ Ñ‡Ð°Ñ‚Ð°Ð¼Ð¸.
    """

    async def _run():
        click.echo("ðŸ§  ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð° Ð¸Ð½ÑÐ°Ð¹Ñ‚Ð¾Ð²...")
        click.echo(f"   ÐŸÐ¾Ñ€Ð¾Ð³ ÑÑ…Ð¾Ð¶ÐµÑÑ‚Ð¸: {threshold}")
        click.echo()

        analyzer = SummaryInsightAnalyzer(
            summaries_dir=Path("artifacts/reports"),
            similarity_threshold=threshold,
        )

        try:
            # Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð³Ñ€Ð°Ñ„
            async with analyzer:
                result = await analyzer.analyze()

            # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚
            click.echo("\n" + "=" * 80)
            click.echo("âœ… Ð“Ñ€Ð°Ñ„ Ð¸Ð½ÑÐ°Ð¹Ñ‚Ð¾Ð² Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½!")
            click.echo("=" * 80)
            click.echo()

            graph_metrics = result.metrics.get("graph", {})
            click.echo("ðŸ“Š ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð³Ñ€Ð°Ñ„Ð°:")
            click.echo(f"   - Ð£Ð·Ð»Ð¾Ð² (Ñ‡Ð°Ñ‚Ð¾Ð²): {graph_metrics.get('nodes', 0)}")
            click.echo(f"   - Ð Ñ‘Ð±ÐµÑ€ (ÑÐ²ÑÐ·ÐµÐ¹): {graph_metrics.get('edges', 0)}")
            click.echo(f"   - ÐšÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²: {graph_metrics.get('components', 0)}")
            click.echo(f"   - ÐŸÐ»Ð¾Ñ‚Ð½Ð¾ÑÑ‚ÑŒ Ð³Ñ€Ð°Ñ„Ð°: {graph_metrics.get('density', 0.0):.3f}")
            click.echo()

            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚
            report_path = Path("insight_graph_report.md")
            report_content = analyzer.generate_report(result)
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)
            click.echo(f"ðŸ“„ ÐžÑ‚Ñ‡Ñ‘Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½: {report_path}")

            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ GraphML ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ
            if graphml:
                export_path = analyzer.export_graphml(result, Path(graphml))
                if export_path:
                    click.echo(f"ðŸ“ GraphML ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½: {export_path}")

        except Exception as e:
            click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ð¸ Ð³Ñ€Ð°Ñ„Ð°: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(_run())


@cli.command()
def stats():
    """ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹"""

    async def _stats():
        import chromadb

        click.echo("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹...")
        click.echo()

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ChromaDB ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
        try:
            chroma_client = chromadb.PersistentClient(path="./chroma_db")

            # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑÐ¼
            total_records = 0
            for coll_name in ["chat_sessions", "chat_messages", "chat_tasks"]:
                try:
                    coll = chroma_client.get_collection(coll_name)
                    count = coll.count()
                    total_records += count
                    icon = "âœ…" if count > 0 else "âš ï¸ "
                    click.echo(f"{icon} {coll_name}: {count} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
                except:
                    click.echo(f"âŒ {coll_name}: Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")

            click.echo()
            click.echo(f"ðŸ“¦ Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ…: {total_records}")

        except Exception as e:
            click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐµ ChromaDB: {e}")

        click.echo()

        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ñ„Ð°Ð¹Ð»Ð°Ð¼
        chats_path = Path("chats")
        if chats_path.exists():
            json_files = list(chats_path.glob("**/*.json"))
            click.echo(f"ðŸ“ JSON Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(json_files)}")

            # ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²
            chat_dirs = [d for d in chats_path.iterdir() if d.is_dir()]
            click.echo(f"ðŸ’¬ Ð§Ð°Ñ‚Ð¾Ð²: {len(chat_dirs)}")
        else:
            click.echo("ðŸ“ JSON Ñ„Ð°Ð¹Ð»Ð¾Ð²: 0")

        # Markdown Ñ„Ð°Ð¹Ð»Ñ‹
        summaries_path = Path("artifacts/reports")
        if summaries_path.exists():
            md_files = list(summaries_path.glob("**/*.md"))
            session_files = list(summaries_path.glob("**/sessions/*.md"))
            click.echo(f"ðŸ“„ MD Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(md_files)}")
            click.echo(f"ðŸ“ Ð¡Ð°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹ ÑÐµÑÑÐ¸Ð¹: {len(session_files)}")
        else:
            click.echo("ðŸ“„ MD Ñ„Ð°Ð¹Ð»Ð¾Ð²: 0")

    asyncio.run(_stats())


@cli.command("indexing-progress")
@click.option("--chat", help="ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð°")
@click.option(
    "--reset",
    is_flag=True,
    help="Ð¡Ð±Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ (Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸)",
)
def indexing_progress(chat, reset):
    """ðŸ”„ Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ¾Ð¼ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸

    ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð°
    Ð¸Ð»Ð¸ ÑÐ±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸.
    """

    import chromadb

    try:
        chroma_client = chromadb.PersistentClient(path="./chroma_db")

        try:
            progress_collection = chroma_client.get_collection("indexing_progress")
        except:
            click.echo("âš ï¸  ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ indexing_progress Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
            click.echo("ðŸ’¡ Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ÐµÑ‰Ñ‘ Ð½Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ð»Ð°ÑÑŒ Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÑÑ‚Ð°Ñ€Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ")
            return

        if reset:
            if chat:
                # Ð¡Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð°
                from ..utils.naming import slugify

                progress_id = f"progress_{slugify(chat)}"
                try:
                    progress_collection.delete(ids=[progress_id])
                    click.echo(f"âœ… ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° '{chat}' ÑÐ±Ñ€Ð¾ÑˆÐµÐ½")
                    click.echo(
                        "ðŸ’¡ ÐŸÑ€Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ Ñ‡Ð°Ñ‚ Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ñ€Ð¾Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½ Ð·Ð°Ð½Ð¾Ð²Ð¾"
                    )
                except Exception as e:
                    click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ±Ñ€Ð¾ÑÐµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°: {e}")
            else:
                # Ð¡Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÐ¼ Ð²ÐµÑÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ
                try:
                    result = progress_collection.get()
                    if result["ids"]:
                        progress_collection.delete(ids=result["ids"])
                        click.echo(
                            f"âœ… ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ ÑÐ±Ñ€Ð¾ÑˆÐµÐ½ Ð´Ð»Ñ {len(result['ids'])} Ñ‡Ð°Ñ‚Ð¾Ð²"
                        )
                        click.echo(
                            "ðŸ’¡ ÐŸÑ€Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ Ð²ÑÐµ Ñ‡Ð°Ñ‚Ñ‹ Ð±ÑƒÐ´ÑƒÑ‚ Ð¿Ñ€Ð¾Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð·Ð°Ð½Ð¾Ð²Ð¾"
                        )
                    else:
                        click.echo("âš ï¸  ÐÐµÑ‚ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸")
                except Exception as e:
                    click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ±Ñ€Ð¾ÑÐµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°: {e}")
        else:
            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ
            click.echo("ðŸ”„ ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸:")
            click.echo()

            try:
                if chat:
                    # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð°
                    from ..utils.naming import slugify

                    progress_id = f"progress_{slugify(chat)}"
                    result = progress_collection.get(
                        ids=[progress_id], include=["metadatas"]
                    )

                    if result["ids"]:
                        metadata = result["metadatas"][0]
                        click.echo(f"ðŸ“‹ Ð§Ð°Ñ‚: {metadata.get('chat_name', chat)}")
                        click.echo(
                            f"   ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ: {metadata.get('last_indexed_date', 'N/A')}"
                        )
                        click.echo(
                            f"   ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ: {metadata.get('last_indexing_time', 'N/A')}"
                        )
                        click.echo(
                            f"   Ð’ÑÐµÐ³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹: {metadata.get('total_messages', 0)}"
                        )
                        click.echo(
                            f"   Ð’ÑÐµÐ³Ð¾ ÑÐµÑÑÐ¸Ð¹: {metadata.get('total_sessions', 0)}"
                        )
                    else:
                        click.echo(f"âš ï¸  ÐÐµÑ‚ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ Ð´Ð»Ñ Ñ‡Ð°Ñ‚Ð° '{chat}'")
                else:
                    # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð´Ð»Ñ Ð²ÑÐµÑ… Ñ‡Ð°Ñ‚Ð¾Ð²
                    result = progress_collection.get(include=["metadatas"])

                    if result["ids"]:
                        click.echo(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {len(result['ids'])}")
                        click.echo()

                        for i, metadata in enumerate(result["metadatas"], 1):
                            chat_name = metadata.get("chat_name", "Unknown")
                            last_date = metadata.get("last_indexed_date", "N/A")
                            last_time = metadata.get("last_indexing_time", "N/A")
                            total_msgs = metadata.get("total_messages", 0)
                            total_sessions = metadata.get("total_sessions", 0)

                            click.echo(f"{i}. {chat_name}")
                            click.echo(f"   ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ: {last_date}")
                            click.echo(f"   ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ: {last_time}")
                            click.echo(
                                f"   Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹: {total_msgs}, Ð¡ÐµÑÑÐ¸Ð¹: {total_sessions}"
                            )
                            click.echo()
                    else:
                        click.echo("âš ï¸  ÐÐµÑ‚ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸")
                        click.echo("ðŸ’¡ Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸ÑŽ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹: memory_mcp index")
            except Exception as e:
                click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°: {e}")
                import traceback

                traceback.print_exc()

    except Exception as e:
        click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¸ Ðº ChromaDB: {e}")


@cli.command("update-summaries")
@click.option("--chat", help="ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ñ‡Ð°Ñ‚Ð°")
@click.option(
    "--force",
    is_flag=True,
    help="ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ñ‹",
)
def update_summaries(chat, force):
    """ðŸ“ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ markdown-Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð² Ð±ÐµÐ· Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸

    Ð§Ð¸Ñ‚Ð°ÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ JSON-ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°ÐµÑ‚ markdown-Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹,
    Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ñ€Ð°Ð·Ð´ÐµÐ» "ÐÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾ Ð·Ð° 30 Ð´Ð½ÐµÐ¹".
    """
    import json
    from datetime import datetime, timedelta
    from zoneinfo import ZoneInfo

    from ..analysis.markdown_renderer import MarkdownRenderer

    async def _update_summaries():
        click.echo("ðŸ“ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ markdown-Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²...")
        click.echo()

        reports_dir = Path("artifacts/reports")

        if not reports_dir.exists():
            click.echo("âŒ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ artifacts/reports Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
            click.echo("ðŸ’¡ Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸ÑŽ: memory_mcp index")
            return

        # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ñ‡Ð°Ñ‚Ñ‹ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
        if chat:
            chat_dirs = [reports_dir / chat] if (reports_dir / chat).exists() else []
            if not chat_dirs:
                click.echo(f"âŒ Ð§Ð°Ñ‚ '{chat}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² artifacts/reports/")
                return
        else:
            chat_dirs = [
                d
                for d in reports_dir.iterdir()
                if d.is_dir() and (d / "sessions").exists()
            ]

        if not chat_dirs:
            click.echo("âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð² Ñ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸ÑÐ¼Ð¸")
            return

        click.echo(f"ðŸ“ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²: {len(chat_dirs)}")
        click.echo()

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ renderer
        renderer = MarkdownRenderer(output_dir=reports_dir)

        def parse_message_time(date_str: str) -> datetime:
            try:
                from ..utils.datetime_utils import parse_datetime_utc

                return parse_datetime_utc(date_str, default=datetime.now(ZoneInfo("UTC")), use_zoneinfo=True)
            except Exception:
                return datetime.now(ZoneInfo("UTC"))

        def load_session_summaries(chat_dir: Path) -> list:
            sessions = []
            sessions_dir = chat_dir / "sessions"
            if not sessions_dir.exists():
                return sessions

            json_files = list(sessions_dir.glob("*.json"))
            for json_file in json_files:
                try:
                    with open(json_file, encoding="utf-8") as f:
                        session = json.load(f)
                        sessions.append(session)
                except Exception as e:
                    click.echo(f"âš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ {json_file.name}: {e}")
                    continue
            return sessions

        # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ñ‡Ð°Ñ‚
        updated = 0

        for chat_dir in chat_dirs:
            chat_name = chat_dir.name.replace("_", " ").title()
            click.echo(f"ðŸ“‹ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‡Ð°Ñ‚Ð°: {chat_name}")

            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
            sessions = load_session_summaries(chat_dir)

            if not sessions:
                click.echo("   âš ï¸  ÐÐµÑ‚ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹ Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ")
                continue

            click.echo(f"   ðŸ“Š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹: {len(sessions)}")

            # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ ÑÐµÑÑÐ¸Ð¸ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 30 Ð´Ð½ÐµÐ¹
            now = datetime.now(ZoneInfo("UTC"))
            thirty_days_ago = now - timedelta(days=30)

            recent_sessions = []
            for session in sessions:
                end_time_str = session.get("meta", {}).get("end_time_utc", "")
                if end_time_str:
                    end_time = parse_message_time(end_time_str)
                    if end_time >= thirty_days_ago:
                        recent_sessions.append(session)

            click.echo(f"   ðŸ“… Ð¡ÐµÑÑÐ¸Ð¹ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 30 Ð´Ð½ÐµÐ¹: {len(recent_sessions)}")

            # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ñƒ
            top_sessions = sorted(
                recent_sessions,
                key=lambda s: s.get("quality", {}).get("score", 0),
                reverse=True,
            )

            # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹
            try:
                renderer.render_chat_summary(
                    chat_name, sessions, top_sessions=top_sessions, force=force
                )
                renderer.render_cumulative_context(chat_name, sessions, force=force)
                renderer.render_chat_index(chat_name, sessions, force=force)
                click.echo("   âœ… ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹ Ð²ÑÐµ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹")
                updated += 1
            except Exception as e:
                click.echo(f"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¸: {e}")

        # Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        click.echo("=" * 80)
        click.echo(f"ðŸ“Š ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²: {updated}")
        click.echo("ðŸ“‚ ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð½Ð°Ñ…Ð¾Ð´ÑÑ‚ÑÑ Ð²: ./artifacts/reports/")

    asyncio.run(_update_summaries())


@cli.command("rebuild-vector-db")
@click.option(
    "--force",
    is_flag=True,
    help="ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð±ÐµÐ· Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ñ",
)
@click.option(
    "--keep-reports",
    is_flag=True,
    help="Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ markdown Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹ Ð¸ JSON ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ChromaDB)",
)
@click.option(
    "--backup",
    is_flag=True,
    help="Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½ÑƒÑŽ ÐºÐ¾Ð¿Ð¸ÑŽ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¹ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿ÐµÑ€ÐµÐ´ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸ÐµÐ¼",
)
@click.option(
    "--no-progress",
    is_flag=True,
    help="ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€ (Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸)",
)
def rebuild_vector_db(force, keep_reports, backup, no_progress):
    """ðŸ”„ ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… ChromaDB

    Ð£Ð´Ð°Ð»ÑÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½ÑƒÑŽ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°ÐµÑ‚ ÐµÑ‘ Ð·Ð°Ð½Ð¾Ð²Ð¾,
    Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ñ‹ (JSON ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸, markdown Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹).

    ÐŸÐ¾Ð»ÐµÐ·Ð½Ð¾ ÐºÐ¾Ð³Ð´Ð°:
    - Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð²Ñ€ÐµÐ¶Ð´ÐµÐ½Ð°
    - ÐÑƒÐ¶Ð½Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÑÑ…ÐµÐ¼Ñƒ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¹
    - ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸

    Ð’ÐÐ˜ÐœÐÐÐ˜Ð•: Ð­Ñ‚Ð° ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° ÑƒÐ´Ð°Ð»Ð¸Ñ‚ Ð²ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· ChromaDB!
    """

    async def _rebuild():
        import json
        import shutil
        from pathlib import Path

        click.echo("=" * 80)
        click.echo("ðŸ”„ ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… ChromaDB")
        click.echo("=" * 80)
        click.echo()

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð²
        reports_dir = Path("artifacts/reports")
        chroma_dir = Path("chroma_db")

        if not reports_dir.exists():
            click.echo("âŒ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ artifacts/reports Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
            click.echo("ðŸ’¡ Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸ÑŽ: memory_mcp index")
            return

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ JSON ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹
        json_files = list(reports_dir.glob("**/*.json"))
        if not json_files:
            click.echo("âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ JSON Ñ„Ð°Ð¹Ð»Ð¾Ð² ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹")
            click.echo("ðŸ’¡ Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸ÑŽ: memory_mcp index")
            return

        click.echo(f"ðŸ“ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ JSON Ñ„Ð°Ð¹Ð»Ð¾Ð² ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹: {len(json_files)}")

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        if chroma_dir.exists():
            try:
                import chromadb

                chroma_client = chromadb.PersistentClient(path=str(chroma_dir))

                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑÑ…
                collections_info = []
                for collection_name in [
                    "chat_sessions",
                    "chat_messages",
                    "chat_tasks",
                    "session_clusters",
                    "indexing_progress",
                ]:
                    try:
                        collection = chroma_client.get_collection(collection_name)
                        count = collection.count()
                        collections_info.append(
                            f"   - {collection_name}: {count} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                        )
                    except:
                        collections_info.append(f"   - {collection_name}: Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")

                click.echo("ðŸ“Š Ð¢ÐµÐºÑƒÑ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ ChromaDB:")
                for info in collections_info:
                    click.echo(info)
                click.echo()

            except Exception as e:
                click.echo(f"âš ï¸  ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒÑÑ Ðº ChromaDB: {e}")
                click.echo("   Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð²Ñ€ÐµÐ¶Ð´ÐµÐ½Ð°")
                click.echo()

        # ÐŸÐ¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ðµ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ
        if not force:
            click.echo("âš ï¸  Ð’ÐÐ˜ÐœÐÐÐ˜Ð•: Ð­Ñ‚Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ Ð²ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· ChromaDB!")
            click.echo("   Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð±ÑƒÐ´ÑƒÑ‚ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ñ‹.")
            click.echo()

            if not click.confirm("ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ?"):
                click.echo("âŒ ÐžÐ¿ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð¼ÐµÐ½ÐµÐ½Ð°")
                return

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½ÑƒÑŽ ÐºÐ¾Ð¿Ð¸ÑŽ ÐµÑÐ»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑˆÐµÐ½Ð¾
        if backup and chroma_dir.exists():
            backup_dir = Path(
                f"chroma_db_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            click.echo(f"ðŸ“¦ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸: {backup_dir}")
            try:
                shutil.copytree(chroma_dir, backup_dir)
                click.echo(f"âœ… Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð°Ñ ÐºÐ¾Ð¿Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð°: {backup_dir}")
            except Exception as e:
                click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸: {e}")
                if not click.confirm("ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ Ð±ÐµÐ· Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸?"):
                    return
            click.echo()

        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        if chroma_dir.exists():
            click.echo("ðŸ—‘ï¸  Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¹ ChromaDB...")
            try:
                shutil.rmtree(chroma_dir)
                click.echo("âœ… Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð°Ñ Ð±Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑƒÐ´Ð°Ð»ÐµÐ½Ð°")
            except Exception as e:
                click.echo(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")
                return
            click.echo()

        # ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð²
        click.echo("ðŸ”„ ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹ Ð¸Ð· ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð²...")
        click.echo()

        try:
            # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ‚Ð¾Ñ€
            from ..core.indexer import TwoLevelIndexer

            click.echo("ðŸ“¦ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ‚Ð¾Ñ€Ð°...")
            indexer = TwoLevelIndexer()
            click.echo("âœ… Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ‚Ð¾Ñ€ Ð³Ð¾Ñ‚Ð¾Ð²")
            click.echo()

            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
            click.echo("ðŸ“š Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹...")

            sessions_data = []
            for json_file in json_files:
                try:
                    with open(json_file, encoding="utf-8") as f:
                        session_data = json.load(f)
                        sessions_data.append(session_data)
                except Exception as e:
                    click.echo(f"âš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ {json_file.name}: {e}")
                    continue

            click.echo(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹: {len(sessions_data)}")
            click.echo()

            if not sessions_data:
                click.echo("âŒ ÐÐµÑ‚ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ñ… ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð±Ð°Ð·Ñ‹")
                return

            # ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
            click.echo("ðŸ”„ ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¹ ChromaDB...")

            # Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ñ‡Ð°Ñ‚Ð°Ð¼
            chats_data = {}
            for session in sessions_data:
                chat_name = session.get("meta", {}).get("chat_name", "Unknown")
                if chat_name not in chats_data:
                    chats_data[chat_name] = []
                chats_data[chat_name].append(session)

            click.echo(f"ðŸ“‹ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²: {len(chats_data)}")

            # Ð˜Ð½Ð´ÐµÐºÑÐ¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´ÑƒÑŽ ÑÐµÑÑÐ¸ÑŽ Ñ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€Ð¾Ð¼
            total_sessions = len(sessions_data)
            indexed_sessions = 0
            indexed_messages = 0
            indexed_tasks = 0

            # Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ tqdm Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€Ð°
            from tqdm import tqdm

            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€
            show_progress = not no_progress

            if show_progress:
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€ Ð´Ð»Ñ Ð²ÑÐµÑ… ÑÐµÑÑÐ¸Ð¹
                with tqdm(
                    total=total_sessions,
                    desc="ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹",
                    unit="ÑÐµÑÑÐ¸Ñ",
                ) as pbar:
                    for chat_name, chat_sessions in chats_data.items():
                        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€Ð°
                        pbar.set_description(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‡Ð°Ñ‚Ð°: {chat_name}")

                        for session in chat_sessions:
                            try:
                                # L1: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸ ÑÐµÑÑÐ¸Ð¸
                                await indexer._index_session_l1(session)
                                indexed_sessions += 1

                                # L2: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
                                messages_count = await indexer._index_messages_l2(
                                    session
                                )
                                indexed_messages += messages_count

                                # L3: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡
                                tasks_count = await indexer._index_tasks(session)
                                indexed_tasks += tasks_count

                            except Exception as e:
                                click.echo(
                                    f"âš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ ÑÐµÑÑÐ¸Ð¸ {session.get('session_id', 'Unknown')}: {e}"
                                )
                                continue

                            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€ Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹
                            pbar.set_postfix(
                                {
                                    "ÑÐµÑÑÐ¸Ð¹": indexed_sessions,
                                    "ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹": indexed_messages,
                                    "Ð·Ð°Ð´Ð°Ñ‡": indexed_tasks,
                                }
                            )
                            pbar.update(1)
            else:
                # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð±ÐµÐ· Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ-Ð±Ð°Ñ€Ð°
                for chat_name, chat_sessions in chats_data.items():
                    click.echo(
                        f"ðŸ“ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‡Ð°Ñ‚Ð°: {chat_name} ({len(chat_sessions)} ÑÐµÑÑÐ¸Ð¹)"
                    )

                    for session in chat_sessions:
                        try:
                            # L1: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸ ÑÐµÑÑÐ¸Ð¸
                            await indexer._index_session_l1(session)
                            indexed_sessions += 1

                            # L2: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
                            messages_count = await indexer._index_messages_l2(session)
                            indexed_messages += messages_count

                            # L3: Ð˜Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡
                            tasks_count = await indexer._index_tasks(session)
                            indexed_tasks += tasks_count

                        except Exception as e:
                            click.echo(
                                f"âš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ ÑÐµÑÑÐ¸Ð¸ {session.get('session_id', 'Unknown')}: {e}"
                            )
                            continue

                    click.echo(f"   âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ ÑÐµÑÑÐ¸Ð¹: {len(chat_sessions)}")

            click.echo()
            click.echo("=" * 80)
            click.echo("âœ… Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð±Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð°!")
            click.echo("=" * 80)
            click.echo()
            click.echo("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:")
            click.echo(f"   - ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¾ ÑÐµÑÑÐ¸Ð¹ (L1): {indexed_sessions}")
            click.echo(f"   - ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ (L2): {indexed_messages}")
            click.echo(f"   - ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¾ Ð·Ð°Ð´Ð°Ñ‡ (L3): {indexed_tasks}")
            click.echo()
            click.echo("ðŸ“‚ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹:")
            click.echo("   - Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð°Ñ Ð±Ð°Ð·Ð°: ./chroma_db/")
            click.echo("   - ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸: chat_sessions, chat_messages, chat_tasks")
            if keep_reports:
                click.echo("   - Markdown Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹: ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² ./artifacts/reports/")
            click.echo()
            click.echo("ðŸ’¡ Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¸ÑÐº: memory_mcp search")

        except Exception as e:
            click.echo()
            click.echo("=" * 80)
            click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð±Ð°Ð·Ñ‹!")
            click.echo("=" * 80)
            click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
            click.echo()
            import traceback

            traceback.print_exc()

    asyncio.run(_rebuild())


@cli.command("extract-messages")
@click.option("--dry-run", is_flag=True, help="Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð°Ð½Ð°Ð»Ð¸Ð·, Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð¾Ð²")
@click.option("--no-date-filter", is_flag=True, help="ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾ Ð´Ð°Ñ‚Ðµ")
@click.option("--chat", help="Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ Ñ‡Ð°Ñ‚Ð°")
@click.option("--input-dir", default="input", help="Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ñ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸")
@click.option(
    "--chats-dir", default="chats", help="Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹"
)
def extract_messages(dry_run, no_date_filter, chat, input_dir, chats_dir):
    """ðŸ“¥ Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð¾Ð²Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¸Ð· input Ð² chats

    Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¸Ð· Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ input Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¸Ñ… Ð² chats,
    Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¿Ð¾ Ð´Ð°Ñ‚Ðµ Ð¸ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸ÐµÐ¹.
    """

    async def _extract_messages():
        click.echo("ðŸ“¥ Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð¾Ð²Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹...")
        click.echo(f"   Ð’Ñ…Ð¾Ð´Ð½Ð°Ñ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ: {input_dir}")
        click.echo(f"   Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð°Ñ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ: {chats_dir}")
        click.echo(
            f"   Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð´Ð°Ñ‚Ðµ: {'âŒ ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½' if no_date_filter else 'âœ… Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½'}"
        )
        click.echo(f"   Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ñ‡Ð°Ñ‚Ñƒ: {chat or 'Ð²ÑÐµ Ñ‡Ð°Ñ‚Ñ‹'}")
        click.echo(f"   Ð ÐµÐ¶Ð¸Ð¼: {'ðŸ”¸ DRY RUN' if dry_run else 'âœ… Ð ÐµÐ°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ'}")
        click.echo()

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐºÑÑ‚Ñ€Ð°ÐºÑ‚Ð¾Ñ€
        extractor = MessageExtractor(input_dir=input_dir, chats_dir=chats_dir)

        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ
        extractor.extract_all_messages(
            dry_run=dry_run, filter_by_date=not no_date_filter, chat_filter=chat
        )

        # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        extractor.print_stats()

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        click.echo("=" * 80)

    asyncio.run(_extract_messages())


@cli.command("deduplicate")
@click.option(
    "--chats-dir", default="chats", help="Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸"
)
def deduplicate(chats_dir):
    """ðŸ§¹ Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹

    Ð£Ð´Ð°Ð»ÑÐµÑ‚ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ð¿Ð¾Ð»ÑŽ 'id' Ð²Ð¾ Ð²ÑÐµÑ… Ñ‡Ð°Ñ‚Ð°Ñ….
    """

    async def _deduplicate():
        click.echo("ðŸ§¹ Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹...")
        click.echo(f"   Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ: {chats_dir}")
        click.echo()

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ñ€
        deduplicator = MessageDeduplicator(chats_dir=chats_dir)

        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸ÑŽ
        deduplicator.deduplicate_all_chats()

        # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        deduplicator.print_stats()

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!")
        click.echo("=" * 80)

    asyncio.run(_deduplicate())


@cli.command("sync-chromadb")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ñ‚Ð¸Ð¿Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚Ð¸",
)
@click.option(
    "--chroma-path",
    default="chroma_db",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº ChromaDB",
)
@click.option(
    "--chat",
    help="Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ñ‡Ð°Ñ‚",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Ð ÐµÐ¶Ð¸Ð¼ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹",
)
def sync_chromadb(db_path: Path, chroma_path: Path, chat: Optional[str], dry_run: bool):
    """Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¸Ð· ChromaDB Ð² Ð³Ñ€Ð°Ñ„ Ð¿Ð°Ð¼ÑÑ‚Ð¸.
    
    Ð­Ñ‚Ð° ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° Ð¼Ð¸Ð³Ñ€Ð¸Ñ€ÑƒÐµÑ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¸Ð· ChromaDB ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¹
    (chat_messages, chat_sessions, chat_tasks) Ð² Ð³Ñ€Ð°Ñ„ Ð¿Ð°Ð¼ÑÑ‚Ð¸ TypedGraphMemory.
    Ð­Ð¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ñ‚Ð°ÐºÐ¶Ðµ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ.
    """
    import chromadb
    from ..memory.ingest import MemoryIngestor
    from ..indexing import MemoryRecord
    from ..utils.datetime_utils import parse_datetime_utc
    from datetime import datetime, timezone
    
    logger.info("ðŸ”„ ÐÐ°Ñ‡Ð°Ð»Ð¾ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ChromaDB â†’ Ð“Ñ€Ð°Ñ„ Ð¿Ð°Ð¼ÑÑ‚Ð¸")
    
    if dry_run:
        logger.info("ðŸ” Ð ÐµÐ¶Ð¸Ð¼ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (dry-run), Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð½Ðµ Ð±ÑƒÐ´ÑƒÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹")
    
    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð³Ñ€Ð°Ñ„Ð°
    graph = TypedGraphMemory(db_path=str(db_path))
    ingestor = MemoryIngestor(graph)
    
    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð² Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð² Ð¸ Qdrant
    from ..memory.embeddings import build_embedding_service_from_env
    from ..memory.vector_store import build_vector_store_from_env
    
    embedding_service = build_embedding_service_from_env()
    vector_store = build_vector_store_from_env()
    
    if vector_store and embedding_service and embedding_service.dimension:
        vector_store.ensure_collection(embedding_service.dimension)
        logger.info("âœ… Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾")
    else:
        logger.warning("âš ï¸  Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾, ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð½Ðµ Ð±ÑƒÐ´ÑƒÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² Qdrant")
    
    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ChromaDB
    chroma_client = chromadb.PersistentClient(path=str(chroma_path))
    
    total_synced = 0
    total_errors = 0
    
    collections_to_sync = ["chat_messages", "chat_sessions", "chat_tasks"]
    
    for collection_name in collections_to_sync:
        try:
            collection = chroma_client.get_collection(collection_name)
            total_count = collection.count()
            
            if total_count == 0:
                logger.info(f"  ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ {collection_name}: Ð¿ÑƒÑÑ‚Ð°, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼")
                continue
            
            logger.info(f"  ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ {collection_name}: {total_count} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸
            offset = 0
            batch_size = 100
            synced_in_collection = 0
            
            while offset < total_count:
                try:
                    result = collection.get(
                        limit=batch_size,
                        offset=offset,
                        include=["documents", "metadatas", "embeddings"]
                    )
                    
                    ids = result.get("ids", [])
                    if not ids:
                        break
                    
                    documents = result.get("documents", [])
                    metadatas = result.get("metadatas", [])
                    embeddings = result.get("embeddings", [])
                    
                    records_to_ingest = []
                    
                    for idx, record_id in enumerate(ids):
                        try:
                            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ Ð»Ð¸ ÑƒÐ¶Ðµ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð² Ð³Ñ€Ð°Ñ„Ðµ
                            if record_id in graph.graph:
                                continue
                            
                            # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ñ‡Ð°Ñ‚Ñƒ, ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½
                            metadata = metadatas[idx] if idx < len(metadatas) else {}
                            if chat and metadata.get("chat") != chat:
                                continue
                            
                            doc = documents[idx] if idx < len(documents) else ""
                            embedding = embeddings[idx] if idx < len(embeddings) else None
                            
                            # ÐŸÐ°Ñ€ÑÐ¸Ð¼ timestamp
                            date_utc = metadata.get("date_utc") or metadata.get("start_time_utc") or metadata.get("end_time_utc")
                            timestamp = None
                            if date_utc:
                                try:
                                    timestamp = parse_datetime_utc(date_utc, use_zoneinfo=True)
                                except Exception:
                                    timestamp = datetime.now(timezone.utc)
                            else:
                                timestamp = datetime.now(timezone.utc)
                            
                            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð°Ð²Ñ‚Ð¾Ñ€Ð°
                            author = metadata.get("sender") or metadata.get("author") or metadata.get("username")
                            
                            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ‚ÐµÐ³Ð¸ Ð¸ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸
                            tags = metadata.get("tags", [])
                            if isinstance(tags, str):
                                tags = [tags] if tags else []
                            
                            entities = metadata.get("entities", [])
                            if isinstance(entities, str):
                                entities = [entities] if entities else []
                            
                            # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ MemoryRecord
                            record = MemoryRecord(
                                record_id=record_id,
                                source=metadata.get("chat", collection_name.replace("chat_", "")),
                                content=doc,
                                timestamp=timestamp,
                                author=author,
                                tags=tags if isinstance(tags, list) else [],
                                entities=entities if isinstance(entities, list) else [],
                                attachments=[],
                                metadata={
                                    "collection": collection_name,
                                    "chat": metadata.get("chat", ""),
                                    **metadata,
                                },
                            )
                            
                            records_to_ingest.append((record, embedding))
                            
                        except Exception as e:
                            logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸ {record_id}: {e}")
                            total_errors += 1
                            continue
                    
                    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð² Ð³Ñ€Ð°Ñ„
                    if records_to_ingest and not dry_run:
                        try:
                            records_only = [r for r, _ in records_to_ingest]
                            ingestor.ingest(records_only)
                            
                            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð² Ð³Ñ€Ð°Ñ„ Ð¸ Qdrant
                            for record, embedding in records_to_ingest:
                                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ Ð¸ Ð½Ðµ Ð¿ÑƒÑÑ‚Ð¾Ð¹
                                if embedding is not None and len(embedding) > 0:
                                    try:
                                        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ numpy Ð¼Ð°ÑÑÐ¸Ð² Ð² ÑÐ¿Ð¸ÑÐ¾Ðº, ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
                                        if hasattr(embedding, 'tolist'):
                                            embedding = embedding.tolist()
                                        elif not isinstance(embedding, list):
                                            embedding = list(embedding)
                                        
                                        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ Ð² Ð³Ñ€Ð°Ñ„
                                        graph.update_node(record.record_id, embedding=embedding)
                                        
                                        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ Ð² Qdrant
                                        if vector_store:
                                            payload_data = {
                                                "record_id": record.record_id,
                                                "source": record.source,
                                                "tags": record.tags,
                                                "timestamp": record.timestamp.timestamp(),
                                                "timestamp_iso": record.timestamp.isoformat(),
                                                "content_preview": record.content[:200],
                                            }
                                            chat_name = record.metadata.get("chat")
                                            if isinstance(chat_name, str):
                                                payload_data["chat"] = chat_name
                                            
                                            try:
                                                vector_store.upsert(record.record_id, embedding, payload_data)
                                            except Exception as e:
                                                logger.debug(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð° Ð² Qdrant Ð´Ð»Ñ {record.record_id}: {e}")
                                    except Exception as e:
                                        logger.debug(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð° Ð´Ð»Ñ {record.record_id}: {e}")
                            
                            synced_in_collection += len(records_to_ingest)
                            total_synced += len(records_to_ingest)
                            
                        except Exception as e:
                            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Ð³Ñ€Ð°Ñ„: {e}")
                            total_errors += len(records_to_ingest)
                    elif records_to_ingest and dry_run:
                        synced_in_collection += len(records_to_ingest)
                        total_synced += len(records_to_ingest)
                    
                    offset += len(ids)
                    if len(ids) < batch_size:
                        break
                    
                except Exception as e:
                    logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð±Ð°Ñ‚Ñ‡Ð° (offset={offset}): {e}")
                    total_errors += batch_size
                    offset += batch_size
            
            if synced_in_collection > 0:
                logger.info(f"  âœ… Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {synced_in_collection} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¸Ð· {collection_name}")
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ {collection_name}: {e}")
            total_errors += 1
    
    if dry_run:
        logger.info(f"ðŸ” Ð ÐµÐ¶Ð¸Ð¼ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ: Ð±Ñ‹Ð»Ð¾ Ð±Ñ‹ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {total_synced} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    else:
        logger.info(f"âœ… Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: {total_synced} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹, {total_errors} Ð¾ÑˆÐ¸Ð±Ð¾Ðº")
        if vector_store:
            logger.info("âœ… Ð­Ð¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² Qdrant")
    
    graph.conn.close()
    if vector_store:
        vector_store.close()
    if embedding_service:
        embedding_service.close()


@cli.command("stop-indexing")
def stop_indexing():
    """ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸

    ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ Ð²ÑÐµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ Ð¸ Ollama ÑÐµÑ€Ð²ÐµÑ€.
    """

    async def _stop_indexing():
        click.echo("ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸...")
        click.echo()

        # ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð²ÑÐµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹
        ProcessManager.stop_all_indexing()

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð² Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!")
        click.echo("=" * 80)

    asyncio.run(_stop_indexing())


@cli.command("review-summaries")
@click.option("--dry-run", is_flag=True, help="Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð°Ð½Ð°Ð»Ð¸Ð·, Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð¾Ð²")
@click.option("--chat", help="ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ñ‡Ð°Ñ‚")
@click.option("--limit", type=int, help="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸")
def review_summaries(dry_run, chat, limit):
    """ðŸ” ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ñ€ÐµÐ²ÑŒÑŽ Ð¸ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹ Ñ ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ¾Ð¼ -needs-review

    ÐÐ°Ñ…Ð¾Ð´Ð¸Ñ‚ Ñ„Ð°Ð¹Ð»Ñ‹ *-needs-review.md, Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ñ… Ñ‡ÐµÑ€ÐµÐ· LLM Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚
    Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð±ÐµÐ· ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ° -needs-review.
    """
    import json

    from ..core.lmstudio_client import LMStudioEmbeddingClient
    from ..config import get_settings

    async def _review_summaries():
        click.echo("ðŸ” ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ñ€ÐµÐ²ÑŒÑŽ Ð¸ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹")
        click.echo()

        if dry_run:
            click.echo("ðŸ”¸ Ð ÐµÐ¶Ð¸Ð¼ DRY RUN - Ñ„Ð°Ð¹Ð»Ñ‹ Ð½Ðµ Ð±ÑƒÐ´ÑƒÑ‚ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ñ‹")
            click.echo()

        reports_dir = Path("artifacts/reports")

        if not reports_dir.exists():
            click.echo("âŒ Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ artifacts/reports Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
            return

        # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ñ„Ð°Ð¹Ð»Ñ‹ Ñ -needs-review
        needs_review_files = []
        for md_file in reports_dir.rglob("*-needs-review.md"):
            json_file = md_file.with_suffix(".json")

            file_info = {
                "md_file": md_file,
                "json_file": json_file if json_file.exists() else None,
                "session_id": md_file.stem.replace("-needs-review", ""),
                "chat": md_file.parent.parent.name,
            }

            # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ñ‡Ð°Ñ‚Ñƒ ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½
            if chat and chat.lower() not in file_info["chat"].lower():
                continue

            needs_review_files.append(file_info)

        # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð»Ð¸Ð¼Ð¸Ñ‚
        if limit:
            needs_review_files = needs_review_files[:limit]

        if not needs_review_files:
            click.echo("âœ… ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ñ ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ¾Ð¼ -needs-review")
            return

        click.echo(f"ðŸ“ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {len(needs_review_files)}")
        click.echo()

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ LLM ÐºÐ»Ð¸ÐµÐ½Ñ‚
        settings = get_settings()
        embedding_client = LMStudioEmbeddingClient(
            model_name=settings.lmstudio_model,
            base_url=f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
        )

        async def review_summary(md_content: str) -> dict:
            prompt = f"""Ð¢Ñ‹ - ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ñƒ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑŽ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¹ Ñ‡Ð°Ñ‚Ð¾Ð².

ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÑƒÑŽ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸ ÐµÑ‘, ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾:

{md_content}

Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð°:
1. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¸ Ð¿Ð¾Ð»Ð½Ð¾Ñ‚Ñƒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸
2. Ð˜ÑÐ¿Ñ€Ð°Ð²Ð¸Ñ‚ÑŒ Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸ ÑÑ‚Ð¸Ð»Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸
3. Ð£Ð»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ ÑÑÐ½Ð¾ÑÑ‚ÑŒ Ð¸ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ð¾ÑÑ‚ÑŒ
4. Ð£Ð±ÐµÐ´Ð¸Ñ‚ÑŒÑÑ, Ñ‡Ñ‚Ð¾ Ð²ÑÐµ ÑÐµÐºÑ†Ð¸Ð¸ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ñ‹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾
5. Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð²Ð°Ð¶Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ, ÐµÑÐ»Ð¸ Ð¾Ð½Ð° Ð¾Ñ‡ÐµÐ²Ð¸Ð´Ð½Ð° Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°

Ð’ÐÐ–ÐÐž:
- Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ markdown (Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸, ÑÐ¿Ð¸ÑÐºÐ¸, Ð¸ Ñ‚.Ð´.)
- ÐÐµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð½ÐµÑ‚ Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»Ðµ
- Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ Ð²ÑÐµ Ð´Ð°Ñ‚Ñ‹, Ð¸Ð¼ÐµÐ½Ð° Ð¸ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´ÐµÑ‚Ð°Ð»Ð¸
- Ð•ÑÐ»Ð¸ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ…Ð¾Ñ€Ð¾ÑˆÐ°Ñ - Ð²ÐµÑ€Ð½Ð¸ ÐµÑ‘ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹

Ð’ÐµÑ€Ð½Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ markdown-Ñ‚ÐµÐºÑÑ‚ Ð‘Ð•Ð— Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ²."""

            try:
                async with embedding_client:
                    improved = await embedding_client.generate_summary(
                        prompt,
                        temperature=0.3,
                        max_tokens=131072,  # Ð”Ð»Ñ gpt-oss-20b (Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð»Ð¸Ð¼Ð¸Ñ‚)
                    )
                    improved = improved.strip()

                    if improved:

                        # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ
                        issues_found = []
                        if (
                            "_(ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…)_" in md_content
                            or "_(Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚)_" in md_content
                        ):
                            issues_found.append("Ð•ÑÑ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐµÐºÑ†Ð¸Ð¸")
                        if len(md_content) < 200:
                            issues_found.append("Ð¡Ð»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ°Ñ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ")
                        if md_content.count("##") < 2:
                            issues_found.append("ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ")

                        improvements = []
                        if md_content != improved:
                            improvements.append("Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð¾ÑˆÐ¸Ð±ÐºÐ¸")
                        if len(improved) > len(md_content) * 1.1:
                            improvements.append("Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚")
                        if not improvements:
                            improvements.append("Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ")

                        return {
                            "improved_content": improved,
                            "issues_found": issues_found,
                            "improvements": improvements,
                            "success": True,
                        }
                    else:
                        return {
                            "improved_content": md_content,
                            "issues_found": [],
                            "improvements": [],
                            "success": False,
                            "error": "ÐÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ LLM",
                        }

            except Exception as e:
                return {
                    "improved_content": md_content,
                    "issues_found": [],
                    "improvements": [],
                    "success": False,
                    "error": str(e),
                }

        # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»

        for file_info in needs_review_files:
            md_file = file_info["md_file"]
            json_file = file_info["json_file"]
            session_id = file_info["session_id"]

            click.echo(f"ðŸ“„ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°: {md_file.name}")

            # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ markdown
            try:
                with open(md_file, encoding="utf-8") as f:
                    md_content = f.read()
            except Exception as e:
                click.echo(f"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ MD: {e}")
                continue

            # ÐŸÑ€Ð¾Ð²Ð¾Ð´Ð¸Ð¼ Ñ€ÐµÐ²ÑŒÑŽ
            click.echo("   ðŸ” ÐÐ½Ð°Ð»Ð¸Ð· Ñ‡ÐµÑ€ÐµÐ· LLM...")
            review_result = await review_summary(md_content)

            if not review_result["success"]:
                click.echo(
                    f"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°: {review_result.get('error', 'Unknown')}"
                )
                continue

            # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
            if review_result["issues_found"]:
                click.echo(
                    f"   âš ï¸  ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼: {', '.join(review_result['issues_found'])}"
                )

            if review_result["improvements"]:
                click.echo(
                    f"   âœ¨ Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ: {', '.join(review_result['improvements'])}"
                )

            if dry_run:
                click.echo("   ðŸ”¸ DRY RUN - Ñ„Ð°Ð¹Ð» Ð½Ðµ Ð¸Ð·Ð¼ÐµÐ½Ñ‘Ð½")
                continue

            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ
            new_md_file = md_file.parent / f"{session_id}.md"
            new_json_file = md_file.parent / f"{session_id}.json"

            try:
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ markdown
                with open(new_md_file, "w", encoding="utf-8") as f:
                    f.write(review_result["improved_content"])

                # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ JSON ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
                if json_file:
                    try:
                        with open(json_file, encoding="utf-8") as f:
                            session_data = json.load(f)

                        session_data["session_id"] = session_id

                        with open(new_json_file, "w", encoding="utf-8") as f:
                            json.dump(session_data, f, ensure_ascii=False, indent=2)

                        if new_json_file != json_file:
                            json_file.unlink()
                            click.echo(f"   ðŸ—‘ï¸  Ð£Ð´Ð°Ð»Ñ‘Ð½ ÑÑ‚Ð°Ñ€Ñ‹Ð¹ JSON: {json_file.name}")
                    except Exception as e:
                        click.echo(f"   âš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ JSON: {e}")

                if new_md_file != md_file:
                    md_file.unlink()
                    click.echo(f"   ðŸ—‘ï¸  Ð£Ð´Ð°Ð»Ñ‘Ð½ ÑÑ‚Ð°Ñ€Ñ‹Ð¹ MD: {md_file.name}")

                click.echo(f"   âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»: {new_md_file.name}")

            except Exception as e:
                click.echo(f"   âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ: {e}")

            # ÐÐµÐ±Ð¾Ð»ÑŒÑˆÐ°Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
            await asyncio.sleep(1)

        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!")
        click.echo("=" * 80)

    asyncio.run(_review_summaries())


@cli.command("backup-database")
@click.option(
    "--backup-path",
    type=click.Path(path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ backup (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: backups/backup_YYYYMMDD_HHMMSS)",
)
@click.option(
    "--include-chromadb/--no-chromadb",
    default=True,
    help="Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ ChromaDB Ð² backup",
)
@click.option(
    "--include-reports/--no-reports",
    default=False,
    help="Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ markdown Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹ Ð² backup",
)
@click.option(
    "--compress/--no-compress",
    default=True,
    help="Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÐ¶Ð°Ñ‚Ñ‹Ð¹ .tar.gz Ð°Ñ€Ñ…Ð¸Ð²",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
)
@click.option(
    "--chroma-path",
    default="chroma_db",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº ChromaDB",
)
def backup_database(backup_path, include_chromadb, include_reports, compress, db_path, chroma_path):
    """ðŸ“¦ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… (SQLite + ChromaDB)
    
    Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½ÑƒÑŽ ÐºÐ¾Ð¿Ð¸ÑŽ Ð²ÑÐµÑ… Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹:
    - SQLite Ð±Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… (memory_graph.db)
    - ChromaDB Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
    - Markdown Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
    """
    import shutil
    import tarfile
    
    click.echo("ðŸ“¦ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
    click.echo()
    
    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿ÑƒÑ‚ÑŒ Ð´Ð»Ñ backup
    if not backup_path:
        backups_dir = Path("backups")
        backups_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = backups_dir / f"backup_{timestamp}"
        if compress:
            backup_path = backup_path.with_suffix(".tar.gz")
    
    backup_path = Path(backup_path)
    backup_path.parent.mkdir(parents=True, exist_ok=True)
    
    includes = []
    temp_backup_dir = None
    
    try:
        if compress:
            # Ð”Ð»Ñ ÑÐ¶Ð°Ñ‚Ð¾Ð³Ð¾ Ð°Ñ€Ñ…Ð¸Ð²Ð° ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ
            temp_backup_dir = Path(f"/tmp/memory_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            temp_backup_dir.mkdir(exist_ok=True)
            actual_backup_path = temp_backup_dir
        else:
            actual_backup_path = backup_path
            actual_backup_path.mkdir(exist_ok=True)
        
        # ÐšÐ¾Ð¿Ð¸Ñ€ÑƒÐµÐ¼ SQLite Ð‘Ð”
        if db_path.exists():
            click.echo(f"ðŸ“„ ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ SQLite Ð‘Ð”: {db_path}")
            db_backup_path = actual_backup_path / "memory_graph.db"
            shutil.copy2(db_path, db_backup_path)
            includes.append("sqlite_database")
            click.echo(f"   âœ… Ð Ð°Ð·Ð¼ÐµÑ€: {db_backup_path.stat().st_size / 1024 / 1024:.2f} MB")
        else:
            click.echo(f"âš ï¸  SQLite Ð‘Ð” Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {db_path}")
        
        # ÐšÐ¾Ð¿Ð¸Ñ€ÑƒÐµÐ¼ ChromaDB
        if include_chromadb and chroma_path.exists():
            click.echo(f"ðŸ” ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ChromaDB: {chroma_path}")
            chroma_backup_path = actual_backup_path / "chroma_db"
            shutil.copytree(chroma_path, chroma_backup_path, dirs_exist_ok=True)
            includes.append("chromadb")
            # ÐŸÐ¾Ð´ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€
            total_size = sum(f.stat().st_size for f in chroma_backup_path.rglob('*') if f.is_file())
            click.echo(f"   âœ… Ð Ð°Ð·Ð¼ÐµÑ€: {total_size / 1024 / 1024:.2f} MB")
        elif include_chromadb:
            click.echo(f"âš ï¸  ChromaDB Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {chroma_path}")
        
        # ÐšÐ¾Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹
        if include_reports:
            reports_path = Path("artifacts/reports")
            if reports_path.exists():
                click.echo(f"ðŸ“Š ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²: {reports_path}")
                reports_backup_path = actual_backup_path / "reports"
                shutil.copytree(reports_path, reports_backup_path, dirs_exist_ok=True)
                includes.append("reports")
                total_size = sum(f.stat().st_size for f in reports_backup_path.rglob('*') if f.is_file())
                click.echo(f"   âœ… Ð Ð°Ð·Ð¼ÐµÑ€: {total_size / 1024 / 1024:.2f} MB")
            else:
                click.echo(f"âš ï¸  ÐžÑ‚Ñ‡ÐµÑ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹: {reports_path}")
        
        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð°Ñ€Ñ…Ð¸Ð² ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        if compress and temp_backup_dir:
            click.echo(f"ðŸ—œï¸  Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð°Ñ€Ñ…Ð¸Ð²Ð°: {backup_path}")
            with tarfile.open(backup_path, "w:gz") as tar:
                tar.add(temp_backup_dir, arcname=backup_path.stem)
            backup_size = backup_path.stat().st_size
            click.echo(f"   âœ… Ð Ð°Ð·Ð¼ÐµÑ€ Ð°Ñ€Ñ…Ð¸Ð²Ð°: {backup_size / 1024 / 1024:.2f} MB")
        
        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð°Ñ ÐºÐ¾Ð¿Ð¸Ñ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð°!")
        click.echo("=" * 80)
        click.echo(f"ðŸ“ ÐŸÑƒÑ‚ÑŒ: {backup_path}")
        click.echo(f"ðŸ“¦ Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾: {', '.join(includes)}")
        if compress:
            click.echo(f"ðŸ“Š Ð Ð°Ð·Ð¼ÐµÑ€: {backup_path.stat().st_size / 1024 / 1024:.2f} MB")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸!")
        click.echo("=" * 80)
        click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()
    finally:
        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ
        if temp_backup_dir and temp_backup_dir.exists():
            shutil.rmtree(temp_backup_dir)


@cli.command("restore-database")
@click.option(
    "--backup-path",
    type=click.Path(exists=True, path_type=Path),
    required=True,
    help="ÐŸÑƒÑ‚ÑŒ Ðº Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸ (Ñ„Ð°Ð¹Ð» .tar.gz Ð¸Ð»Ð¸ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ)",
)
@click.option(
    "--confirm",
    is_flag=True,
    help="ÐŸÐ¾Ð´Ñ‚Ð²ÐµÑ€Ð´Ð¸Ñ‚ÑŒ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ (ÑƒÐ´Ð°Ð»Ð¸Ñ‚ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ)",
)
@click.option(
    "--restore-chromadb/--no-chromadb",
    default=True,
    help="Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ChromaDB",
)
@click.option(
    "--restore-reports/--no-reports",
    default=False,
    help="Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ markdown Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
)
@click.option(
    "--chroma-path",
    default="chroma_db",
    type=click.Path(file_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº ChromaDB",
)
def restore_database(backup_path, confirm, restore_chromadb, restore_reports, db_path, chroma_path):
    """ðŸ”„ Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸
    
    Ð’ÐÐ˜ÐœÐÐÐ˜Ð•: Ð­Ñ‚Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¸Ð· backup!
    """
    import shutil
    import tarfile
    
    click.echo("ðŸ”„ Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ð¾Ð¹ ÐºÐ¾Ð¿Ð¸Ð¸")
    click.echo()
    
    if not confirm:
        click.echo("âš ï¸  Ð’ÐÐ˜ÐœÐÐÐ˜Ð•: Ð­Ñ‚Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ!")
        click.echo(f"   Ð‘ÑƒÐ´ÐµÑ‚ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð¸Ð·: {backup_path}")
        if not click.confirm("ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ?"):
            click.echo("âŒ ÐžÐ¿ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð¼ÐµÐ½ÐµÐ½Ð°")
            return
    
    backup_path = Path(backup_path)
    temp_extract_dir = None
    
    try:
        # Ð Ð°ÑÐ¿Ð°ÐºÐ¾Ð²Ñ‹Ð²Ð°ÐµÐ¼ Ð°Ñ€Ñ…Ð¸Ð² ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        if backup_path.suffix == ".gz" or backup_path.suffixes == [".tar", ".gz"]:
            click.echo(f"ðŸ“¦ Ð Ð°ÑÐ¿Ð°ÐºÐ¾Ð²ÐºÐ° Ð°Ñ€Ñ…Ð¸Ð²Ð°: {backup_path}")
            temp_extract_dir = Path(f"/tmp/memory_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            temp_extract_dir.mkdir(exist_ok=True)
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(temp_extract_dir)
            # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ñ€Ð°ÑÐ¿Ð°ÐºÐ¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ
            extracted_dirs = [d for d in temp_extract_dir.iterdir() if d.is_dir()]
            if extracted_dirs:
                source_dir = extracted_dirs[0]
            else:
                source_dir = temp_extract_dir
        else:
            source_dir = backup_path
        
        # Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ SQLite Ð‘Ð”
        db_backup = source_dir / "memory_graph.db"
        if db_backup.exists():
            click.echo(f"ðŸ“„ Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ SQLite Ð‘Ð”: {db_path}")
            if db_path.exists():
                # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ backup Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð‘Ð”
                old_db_backup = Path(f"{db_path}.old_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
                shutil.copy2(db_path, old_db_backup)
                click.echo(f"   ðŸ’¾ Ð¢ÐµÐºÑƒÑ‰Ð°Ñ Ð‘Ð” ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° ÐºÐ°Ðº: {old_db_backup}")
            db_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(db_backup, db_path)
            click.echo("   âœ… SQLite Ð‘Ð” Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°")
        else:
            click.echo(f"âš ï¸  SQLite Ð‘Ð” Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð² backup: {db_backup}")
        
        # Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ChromaDB
        if restore_chromadb:
            chroma_backup = source_dir / "chroma_db"
            if chroma_backup.exists() and chroma_backup.is_dir():
                click.echo(f"ðŸ” Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ChromaDB: {chroma_path}")
                if chroma_path.exists():
                    shutil.rmtree(chroma_path)
                chroma_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copytree(chroma_backup, chroma_path)
                click.echo("   âœ… ChromaDB Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°")
            else:
                click.echo(f"âš ï¸  ChromaDB Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð² backup: {chroma_backup}")
        
        # Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹
        if restore_reports:
            reports_backup = source_dir / "reports"
            if reports_backup.exists() and reports_backup.is_dir():
                reports_path = Path("artifacts/reports")
                click.echo(f"ðŸ“Š Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²: {reports_path}")
                if reports_path.exists():
                    shutil.rmtree(reports_path)
                reports_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copytree(reports_backup, reports_path)
                click.echo("   âœ… ÐžÑ‚Ñ‡ÐµÑ‚Ñ‹ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹")
            else:
                click.echo(f"âš ï¸  ÐžÑ‚Ñ‡ÐµÑ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ð² backup: {reports_backup}")
        
        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°!")
        click.echo("=" * 80)
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¸ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…!")
        click.echo("=" * 80)
        click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()
    finally:
        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ
        if temp_extract_dir and temp_extract_dir.exists():
            shutil.rmtree(temp_extract_dir)


@cli.command("optimize-database")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
)
@click.option(
    "--vacuum/--no-vacuum",
    default=True,
    help="Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ VACUUM Ð´Ð»Ñ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¼ÐµÑÑ‚Ð°",
)
@click.option(
    "--analyze/--no-analyze",
    default=True,
    help="Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ ANALYZE Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸",
)
@click.option(
    "--reindex/--no-reindex",
    default=False,
    help="Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ REINDEX Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²",
)
@click.option(
    "--optimize-fts/--no-optimize-fts",
    default=True,
    help="ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ FTS5 Ð¸Ð½Ð´ÐµÐºÑ",
)
def optimize_database(db_path, vacuum, analyze, reindex, optimize_fts):
    """âš¡ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ SQLite Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    
    Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸:
    - VACUUM: Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´Ð°ÐµÑ‚ Ð¼ÐµÑÑ‚Ð¾, ÑƒÐ´Ð°Ð»ÑÑ Ð½ÐµÐ¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼Ñ‹Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
    - ANALYZE: Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
    - REINDEX: Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¸Ð½Ð´ÐµÐºÑÑ‹
    - FTS5 Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ: Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾Ð»Ð½Ð¾Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº
    """
    import sqlite3
    import time
    
    click.echo("âš¡ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ SQLite Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
    click.echo()
    
    if not db_path.exists():
        click.echo(f"âŒ Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {db_path}")
        raise click.Abort()
    
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð´Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    size_before = db_path.stat().st_size
    
    operations_performed = []
    start_time = time.time()
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # VACUUM
        if vacuum:
            click.echo("ðŸ§¹ Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ VACUUM...")
            cursor.execute("VACUUM")
            conn.commit()
            operations_performed.append("VACUUM")
            click.echo("   âœ… VACUUM Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½")
        
        # ANALYZE
        if analyze:
            click.echo("ðŸ“Š Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ ANALYZE...")
            cursor.execute("ANALYZE")
            conn.commit()
            operations_performed.append("ANALYZE")
            click.echo("   âœ… ANALYZE Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½")
        
        # REINDEX
        if reindex:
            click.echo("ðŸ”„ Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ REINDEX...")
            cursor.execute("REINDEX")
            conn.commit()
            operations_performed.append("REINDEX")
            click.echo("   âœ… REINDEX Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½")
        
        # FTS5 Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ
        if optimize_fts:
            click.echo("ðŸ” ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ FTS5 Ð¸Ð½Ð´ÐµÐºÑÐ°...")
            try:
                cursor.execute("INSERT INTO node_search(node_search) VALUES('optimize')")
                conn.commit()
                operations_performed.append("FTS5_optimize")
                click.echo("   âœ… FTS5 Ð¸Ð½Ð´ÐµÐºÑ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")
            except sqlite3.OperationalError as e:
                if "no such table" not in str(e).lower():
                    raise
                click.echo("   âš ï¸  FTS5 Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼")
        
        conn.close()
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        size_after = db_path.stat().st_size
        space_freed = size_before - size_after
        duration = time.time() - start_time
        
        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!")
        click.echo("=" * 80)
        click.echo(f"ðŸ“Š ÐžÐ¿ÐµÑ€Ð°Ñ†Ð¸Ð¸: {', '.join(operations_performed)}")
        click.echo(f"ðŸ“¦ Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð¾: {size_before / 1024 / 1024:.2f} MB")
        click.echo(f"ðŸ“¦ Ð Ð°Ð·Ð¼ÐµÑ€ Ð¿Ð¾ÑÐ»Ðµ: {size_after / 1024 / 1024:.2f} MB")
        if space_freed > 0:
            click.echo(f"ðŸ’¾ ÐžÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¾: {space_freed / 1024 / 1024:.2f} MB")
        click.echo(f"â±ï¸  Ð’Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ: {duration:.2f} ÑÐµÐº")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…!")
        click.echo("=" * 80)
        click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("validate-database")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
)
@click.option(
    "--check-integrity/--no-check-integrity",
    default=True,
    help="ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ SQLite (PRAGMA integrity_check)",
)
@click.option(
    "--check-foreign-keys/--no-check-foreign-keys",
    default=True,
    help="ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð²Ð½ÐµÑˆÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡Ð¸ (PRAGMA foreign_key_check)",
)
@click.option(
    "--check-orphaned-nodes/--no-check-orphaned-nodes",
    default=True,
    help="ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑƒÐ·Ð»Ñ‹ Ð±ÐµÐ· ÑÐ²ÑÐ·ÐµÐ¹",
)
@click.option(
    "--check-orphaned-edges/--no-check-orphaned-edges",
    default=True,
    help="ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ€Ñ‘Ð±Ñ€Ð° Ñ Ð½ÐµÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ ÑƒÐ·Ð»Ð°Ð¼Ð¸",
)
def validate_database(db_path, check_integrity, check_foreign_keys, check_orphaned_nodes, check_orphaned_edges):
    """ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    
    Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½ÑƒÑŽ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…:
    - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ SQLite
    - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²Ð½ÐµÑˆÐ½Ð¸Ñ… ÐºÐ»ÑŽÑ‡ÐµÐ¹
    - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ñ€Ð°Ñ„Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹ (ÑÐ¸Ñ€Ð¾Ñ‚ÑÐºÐ¸Ðµ ÑƒÐ·Ð»Ñ‹ Ð¸ Ñ€Ñ‘Ð±Ñ€Ð°)
    """
    import sqlite3
    
    click.echo("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
    click.echo()
    
    if not db_path.exists():
        click.echo(f"âŒ Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {db_path}")
        raise click.Abort()
    
    issues = []
    checks_performed = []
    
    try:
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ SQLite
        if check_integrity:
            click.echo("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸ SQLite...")
            cursor.execute("PRAGMA integrity_check")
            result = cursor.fetchone()[0]
            checks_performed.append("integrity_check")
            if result == "ok":
                click.echo("   âœ… Ð¦ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ SQLite: OK")
            else:
                click.echo(f"   âŒ ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒÑŽ: {result}")
                issues.append({
                    "type": "integrity",
                    "severity": "error",
                    "message": f"SQLite integrity check failed: {result}",
                    "details": {"result": result}
                })
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²Ð½ÐµÑˆÐ½Ð¸Ñ… ÐºÐ»ÑŽÑ‡ÐµÐ¹
        if check_foreign_keys:
            click.echo("ðŸ”— ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²Ð½ÐµÑˆÐ½Ð¸Ñ… ÐºÐ»ÑŽÑ‡ÐµÐ¹...")
            cursor.execute("PRAGMA foreign_key_check")
            foreign_key_issues = cursor.fetchall()
            checks_performed.append("foreign_key_check")
            if not foreign_key_issues:
                click.echo("   âœ… Ð’Ð½ÐµÑˆÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡Ð¸: OK")
            else:
                click.echo(f"   âŒ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ñ Ð²Ð½ÐµÑˆÐ½Ð¸Ð¼Ð¸ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸: {len(foreign_key_issues)}")
                for issue in foreign_key_issues:
                    issues.append({
                        "type": "foreign_key",
                        "severity": "error",
                        "message": f"Foreign key violation: {dict(issue)}",
                        "details": dict(issue)
                    })
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ñ€Ð°Ñ„Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹
        if check_orphaned_nodes or check_orphaned_edges:
            click.echo("ðŸ•¸ï¸  ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ñ€Ð°Ñ„Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹...")
            from ..memory.typed_graph import TypedGraphMemory
            graph = TypedGraphMemory(db_path=str(db_path))
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸Ñ€Ð¾Ñ‚ÑÐºÐ¸Ñ… ÑƒÐ·Ð»Ð¾Ð²
            if check_orphaned_nodes:
                cursor.execute("""
                    SELECT id, type 
                    FROM nodes 
                    WHERE id NOT IN (
                        SELECT DISTINCT source_id FROM edges
                        UNION
                        SELECT DISTINCT target_id FROM edges
                    )
                """)
                orphaned_nodes = cursor.fetchall()
                checks_performed.append("orphaned_nodes")
                if not orphaned_nodes:
                    click.echo("   âœ… Ð¡Ð¸Ñ€Ð¾Ñ‚ÑÐºÐ¸Ðµ ÑƒÐ·Ð»Ñ‹: Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾")
                else:
                    click.echo(f"   âš ï¸  ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑÐ¸Ñ€Ð¾Ñ‚ÑÐºÐ¸Ñ… ÑƒÐ·Ð»Ð¾Ð²: {len(orphaned_nodes)}")
                    for node in orphaned_nodes[:10]:  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10
                        issues.append({
                            "type": "orphaned_node",
                            "severity": "warning",
                            "message": f"Node '{node['id']}' has no connections",
                            "details": {"node_id": node["id"], "node_type": node["type"]}
                        })
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸Ñ€Ð¾Ñ‚ÑÐºÐ¸Ñ… Ñ€Ñ‘Ð±ÐµÑ€
            if check_orphaned_edges:
                cursor.execute("""
                    SELECT e.id, e.source_id, e.target_id, e.type
                    FROM edges e
                    LEFT JOIN nodes n1 ON e.source_id = n1.id
                    LEFT JOIN nodes n2 ON e.target_id = n2.id
                    WHERE n1.id IS NULL OR n2.id IS NULL
                """)
                orphaned_edges = cursor.fetchall()
                checks_performed.append("orphaned_edges")
                if not orphaned_edges:
                    click.echo("   âœ… Ð¡Ð¸Ñ€Ð¾Ñ‚ÑÐºÐ¸Ðµ Ñ€Ñ‘Ð±Ñ€Ð°: Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾")
                else:
                    click.echo(f"   âŒ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑÐ¸Ñ€Ð¾Ñ‚ÑÐºÐ¸Ñ… Ñ€Ñ‘Ð±ÐµÑ€: {len(orphaned_edges)}")
                    for edge in orphaned_edges[:10]:  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10
                        issues.append({
                            "type": "orphaned_edge",
                            "severity": "error",
                            "message": f"Edge '{edge['id']}' references non-existent node",
                            "details": {
                                "edge_id": edge["id"],
                                "source_id": edge["source_id"],
                                "target_id": edge["target_id"],
                                "edge_type": edge["type"]
                            }
                        })
        
        conn.close()
        
        click.echo()
        click.echo("=" * 80)
        if not issues:
            click.echo("âœ… Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð²Ð°Ð»Ð¸Ð´Ð½Ð°! ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼ Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¾.")
        else:
            click.echo(f"âš ï¸  ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼: {len(issues)}")
            click.echo()
            for issue in issues[:20]:  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 20
                severity_icon = "âŒ" if issue["severity"] == "error" else "âš ï¸"
                click.echo(f"{severity_icon} [{issue['type']}] {issue['message']}")
        click.echo("=" * 80)
        click.echo(f"ðŸ“Š Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¾Ðº: {', '.join(checks_performed)}")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐµ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…!")
        click.echo("=" * 80)
        click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("calculate-importance")
@click.option(
    "--record-id",
    required=True,
    help="ID Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸",
)
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
)
@click.option(
    "--entity-weight",
    type=float,
    default=0.1,
    help="Ð’ÐµÑ Ð·Ð° ÐºÐ°Ð¶Ð´ÑƒÑŽ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÑŒ",
)
@click.option(
    "--task-weight",
    type=float,
    default=0.3,
    help="Ð’ÐµÑ Ð·Ð° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸",
)
@click.option(
    "--length-weight",
    type=float,
    default=0.2,
    help="Ð’ÐµÑ Ð·Ð° Ð´Ð»Ð¸Ð½Ñƒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ",
)
@click.option(
    "--search-hits-weight",
    type=float,
    default=0.4,
    help="Ð’ÐµÑ Ð·Ð° Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñƒ Ð¿Ð¾Ð¸ÑÐºÐ°",
)
def calculate_importance(record_id, db_path, entity_weight, task_weight, length_weight, search_hits_weight):
    """ðŸ“Š Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸
    
    Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ importance score (0.0-1.0) Ð´Ð»Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ:
    - ÐÐ°Ð»Ð¸Ñ‡Ð¸Ñ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹
    - ÐÐ°Ð»Ð¸Ñ‡Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡/action items
    - Ð”Ð»Ð¸Ð½Ñ‹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
    - Ð§Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ°
    """
    from ..memory.importance_scoring import ImportanceScorer
    from ..memory.typed_graph import TypedGraphMemory
    import sqlite3
    
    click.echo(f"ðŸ“Š Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸: {record_id}")
    click.echo()
    
    try:
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð³Ñ€Ð°Ñ„ Ð¸ scorer
        graph = TypedGraphMemory(db_path=str(db_path))
        scorer = ImportanceScorer(
            entity_weight=entity_weight,
            task_weight=task_weight,
            length_weight=length_weight,
            search_hits_weight=search_hits_weight
        )
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð¸Ð· Ð‘Ð”
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM nodes WHERE id = ?", (record_id,))
        node = cursor.fetchone()
        
        if not node:
            click.echo(f"âŒ Ð—Ð°Ð¿Ð¸ÑÑŒ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {record_id}")
            raise click.Abort()
        
        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ ÑƒÐ·ÐµÐ» Ð² ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ
        node_dict = dict(node)
        properties = json.loads(node_dict.get("properties", "{}") or "{}")
        node_dict.update(properties)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð° Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸ Ñ‚.Ð´.)
        metadata = {
            "_search_hits": properties.get("_search_hits", 0)
        }
        
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ
        importance_score = scorer.compute_importance(node_dict, metadata)
        
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ð´ÐµÑ‚Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        factors = {}
        entities = node_dict.get("entities") or properties.get("entities", [])
        if entities:
            factors["entities"] = min(len(entities) * entity_weight, 0.5)
        if node_dict.get("has_task") or node_dict.get("is_action_item") or properties.get("has_task") or properties.get("is_action_item"):
            factors["task"] = task_weight
        text = node_dict.get("text", "") or node_dict.get("content", "") or properties.get("content", "")
        if len(text) > 500:
            factors["length"] = length_weight
        elif len(text) > 200:
            factors["length"] = length_weight * 0.5
        if metadata.get("_search_hits", 0) > 0:
            factors["search_hits"] = min(metadata["_search_hits"] / 10.0, 1.0) * search_hits_weight
        
        conn.close()
        
        click.echo("=" * 80)
        click.echo("ðŸ“Š Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸")
        click.echo("=" * 80)
        click.echo(f"ðŸ“ Ð—Ð°Ð¿Ð¸ÑÑŒ: {record_id}")
        click.echo(f"â­ Importance Score: {importance_score:.3f} (0.0 - 1.0)")
        click.echo()
        click.echo("ðŸ“ˆ Ð¤Ð°ÐºÑ‚Ð¾Ñ€Ñ‹:")
        for factor, value in factors.items():
            click.echo(f"   â€¢ {factor}: {value:.3f}")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¸ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸!")
        click.echo("=" * 80)
        click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("prune-memory")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
)
@click.option(
    "--max-records",
    type=int,
    default=100000,
    help="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹",
)
@click.option(
    "--eviction-threshold",
    type=float,
    default=0.7,
    help="ÐŸÐ¾Ñ€Ð¾Ð³ eviction score Ð´Ð»Ñ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ (0.0-1.0)",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð°Ð½Ð°Ð»Ð¸Ð·, Ð±ÐµÐ· ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ",
)
@click.option(
    "--source",
    help="Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÑƒ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)",
)
def prune_memory(db_path, max_records, eviction_threshold, dry_run, source):
    """ðŸ§¹ ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð½ÐµÐ²Ð°Ð¶Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
    
    Ð£Ð´Ð°Ð»ÑÐµÑ‚ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ñ Ð½Ð¸Ð·ÐºÐ¾Ð¹ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð¼ Ð‘Ð”.
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ (Importance Scoring).
    """
    from ..memory.importance_scoring import MemoryPruner, EvictionScorer
    from ..memory.typed_graph import TypedGraphMemory
    import sqlite3
    
    click.echo("ðŸ§¹ ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸")
    click.echo()
    
    if dry_run:
        click.echo("ðŸ”¸ Ð ÐµÐ¶Ð¸Ð¼ DRY RUN - Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð½Ðµ Ð±ÑƒÐ´ÑƒÑ‚ ÑƒÐ´Ð°Ð»ÐµÐ½Ñ‹")
        click.echo()
    
    try:
        graph = TypedGraphMemory(db_path=str(db_path))
        eviction_scorer = EvictionScorer()
        pruner = MemoryPruner(
            eviction_scorer=eviction_scorer,
            max_messages=max_records,
            eviction_threshold=eviction_threshold
        )
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        query = "SELECT * FROM nodes"
        params = []
        if source:
            query += " WHERE properties LIKE ?"
            params.append(f'%"source": "{source}"%')
        
        cursor.execute(query, params)
        nodes = cursor.fetchall()
        
        current_count = len(nodes)
        click.echo(f"ðŸ“Š Ð¢ÐµÐºÑƒÑ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {current_count}")
        
        if not pruner.should_prune(current_count):
            click.echo("âœ… ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ (ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Ð¿Ñ€ÐµÐ´ÐµÐ»Ð°Ñ… Ð»Ð¸Ð¼Ð¸Ñ‚Ð°)")
            conn.close()
            return
        
        click.echo(f"âš ï¸  ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½ Ð»Ð¸Ð¼Ð¸Ñ‚ ({max_records}), Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ°")
        click.echo()
        
        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ ÑƒÐ·Ð»Ñ‹ Ð² ÑÐ»Ð¾Ð²Ð°Ñ€Ð¸
        messages = []
        for node in nodes:
            node_dict = dict(node)
            properties = json.loads(node_dict.get("properties", "{}") or "{}")
            node_dict.update(properties)
            # Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ ÐµÑÑ‚ÑŒ Ð¿Ð¾Ð»Ðµ id Ð¸Ð»Ð¸ msg_id Ð´Ð»Ñ get_eviction_candidates
            if "id" not in node_dict and "msg_id" not in node_dict:
                node_dict["id"] = node_dict.get("node_id") or node["id"]
            messages.append(node_dict)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð½Ð° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ
        candidates = pruner.get_eviction_candidates(
            messages,
            threshold=eviction_threshold
        )
        
        click.echo(f"ðŸŽ¯ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð½Ð° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ðµ: {len(candidates)}")
        click.echo()
        
        if not dry_run and candidates:
            click.echo("ðŸ—‘ï¸  Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹...")
            removed_count = 0
            for candidate in candidates:
                try:
                    # get_eviction_candidates Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ msg_id
                    node_id = candidate.get("msg_id") or candidate.get("message", {}).get("id")
                    if node_id:
                        graph.delete_node(node_id)
                        removed_count += 1
                except Exception as e:
                    click.echo(f"   âš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ {node_id}: {e}")
            
            click.echo(f"   âœ… Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {removed_count}")
        
        conn.close()
        
        click.echo()
        click.echo("=" * 80)
        if dry_run:
            click.echo("ðŸ”¸ DRY RUN Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½")
            click.echo(f"ðŸ“Š Ð‘ÑƒÐ´ÐµÑ‚ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {len(candidates)}")
        else:
            click.echo("âœ… ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°")
        click.echo("=" * 80)
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐµ Ð¿Ð°Ð¼ÑÑ‚Ð¸!")
        click.echo("=" * 80)
        click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


@cli.command("update-importance-scores")
@click.option(
    "--db-path",
    default="data/memory_graph.db",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="ÐŸÑƒÑ‚ÑŒ Ðº SQLite Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
)
@click.option(
    "--source",
    help="ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°",
)
@click.option(
    "--batch-size",
    type=int,
    default=1000,
    help="Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð° Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸",
)
def update_importance_scores(db_path, source, batch_size):
    """ðŸ”„ ÐœÐ°ÑÑÐ¾Ð²Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
    
    ÐŸÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ importance scores Ð´Ð»Ñ Ð²ÑÐµÑ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ….
    ÐŸÐ¾Ð»ÐµÐ·Ð½Ð¾ Ð¿Ð¾ÑÐ»Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð¸Ð»Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð¾Ñ†ÐµÐ½ÐºÐ¸.
    """
    from ..memory.importance_scoring import ImportanceScorer
    from ..memory.typed_graph import TypedGraphMemory
    import sqlite3
    
    click.echo("ðŸ”„ ÐœÐ°ÑÑÐ¾Ð²Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    click.echo()
    
    try:
        graph = TypedGraphMemory(db_path=str(db_path))
        scorer = ImportanceScorer()
        
        conn = sqlite3.connect(str(db_path))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð¿Ð¸ÑÐ¸
        query = "SELECT * FROM nodes"
        params = []
        if source:
            query += " WHERE properties LIKE ?"
            params.append(f'%"source": "{source}"%')
        
        cursor.execute(query, params)
        nodes = cursor.fetchall()
        
        total_nodes = len(nodes)
        click.echo(f"ðŸ“Š Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {total_nodes}")
        click.echo()
        
        updated_count = 0
        importance_scores = []
        
        for i, node in enumerate(nodes, 1):
            try:
                node_dict = dict(node)
                properties = json.loads(node_dict.get("properties", "{}"))
                node_dict.update(properties)
                
                metadata = {
                    "_search_hits": properties.get("_search_hits", 0)
                }
                
                importance_score = scorer.compute_importance(node_dict, metadata)
                importance_scores.append(importance_score)
                
                # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ properties Ñ Ð½Ð¾Ð²Ñ‹Ð¼ importance_score
                properties["_importance_score"] = importance_score
                
                # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑƒÐ·ÐµÐ» Ð² Ð³Ñ€Ð°Ñ„Ðµ
                graph.update_node(
                    node_id=node_dict["id"],
                    properties=properties
                )
                
                updated_count += 1
                
                if i % batch_size == 0:
                    click.echo(f"   â³ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾: {i}/{total_nodes} ({i*100//total_nodes}%)")
            
            except Exception as e:
                node_id_str = node_dict.get("id", "unknown")
                click.echo(f"   âš ï¸  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ {node_id_str}: {e}")
        
        conn.close()
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        avg_importance = sum(importance_scores) / len(importance_scores) if importance_scores else 0
        min_importance = min(importance_scores) if importance_scores else 0
        max_importance = max(importance_scores) if importance_scores else 0
        
        click.echo()
        click.echo("=" * 80)
        click.echo("âœ… ÐŸÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½")
        click.echo("=" * 80)
        click.echo(f"ðŸ“Š ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {updated_count}")
        click.echo(f"â­ Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ: {avg_importance:.3f}")
        click.echo(f"ðŸ“‰ ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ: {min_importance:.3f}")
        click.echo(f"ðŸ“ˆ ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ: {max_importance:.3f}")
        click.echo()
        
    except Exception as e:
        click.echo()
        click.echo("=" * 80)
        click.echo("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿ÐµÑ€ÐµÑÑ‡Ñ‘Ñ‚Ðµ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚Ð¸!")
        click.echo("=" * 80)
        click.echo(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        import traceback
        traceback.print_exc()
        raise click.Abort()


def main():
    """Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ CLI"""
    cli()


if __name__ == "__main__":
    main()
