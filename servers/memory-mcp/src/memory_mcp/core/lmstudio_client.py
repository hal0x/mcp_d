#!/usr/bin/env python3
"""–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LM Studio Server API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""

import asyncio
import logging
import random
import re
from typing import Any, Dict, List, Optional

import aiohttp

from .constants import (
    DEFAULT_TIMEOUT_CONNECT,
    DEFAULT_TIMEOUT_MAX_RETRY,
    HTTP_CONNECTOR_LIMIT,
    HTTP_CONNECTOR_LIMIT_PER_HOST,
)

logger = logging.getLogger(__name__)


class LMStudioEmbeddingClient:
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ LM Studio Server.
    
    model_name: —Ç–æ–ª—å–∫–æ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (/v1/embeddings)
    llm_model_name: –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ (/v1/chat/completions)
    """

    def __init__(
        self,
        model_name: str = "text-embedding-qwen3-embedding-0.6b",
        llm_model_name: Optional[str] = None,
        base_url: str = "http://127.0.0.1:1234",
        max_text_length: int = 16384,
    ):
        self.model_name = model_name
        self.llm_model_name = llm_model_name
        self.base_url = base_url.rstrip("/")
        self.max_text_length = max_text_length
        self.session = None
        self._embedding_dimension: Optional[int] = None
        self._reasoning_models = {
            "gpt-oss-20b",
            "gpt-oss-20b:latest",
            "gpt-oss",
            "deepseek",
            "deepseek-reasoner",
        }

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥."""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥."""
        if self.session:
            await self.session.close()

    async def check_model_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å."""
        try:
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            async with self.session.get(f"{self.base_url}/v1/models") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model.get("id", "") for model in data.get("data", [])]
                    if self.model_name not in models:
                        logger.warning(
                            f"–ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ø–∏—Å–∫–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. "
                            f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(models[:5])}"
                        )
            
            test_payload = {"model": self.model_name, "input": "test"}
            async with self.session.post(
                f"{self.base_url}/v1/embeddings",
                json=test_payload,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    if "data" in data and len(data.get("data", [])) > 0:
                        embedding = data["data"][0].get("embedding")
                        if embedding and isinstance(embedding, list):
                            if self._embedding_dimension is None:
                                self._embedding_dimension = len(embedding)
                            return True
                
                if response.status != 200:
                    error_text = await response.text()
                    logger.warning(
                        f"–ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. "
                        f"HTTP {response.status}: {error_text[:200]}"
                    )
                return False
                
        except asyncio.TimeoutError:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏ '{self.model_name}'")
            return False
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏ '{self.model_name}': {e}")
            return False

    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        embeddings = await self.generate_embeddings([text])
        if embeddings and self._embedding_dimension is None:
            self._embedding_dimension = len(embeddings[0])
        return embeddings[0] if embeddings else [0.0] * (self._embedding_dimension or 1024)

    async def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤."""
        if not texts:
            return []

        if not await self.check_model_availability():
            logger.error("LM Studio Server –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            default_dim = self._embedding_dimension or 1024
            return [[0.0] * default_dim] * len(texts)

        if len(texts) == 1:
            text_preview = texts[0][:30] + "..." if len(texts[0]) > 30 else texts[0]
            logger.debug(f"üî§ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {text_preview}")
            embedding = await self._process_single_text_async(texts[0], 0, 1)
            return [embedding] if embedding else [[0.0] * (self._embedding_dimension or 1024)]

        logger.info(f"üî§ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size}...")

        batches = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batches.append((i, batch))

        all_embeddings = []
        default_dim = self._embedding_dimension or 1024

        for batch_idx, (start_idx, batch_texts) in enumerate(batches):
            try:
                logger.debug(f"üî§ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_idx + 1}/{len(batches)} ({len(batch_texts)} —Ç–µ–∫—Å—Ç–æ–≤)")
                
                batch_embeddings = await self._generate_batch_embeddings(batch_texts)
                
                if batch_embeddings:
                    all_embeddings.extend(batch_embeddings)
                    if self._embedding_dimension is None and batch_embeddings[0]:
                        self._embedding_dimension = len(batch_embeddings[0])
                else:
                    logger.warning(f"–ë–∞—Ç—á {batch_idx + 1} –Ω–µ —É–¥–∞–ª—Å—è, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
                    all_embeddings.extend([[0.0] * default_dim] * len(batch_texts))
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {batch_idx + 1}: {e}")
                all_embeddings.extend([[0.0] * default_dim] * len(batch_texts))

        return all_embeddings

    async def _generate_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤."""
        if not texts:
            return []
        
        processed_texts = []
        for text in texts:
            if len(text) > self.max_text_length:
                logger.warning(
                    f"–¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({len(text)} > {self.max_text_length}), –æ–±—Ä–µ–∑–∞–µ–º"
                )
                text = text[:self.max_text_length]
            processed_texts.append(text)
        
        payload = {"model": self.model_name, "input": processed_texts}
        
        if not self.session:
            connector = aiohttp.TCPConnector(
                limit=HTTP_CONNECTOR_LIMIT,
                limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,
                ttl_dns_cache=300,
                use_dns_cache=True,
                enable_cleanup_closed=True,
                force_close=True,
                ssl=False,
            )
            timeout = aiohttp.ClientTimeout(
                total=DEFAULT_TIMEOUT_MAX_RETRY,
                connect=DEFAULT_TIMEOUT_CONNECT,
                sock_read=DEFAULT_TIMEOUT_MAX_RETRY,
                sock_connect=DEFAULT_TIMEOUT_CONNECT,
            )
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={"Connection": "close"},
            )
        
        default_dim = self._embedding_dimension or 1024
        for attempt in range(3):
            try:
                timeout_seconds = 60 + (attempt * 30)
                
                if attempt > 0:
                    logger.debug(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio ({attempt + 1}/3)")
                
                async with asyncio.timeout(timeout_seconds):
                    async with self.session.post(
                        f"{self.base_url}/v1/embeddings", json=payload
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            if "data" in data and isinstance(data["data"], list):
                                sorted_data = sorted(data["data"], key=lambda x: x.get("index", 0))
                                embeddings = [item.get("embedding") for item in sorted_data]
                                
                                if len(embeddings) == len(processed_texts) and all(emb for emb in embeddings):
                                    if self._embedding_dimension is None and embeddings[0]:
                                        self._embedding_dimension = len(embeddings[0])
                                    logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–µ–º")
                                    return embeddings
                                else:
                                    logger.warning(
                                        f"–ü–æ–ª—É—á–µ–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤–º–µ—Å—Ç–æ {len(processed_texts)}"
                                    )
                                    result = []
                                    for i, emb in enumerate(embeddings):
                                        if emb and isinstance(emb, list):
                                            result.append(emb)
                                        else:
                                            result.append([0.0] * default_dim)
                                    while len(result) < len(processed_texts):
                                        result.append([0.0] * default_dim)
                                    return result
                            else:
                                logger.error("LM Studio –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∞—Ç—á–∞")
                                if attempt < 2:
                                    await asyncio.sleep(2 + attempt)
                                    continue
                                return [[0.0] * default_dim] * len(processed_texts)
                        else:
                            error_text = await response.text()
                            logger.error(
                                f"–û—à–∏–±–∫–∞ API LM Studio –ø—Ä–∏ –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å–µ: {response.status} - {error_text[:200]}"
                            )
                            if attempt < 2:
                                await asyncio.sleep(2 + attempt)
                                continue
                            return [[0.0] * default_dim] * len(processed_texts)
            except asyncio.TimeoutError:
                logger.error(
                    f"–¢–∞–π–º–∞—É—Ç –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3)"
                )
                if attempt < 2:
                    delay = (2 ** attempt) + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                return [[0.0] * default_dim] * len(processed_texts)
            except aiohttp.ClientError as e:
                error_str = str(e)
                is_connection_reset = "Connection reset" in error_str or "Errno 54" in error_str
                
                if is_connection_reset:
                    logger.warning(
                        f"–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å LM Studio —Ä–∞–∑–æ—Ä–≤–∞–Ω–æ –ø—Ä–∏ –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å–µ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3). "
                        f"–û—à–∏–±–∫–∞: {e}"
                    )
                    if attempt < 2:
                        try:
                            if self.session:
                                await self.session.close()
                        except Exception:
                            pass
                        connector = aiohttp.TCPConnector(
                            limit=HTTP_CONNECTOR_LIMIT,
                            limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,
                            ttl_dns_cache=300,
                            use_dns_cache=True,
                            enable_cleanup_closed=True,
                            force_close=True,
                            ssl=False,
                        )
                        timeout = aiohttp.ClientTimeout(
                            total=DEFAULT_TIMEOUT_MAX_RETRY,
                            connect=DEFAULT_TIMEOUT_CONNECT,
                            sock_read=DEFAULT_TIMEOUT_MAX_RETRY,
                            sock_connect=DEFAULT_TIMEOUT_CONNECT,
                        )
                        self.session = aiohttp.ClientSession(
                            connector=connector,
                            timeout=timeout,
                            headers={"Connection": "close"},
                        )
                        delay = (5 * (2 ** attempt)) + random.uniform(0, 2)
                        logger.debug(f"–û–∂–∏–¥–∞–Ω–∏–µ {delay:.2f} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å–∞...")
                        await asyncio.sleep(delay)
                        continue
                else:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å LM Studio –ø—Ä–∏ –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å–µ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3): {e}"
                    )
                    if attempt < 2:
                        delay = (2 ** attempt) * 2 + random.uniform(0, 1)
                        await asyncio.sleep(delay)
                        continue
                return [[0.0] * default_dim] * len(processed_texts)
            except Exception as e:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å–µ –∫ LM Studio: {e}")
                if attempt < 2:
                    delay = (2 ** attempt) * 2 + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                return [[0.0] * default_dim] * len(processed_texts)
        
        return [[0.0] * default_dim] * len(processed_texts)

    async def _process_single_text_async(
        self, text: str, index: int, total: int
    ) -> List[float]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
        try:
            text_chunks = self._split_text_into_chunks(text)

            if len(text_chunks) == 1:
                embedding = await self._generate_single_embedding(text)
                if embedding:
                    return embedding
                else:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {text[:50]}..."
                    )
                    default_dim = self._embedding_dimension or 1024
                    return [0.0] * default_dim
            else:
                chunk_embeddings = []
                for j, chunk in enumerate(text_chunks):
                    if len(text_chunks) > 3 and j == 0:
                        logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º ({len(text_chunks)} —á–∞—Å—Ç–µ–π)")
                    chunk_embedding = await self._generate_single_embedding(chunk)
                    if chunk_embedding:
                        chunk_embeddings.append(chunk_embedding)
                    else:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞—Å—Ç–∏ {j+1}")
                        default_dim = self._embedding_dimension or 1024
                        chunk_embeddings.append([0.0] * default_dim)

                if chunk_embeddings:
                    averaged_embedding = self._average_embeddings(chunk_embeddings)
                    if len(chunk_embeddings) > 3:
                        logger.debug(
                            f"–£—Å—Ä–µ–¥–Ω–µ–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ {len(chunk_embeddings)} —á–∞—Å—Ç–µ–π"
                        )
                    return averaged_embedding
                else:
                    default_dim = self._embedding_dimension or 1024
                    return [0.0] * default_dim

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            default_dim = self._embedding_dimension or 1024
            return [0.0] * default_dim

    def _split_text_into_chunks(self, text: str, max_length: int = None) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."""
        if max_length is None:
            max_length = self.max_text_length

        estimated_tokens = len(text) // 1.2
        max_tokens = max_length // 3.5

        if estimated_tokens <= max_tokens:
            return [text]

        chunks = []
        start = 0
        chunk_size_chars = max_tokens * 4

        while start < len(text):
            end = start + chunk_size_chars

            if end >= len(text):
                chunks.append(text[start:])
                break

            last_space = text.rfind(" ", start, end)
            if last_space > start + chunk_size_chars * 0.7:
                chunks.append(text[start:last_space])
                start = last_space + 1
            else:
                chunks.append(text[start:end])
                start = end

        if len(chunks) > 1:
            chunk_tokens = [len(chunk) // 3.5 for chunk in chunks]
            logger.debug(
                f"–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π "
                f"(~{estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤ -> ~{sum(chunk_tokens)} —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–∞—Ö)"
            )

        return chunks

    def _average_embeddings(self, embeddings: List[List[float]]) -> List[float]:
        """–£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –æ–¥–∏–Ω."""
        if not embeddings:
            default_dim = self._embedding_dimension or 1024
            return [0.0] * default_dim

        if len(embeddings) == 1:
            return embeddings[0]

        dimension = len(embeddings[0])
        averaged = []

        for i in range(dimension):
            sum_val = sum(emb[i] for emb in embeddings)
            averaged.append(sum_val / len(embeddings))

        return averaged

    def _truncate_text(self, text: str, max_length: int = None) -> str:
        """–û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã."""
        if max_length is None:
            max_length = self.max_text_length

        if len(text) <= max_length:
            return text

        truncated = text[:max_length]
        last_space = truncated.rfind(" ")
        if last_space > max_length * 0.8:
            result = truncated[:last_space]
        else:
            result = truncated

        logger.warning(
            f"–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω —Å {len(text)} –¥–æ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤"
        )
        return result

    async def _generate_single_embedding(self, text: str) -> Optional[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        if len(text) > self.max_text_length:
            logger.warning(
                f"–¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({len(text)} > {self.max_text_length}), –æ–±—Ä–µ–∑–∞–µ–º"
            )
            text = text[: self.max_text_length]

        payload = {"model": self.model_name, "input": text}

        if not self.session:
            connector = aiohttp.TCPConnector(
                limit=HTTP_CONNECTOR_LIMIT,
                limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,
                ttl_dns_cache=300,
                use_dns_cache=True,
                enable_cleanup_closed=True,
                force_close=True,
                ssl=False,
            )
            timeout = aiohttp.ClientTimeout(
                total=DEFAULT_TIMEOUT_MAX_RETRY,
                connect=DEFAULT_TIMEOUT_CONNECT,
                sock_read=DEFAULT_TIMEOUT_MAX_RETRY,
                sock_connect=DEFAULT_TIMEOUT_CONNECT,
            )
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={"Connection": "close"},
            )

        for attempt in range(3):
            try:
                timeout_seconds = 60 + (attempt * 30)

                if attempt > 0:
                    logger.debug(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∫ LM Studio ({attempt + 1}/3)")

                async with asyncio.timeout(timeout_seconds):
                    async with self.session.post(
                        f"{self.base_url}/v1/embeddings", json=payload
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            if "data" in data and len(data["data"]) > 0:
                                embedding = data["data"][0].get("embedding")
                                if embedding:
                                    if self._embedding_dimension is None:
                                        self._embedding_dimension = len(embedding)
                                    return embedding
                                else:
                                    logger.error("LM Studio –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π embedding")
                                    if attempt < 2:
                                        await asyncio.sleep(2 + attempt)
                                        continue
                                    return None
                            else:
                                logger.error("LM Studio –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö")
                                if attempt < 2:
                                    await asyncio.sleep(2 + attempt)
                                    continue
                                return None
                        else:
                            error_text = await response.text()
                            logger.error(
                                f"–û—à–∏–±–∫–∞ API LM Studio: {response.status} - {error_text}"
                            )
                            if attempt < 2:
                                await asyncio.sleep(2 + attempt)
                                continue
                            return None
            except asyncio.TimeoutError:
                logger.error(
                    f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3)"
                )
                if attempt < 2:
                    delay = (2 ** attempt) + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                return None
            except aiohttp.ClientError as e:
                error_str = str(e)
                is_connection_reset = "Connection reset" in error_str or "Errno 54" in error_str
                
                if is_connection_reset:
                    logger.warning(
                        f"–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å LM Studio —Ä–∞–∑–æ—Ä–≤–∞–Ω–æ —Å–µ—Ä–≤–µ—Ä–æ–º (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3). "
                        f"–í–æ–∑–º–æ–∂–Ω–æ, —Å–µ—Ä–≤–µ—Ä –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è. "
                        f"–û—à–∏–±–∫–∞: {e}"
                    )
                    if attempt < 2:
                        try:
                            if self.session:
                                await self.session.close()
                        except Exception:
                            pass
                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
                        connector = aiohttp.TCPConnector(
                            limit=HTTP_CONNECTOR_LIMIT,
                            limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,
                            ttl_dns_cache=300,
                            use_dns_cache=True,
                            enable_cleanup_closed=True,
                            force_close=True,
                            ssl=False,
                        )
                        timeout = aiohttp.ClientTimeout(
                            total=DEFAULT_TIMEOUT_MAX_RETRY,
                            connect=DEFAULT_TIMEOUT_CONNECT,
                            sock_read=DEFAULT_TIMEOUT_MAX_RETRY,
                            sock_connect=DEFAULT_TIMEOUT_CONNECT,
                        )
                        self.session = aiohttp.ClientSession(
                            connector=connector,
                            timeout=timeout,
                            headers={"Connection": "close"},
                        )
                        delay = (5 * (2 ** attempt)) + random.uniform(0, 2)
                        logger.debug(f"–û–∂–∏–¥–∞–Ω–∏–µ {delay:.2f} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                        await asyncio.sleep(delay)
                        continue
                else:
                    logger.error(
                        f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å LM Studio (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3): {e}"
                    )
                    if attempt < 2:
                        delay = (2 ** attempt) * 2 + random.uniform(0, 1)
                        await asyncio.sleep(delay)
                        continue
                return None
            except aiohttp.InvalidURL as e:
                logger.error(
                    f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –≤ –∑–∞–ø—Ä–æ—Å–µ –∫ LM Studio (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3): {e}"
                )
                return None
            except aiohttp.ServerTimeoutError as e:
                logger.error(f"–¢–∞–π–º–∞—É—Ç —Å–µ—Ä–≤–µ—Ä–∞ LM Studio (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3): {e}")
                if attempt < 2:
                    delay = (2 ** attempt) * 3 + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                return None
            except Exception as e:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LM Studio: {e}")
                if attempt < 2:
                    delay = (2 ** attempt) * 2 + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                return None

        return None

    async def test_connection(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ LM Studio Server."""
        result = {
            "lmstudio_available": False,
            "model_available": False,
            "model_name": self.model_name,
            "base_url": self.base_url,
            "error": None,
        }

        try:
            if not self.session:
                self.session = aiohttp.ClientSession()

            try:
                async with self.session.get(
                    f"{self.base_url}/v1/models",
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    if response.status == 200:
                        result["lmstudio_available"] = True
                        data = await response.json()
                        models = data.get("data", [])
                        result["available_models"] = [m.get("id", "") for m in models]
                    else:
                        result["error"] = f"HTTP {response.status}: {await response.text()}"
            except asyncio.TimeoutError:
                result["error"] = "–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ LM Studio Server"
                return result
            except Exception as e:
                result["error"] = f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {str(e)}"
                return result

            if result["lmstudio_available"]:
                result["model_available"] = await self.check_model_availability()
                if not result["model_available"]:
                    if not result.get("error"):
                        result["error"] = (
                            f"–ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. "
                            f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ LM Studio –∏ –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ /v1/embeddings endpoint."
                        )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            result["error"] = str(e)

        return result

    def _estimate_tokens(self, text: str) -> int:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
        return len(text) // 1.2

    def _split_prompt(self, prompt: str, max_prompt_tokens: int = 31000) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
        estimated_tokens = self._estimate_tokens(prompt)

        if estimated_tokens <= max_prompt_tokens:
            return [prompt]

        logger.warning(
            f"‚ö†Ô∏è  –ü—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤), —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ (–ª–∏–º–∏—Ç {max_prompt_tokens})"
        )

        conversation_markers = [
            "–ò–°–•–û–î–ù–´–ô –†–ê–ó–ì–û–í–û–†:",
            "–†–∞–∑–≥–æ–≤–æ—Ä:",
            "–°–æ–æ–±—â–µ–Ω–∏—è:",
            "conversation_text:",
            "conversation:",
        ]

        system_part = ""
        conversation_part = ""

        for marker in conversation_markers:
            if marker in prompt:
                parts = prompt.split(marker, 1)
                if len(parts) == 2:
                    system_part = parts[0] + marker
                    conversation_part = parts[1]
                    break

        if not conversation_part:
            max_chars = int(max_prompt_tokens * 1.2)
            chunks = []
            for i in range(0, len(prompt), max_chars):
                chunks.append(prompt[i : i + max_chars])
            return chunks

        max_conversation_tokens = (
            max_prompt_tokens - self._estimate_tokens(system_part) - 100
        )
        max_conversation_chars = int(max_conversation_tokens * 1.2)

        conversation_chunks = []
        for i in range(0, len(conversation_part), max_conversation_chars):
            conversation_chunks.append(
                conversation_part[i : i + max_conversation_chars]
            )

        prompts = []
        for i, chunk in enumerate(conversation_chunks):
            if i == 0:
                chunk_prompt = system_part + chunk
            else:
                chunk_prompt = f"{system_part}\n(–ü–†–û–î–û–õ–ñ–ï–ù–ò–ï –†–ê–ó–ì–û–í–û–†–ê - —á–∞—Å—Ç—å {i+1}/{len(conversation_chunks)})\n{chunk}"
            prompts.append(chunk_prompt)

        logger.info(f"üìù –ü—Ä–æ–º–ø—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(prompts)} —á–∞—Å—Ç–µ–π")
        return prompts

    async def generate_summary(
        self,
        prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 131072,
        top_p: float = 0.93,
        presence_penalty: float = 0.05,
        max_prompt_tokens: int = 100000,
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤."""
        prompt_parts = self._split_prompt(prompt, max_prompt_tokens)

        if len(prompt_parts) == 1:
            return await self._generate_single_summary(
                prompt_parts[0], temperature, max_tokens, top_p, presence_penalty
            )
        else:
            logger.info(
                f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ —á–∞—Å—Ç—è–º ({len(prompt_parts)} —á–∞—Å—Ç–µ–π)"
            )
            summaries = []

            for i, part_prompt in enumerate(prompt_parts):
                logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i+1}/{len(prompt_parts)}")
                part_summary = await self._generate_single_summary(
                    part_prompt, temperature, max_tokens, top_p, presence_penalty
                )
                summaries.append(part_summary)

            combined = "\n\n".join(summaries)
            logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã {len(summaries)} —á–∞—Å—Ç–µ–π —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
            return combined

    def _is_reasoning_model(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å reasoning-–º–æ–¥–µ–ª—å—é."""
        if not model_name:
            return False
        model_lower = model_name.lower()
        for reasoning_model in self._reasoning_models:
            if reasoning_model.lower() in model_lower:
                return True
        return False

    async def _generate_single_summary(
        self,
        prompt: str,
        temperature: float,
        max_tokens: int,
        top_p: float,
        presence_penalty: float,
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —á–µ—Ä–µ–∑ LM Studio Server."""
        llm_model = self.llm_model_name
        if not llm_model:
            error_msg = (
                f"–û–®–ò–ë–ö–ê: –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω—É–∂–Ω–∞ LLM –º–æ–¥–µ–ª—å, –∞ –Ω–µ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. "
                f"–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: '{self.model_name}'. "
                f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llm_model_name –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ LMStudioEmbeddingClient –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞."
            )
            logger.error(error_msg)
            return f"–û—à–∏–±–∫–∞: {error_msg}"
        
        payload = {
            "model": llm_model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful AI assistant. Follow the user's instructions carefully and provide accurate responses.",
                },
                {"role": "user", "content": prompt},
            ],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "presence_penalty": presence_penalty,
            "stream": False,
        }

        for attempt in range(2):
            try:
                if not self.session:
                    connector = aiohttp.TCPConnector(
                        limit=HTTP_CONNECTOR_LIMIT,
                        limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,
                        ttl_dns_cache=300,
                        use_dns_cache=True,
                        enable_cleanup_closed=True,
                        force_close=True,
                        ssl=False,
                    )
                    timeout = aiohttp.ClientTimeout(
                        total=1800,  # 30 –º–∏–Ω—É—Ç –¥–ª—è LLM (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å–∞–º–º–∞—Ä–∏)
                        connect=DEFAULT_TIMEOUT_CONNECT,
                        sock_read=1800,  # –¢–∞–π–º–∞—É—Ç —á—Ç–µ–Ω–∏—è —Å–æ–∫–µ—Ç–∞
                    )
                    self.session = aiohttp.ClientSession(
                        connector=connector,
                        timeout=timeout,
                        headers={"Connection": "keep-alive"},  # Keep-alive –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                    )
                
                # –î–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º streaming, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ
                # Streaming –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–ª–∏–µ–Ω—Ç—É –ø–æ–ª—É—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                use_streaming = len(prompt) > 50000  # ~12.5k —Ç–æ–∫–µ–Ω–æ–≤
                
                if use_streaming:
                    payload["stream"] = True
                    logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è streaming –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ ({len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    
                    async with self.session.post(
                        f"{self.base_url}/v1/chat/completions", json=payload
                    ) as response:
                        if response.status == 200:
                            content_parts = []
                            async for line in response.content:
                                if line:
                                    try:
                                        line_text = line.decode('utf-8').strip()
                                        if line_text.startswith('data: '):
                                            json_str = line_text[6:]  # –£–±–∏—Ä–∞–µ–º 'data: '
                                            if json_str == '[DONE]':
                                                break
                                            chunk_data = await asyncio.to_thread(lambda: __import__('json').loads(json_str))
                                            if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                                delta = chunk_data['choices'][0].get('delta', {})
                                                if 'content' in delta:
                                                    content_parts.append(delta['content'])
                                    except Exception as e:
                                        logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ streaming chunk: {e}")
                                        continue
                            
                            content = ''.join(content_parts).strip()
                            if content:
                                return content
                            else:
                                logger.warning("Streaming –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
                                payload["stream"] = False
                
                # –û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å (–±–µ–∑ streaming –∏–ª–∏ fallback)
                if not use_streaming or payload.get("stream") is False:
                    payload["stream"] = False
                    async with self.session.post(
                        f"{self.base_url}/v1/chat/completions", json=payload
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            if "choices" in data and len(data["choices"]) > 0:
                                choice = data["choices"][0]
                                message = choice.get("message", {})
                                
                                content = message.get("content", "").strip()
                                reasoning = message.get("reasoning", "")
                                finish_reason = choice.get("finish_reason", "")
                                
                                if not content and reasoning:
                                    logger.warning(
                                        f"–ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç reasoning-–º–æ–¥–µ–ª–∏ '{llm_model}'. "
                                        f"–ò—Å–ø–æ–ª—å–∑—É–µ–º reasoning –∫–∞–∫ fallback. "
                                        f"Finish reason: {finish_reason}. "
                                        f"–í–æ–∑–º–æ–∂–Ω–æ, max_tokens ({max_tokens}) –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ content."
                                    )
                                    json_match = re.search(r"\{.*\}", reasoning, re.DOTALL)
                                    if json_match:
                                        content = json_match.group(0)
                                        logger.info("–ò–∑–≤–ª–µ—á–µ–Ω JSON –∏–∑ reasoning")
                                    else:
                                        content = reasoning.strip()
                                
                                if finish_reason == "length" and not content:
                                    logger.error(
                                        f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ '{llm_model}'. "
                                        f"–¢–µ–∫—É—â–∏–π max_tokens: {max_tokens}. "
                                        f"–í—Å–µ —Ç–æ–∫–µ–Ω—ã —É—à–ª–∏ –Ω–∞ reasoning. –£–≤–µ–ª–∏—á—å—Ç–µ max_tokens."
                                    )
                                
                                return content if content else "–û—à–∏–±–∫–∞: –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏"
                            else:
                                logger.error("LM Studio –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö")
                                return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞"
                        else:
                            error_text = await response.text()
                            logger.error(f"–û—à–∏–±–∫–∞ API LM Studio: {response.status} - {error_text}")
                            if attempt < 1:
                                await asyncio.sleep(3 + random.uniform(0, 1))
                                continue
                            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: HTTP {response.status}"
            except aiohttp.ClientError as e:
                error_str = str(e)
                is_connection_reset = "Connection reset" in error_str or "Errno 54" in error_str
                
                if is_connection_reset:
                    logger.warning(
                        f"–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å LM Studio —Ä–∞–∑–æ—Ä–≤–∞–Ω–æ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/2). "
                        f"–û—à–∏–±–∫–∞: {e}"
                    )
                    if attempt < 1:
                        try:
                            if self.session:
                                await self.session.close()
                        except Exception:
                            pass
                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
                        connector = aiohttp.TCPConnector(
                            limit=HTTP_CONNECTOR_LIMIT,
                            limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,
                            ttl_dns_cache=300,
                            use_dns_cache=True,
                            enable_cleanup_closed=True,
                            force_close=True,
                            ssl=False,
                        )
                        timeout = aiohttp.ClientTimeout(
                            total=1800,  # 30 –º–∏–Ω—É—Ç –¥–ª—è LLM (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å–∞–º–º–∞—Ä–∏)
                            connect=DEFAULT_TIMEOUT_CONNECT,
                            sock_read=1800,  # –¢–∞–π–º–∞—É—Ç —á—Ç–µ–Ω–∏—è —Å–æ–∫–µ—Ç–∞
                        )
                        self.session = aiohttp.ClientSession(
                            connector=connector,
                            timeout=timeout,
                            # –£–±–∏—Ä–∞–µ–º "Connection": "close" –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                        )
                        delay = 5 + random.uniform(0, 2)
                        logger.debug(f"–û–∂–∏–¥–∞–Ω–∏–µ {delay:.2f} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
                        await asyncio.sleep(delay)
                        continue
                else:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å LM Studio –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏: {e}")
                    if attempt < 1:
                        delay = 3 + random.uniform(0, 1)
                        await asyncio.sleep(delay)
                        continue
                return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
            except aiohttp.InvalidURL as e:
                logger.error(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –≤ –∑–∞–ø—Ä–æ—Å–µ –∫ LM Studio: {e}")
                return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL"
            except asyncio.TimeoutError:
                logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/2)")
                if attempt < 1:
                    delay = 5 + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞"
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
                if attempt < 1:
                    delay = 3 + random.uniform(0, 1)
                    await asyncio.sleep(delay)
                    continue
                return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
        
        return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –Ω–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫"

