#!/usr/bin/env python3
"""
–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
"""

import asyncio
import logging
from typing import Any, Dict, List, Optional

import aiohttp

from .constants import (
    DEFAULT_TIMEOUT_CONNECT,
    DEFAULT_TIMEOUT_MAX_RETRY,
    HTTP_CONNECTOR_LIMIT,
    HTTP_CONNECTOR_LIMIT_PER_HOST,
)

logger = logging.getLogger(__name__)


class OllamaEmbeddingClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama"""

    def __init__(
        self,
        model_name: str = "hf.co/lmstudio-community/Magistral-Small-2509-GGUF:Q4_K_M",
        llm_model_name: str = "hf.co/lmstudio-community/Magistral-Small-2509-GGUF:Q4_K_M",
        base_url: str = "http://localhost:11434",
        max_text_length: int = 16384,  # 4096 —Ç–æ–∫–µ–Ω–æ–≤ * 4 —Å–∏–º–≤–æ–ª–∞/—Ç–æ–∫–µ–Ω –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞
        llm_thinking_level: str | None = None,
    ):
        self.model_name = model_name
        self.llm_model_name = llm_model_name
        self.base_url = base_url
        self.max_text_length = max_text_length
        self.llm_thinking_level = llm_thinking_level
        self.session = None

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()

    async def check_model_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model["name"] for model in data.get("models", [])]
                    return self.model_name in models
                return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return False

    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        embeddings = await self.generate_embeddings([text])
        return embeddings[0] if embeddings else [0.0] * 5120

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        if not texts:
            return []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã
        if not await self.check_model_availability():
            logger.error("Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return [[0.0] * 5120] * len(texts)

        embeddings = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if len(texts) == 1:
            # –î–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ª–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            text_preview = texts[0][:30] + "..." if len(texts[0]) > 30 else texts[0]
            logger.debug(f"üî§ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {text_preview}")
        else:
            logger.info(f"üî§ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ...")

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        tasks = []
        for i, text in enumerate(texts):
            text_preview = text[:30] + "..." if len(text) > 30 else text
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥–æ–µ 10-–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ª–æ–≥–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤)
            if len(texts) > 1 and ((i + 1) % 10 == 0 or i == 0):
                logger.info(
                    f"üî§ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ [{i+1}/{len(texts)}]: {text_preview}"
                )
            task = self._process_single_text_async(text, i, len(texts))
            tasks.append(task)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        semaphore = asyncio.Semaphore(10)

        async def limited_task(task):
            async with semaphore:
                return await task

        limited_tasks = [limited_task(task) for task in tasks]
        results = await asyncio.gather(*limited_tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ {i+1}: {result}")
                embeddings.append([0.0] * 5120)
            else:
                embeddings.append(result)

        return embeddings

    async def _process_single_text_async(
        self, text: str, index: int, total: int
    ) -> List[float]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞—Å—Ç–∏
            text_chunks = self._split_text_into_chunks(text)

            if len(text_chunks) == 1:
                # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
                embedding = await self._generate_single_embedding(text)
                if embedding:
                    return embedding
                else:
                    logger.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {text[:50]}..."
                    )
                    return [0.0] * 5120
            else:
                # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º
                chunk_embeddings = []
                for j, chunk in enumerate(text_chunks):
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–±–æ–ª–µ–µ 3 —á–∞—Å—Ç–µ–π)
                    if len(text_chunks) > 3 and j == 0:
                        logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º ({len(text_chunks)} —á–∞—Å—Ç–µ–π)")
                    chunk_embedding = await self._generate_single_embedding(chunk)
                    if chunk_embedding:
                        chunk_embeddings.append(chunk_embedding)
                    else:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞—Å—Ç–∏ {j+1}")
                        chunk_embeddings.append([0.0] * 5120)

                # –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
                if chunk_embeddings:
                    averaged_embedding = self._average_embeddings(chunk_embeddings)
                    # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ª–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä—É–ø–ø)
                    if len(chunk_embeddings) > 3:
                        logger.debug(
                            f"–£—Å—Ä–µ–¥–Ω–µ–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ {len(chunk_embeddings)} —á–∞—Å—Ç–µ–π"
                        )
                    return averaged_embedding
                else:
                    return [0.0] * 5120

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return [0.0] * 5120

    def _split_text_into_chunks(self, text: str, max_length: int = None) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        if max_length is None:
            max_length = self.max_text_length

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º, –∞ –Ω–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        # –î–ª—è Magistral-Small-2509 –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ü–µ–Ω–∫—É: ~1.2 —Å–∏–º–≤–æ–ª–∞/—Ç–æ–∫–µ–Ω
        estimated_tokens = len(text) // 1.2
        max_tokens = max_length // 3.5

        if estimated_tokens <= max_tokens:
            return [text]

        chunks = []
        start = 0
        chunk_size_chars = max_tokens * 4  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö

        while start < len(text):
            end = start + chunk_size_chars

            if end >= len(text):
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∫—É—Å–æ–∫
                chunks.append(text[start:])
                break

            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö chunk_size_chars
            last_space = text.rfind(" ", start, end)
            if (
                last_space > start + chunk_size_chars * 0.7
            ):  # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ
                chunks.append(text[start:last_space])
                start = last_space + 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª
            else:
                # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ, –æ–±—Ä–µ–∑–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
                chunks.append(text[start:end])
                start = end

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–∞—Ö —á–∞–Ω–∫–æ–≤
        if len(chunks) > 1:
            chunk_tokens = [len(chunk) // 3.5 for chunk in chunks]
            logger.debug(
                f"–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π "
                f"(~{estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤ -> ~{sum(chunk_tokens)} —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–∞—Ö)"
            )

        return chunks

    def _average_embeddings(self, embeddings: List[List[float]]) -> List[float]:
        """–£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –æ–¥–∏–Ω"""
        if not embeddings:
            return [0.0] * 5120

        if len(embeddings) == 1:
            return embeddings[0]

        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –∫–∞–∂–¥–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é
        dimension = len(embeddings[0])
        averaged = []

        for i in range(dimension):
            sum_val = sum(emb[i] for emb in embeddings)
            averaged.append(sum_val / len(embeddings))

        return averaged

    def _truncate_text(self, text: str, max_length: int = None) -> str:
        """–û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        if max_length is None:
            max_length = self.max_text_length

        if len(text) <= max_length:
            return text

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ–±–µ–ª–∞, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞
        truncated = text[:max_length]
        last_space = truncated.rfind(" ")
        if last_space > max_length * 0.8:  # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ
            result = truncated[:last_space]
        else:
            result = truncated

        logger.warning(
            f"–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω —Å {len(text)} –¥–æ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤"
        )
        return result

    async def _generate_single_embedding(self, text: str) -> Optional[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        if len(text) > self.max_text_length:
            logger.warning(
                f"–¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({len(text)} > {self.max_text_length}), –æ–±—Ä–µ–∑–∞–µ–º"
            )
            text = text[: self.max_text_length]

        payload = {"model": self.model_name, "prompt": text}

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é –æ–¥–∏–Ω —Ä–∞–∑ –ø–µ—Ä–µ–¥ —Ü–∏–∫–ª–æ–º retry, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not self.session:
            connector = aiohttp.TCPConnector(
                limit=HTTP_CONNECTOR_LIMIT,
                limit_per_host=HTTP_CONNECTOR_LIMIT_PER_HOST,
                ttl_dns_cache=300,
                use_dns_cache=True,
                enable_cleanup_closed=True,
                force_close=True,
                ssl=False,
            )
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å–µ—Å—Å–∏–∏, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –±—É–¥—É—Ç –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö
            timeout = aiohttp.ClientTimeout(
                total=DEFAULT_TIMEOUT_MAX_RETRY,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è retry
                connect=DEFAULT_TIMEOUT_CONNECT,
                sock_read=DEFAULT_TIMEOUT_MAX_RETRY,
                sock_connect=DEFAULT_TIMEOUT_CONNECT,
            )
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={"Connection": "close"},
            )

        # –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ —Å —Ä–∞–∑—É–º–Ω—ã–º–∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
        for attempt in range(3):
            try:
                # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è Qwen3-Embedding-4B: 60, 90, 120 —Å–µ–∫—É–Ω–¥
                timeout_seconds = 60 + (attempt * 30)

                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–∫–∞—Ö
                if attempt > 0:
                    logger.debug(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∫ Ollama ({attempt + 1}/3)")

                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –≤—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                async with asyncio.timeout(timeout_seconds):
                    async with self.session.post(
                        f"{self.base_url}/api/embeddings", json=payload
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            return data.get("embedding")
                        else:
                            logger.error(
                                f"–û—à–∏–±–∫–∞ API Ollama: {response.status}"
                            )
                            if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                                await asyncio.sleep(2 + attempt)  # 2, 3 —Å–µ–∫—É–Ω–¥—ã
                                continue
                            return None

            except asyncio.TimeoutError:
                logger.error(
                    f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3)"
                )
                if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    await asyncio.sleep(2**attempt)
                    continue
                return None
            except aiohttp.ClientError as e:
                logger.error(
                    f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3): {e}"
                )
                if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    await asyncio.sleep(5 + (attempt * 2))  # 5, 7, 9 —Å–µ–∫—É–Ω–¥
                    continue
                return None
            except aiohttp.InvalidURL as e:
                logger.error(
                    f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –≤ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3): {e}"
                )
                # –î–ª—è –æ—à–∏–±–æ–∫ URL –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫–∏
                return None
            except aiohttp.ServerTimeoutError as e:
                logger.error(f"–¢–∞–π–º–∞—É—Ç —Å–µ—Ä–≤–µ—Ä–∞ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3): {e}")
                if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    await asyncio.sleep(5 + (attempt * 2))
                    continue
                return None
            except Exception as e:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama: {e}")
                if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    await asyncio.sleep(5 + (attempt * 2))
                    continue
                return None

    async def test_connection(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama"""
        result = {
            "ollama_available": False,
            "model_available": False,
            "llm_model_available": False,
            "model_name": self.model_name,
            "llm_model_name": self.llm_model_name,
            "base_url": self.base_url,
        }

        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é –µ—Å–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self.session:
                self.session = aiohttp.ClientSession()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            async with self.session.get(f"{self.base_url}/api/version") as response:
                if response.status == 200:
                    result["ollama_available"] = True
                    version_data = await response.json()
                    result["ollama_version"] = version_data.get("version", "unknown")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if result["ollama_available"]:
                result["model_available"] = await self.check_model_availability()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LLM –º–æ–¥–µ–ª–∏
                result[
                    "llm_model_available"
                ] = await self.check_llm_model_availability()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            result["error"] = str(e)

        return result

    async def check_llm_model_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LLM –º–æ–¥–µ–ª–∏"""
        try:
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model["name"] for model in data.get("models", [])]
                    return self.llm_model_name in models
                return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ LLM –º–æ–¥–µ–ª–∏: {e}")
            return False

    def _estimate_tokens(self, text: str) -> int:
        """
        –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        –î–ª—è Magistral-Small-2509 –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ü–µ–Ω–∫—É: ~1.2 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω
        """
        return len(text) // 1.2

    def _split_prompt(self, prompt: str, max_prompt_tokens: int = 31000) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤

        Args:
            prompt: –ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            max_prompt_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π –ø—Ä–æ–º–ø—Ç–∞
        """
        estimated_tokens = self._estimate_tokens(prompt)

        if estimated_tokens <= max_prompt_tokens:
            return [prompt]

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ —á–∞—Å—Ç–∏
        logger.warning(
            f"‚ö†Ô∏è  –ü—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤), —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ (–ª–∏–º–∏—Ç {max_prompt_tokens})"
        )

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
        conversation_markers = [
            "–ò–°–•–û–î–ù–´–ô –†–ê–ó–ì–û–í–û–†:",
            "–†–∞–∑–≥–æ–≤–æ—Ä:",
            "–°–æ–æ–±—â–µ–Ω–∏—è:",
            "conversation_text:",
            "conversation:",
        ]

        system_part = ""
        conversation_part = ""

        for marker in conversation_markers:
            if marker in prompt:
                parts = prompt.split(marker, 1)
                if len(parts) == 2:
                    system_part = parts[0] + marker
                    conversation_part = parts[1]
                    break

        if not conversation_part:
            # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥—Ä—É–≥–∞—è, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
            max_chars = max_prompt_tokens * 1.2
            chunks = []
            for i in range(0, len(prompt), max_chars):
                chunks.append(prompt[i : i + max_chars])
            return chunks

        # –†–∞–∑–±–∏–≤–∞–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä –Ω–∞ —á–∞—Å—Ç–∏
        max_conversation_tokens = (
            max_prompt_tokens - self._estimate_tokens(system_part) - 100
        )  # –∑–∞–ø–∞—Å
        max_conversation_chars = max_conversation_tokens * 1.2

        conversation_chunks = []
        for i in range(0, len(conversation_part), max_conversation_chars):
            conversation_chunks.append(
                conversation_part[i : i + max_conversation_chars]
            )

        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
        prompts = []
        for i, chunk in enumerate(conversation_chunks):
            if i == 0:
                chunk_prompt = system_part + chunk
            else:
                chunk_prompt = f"{system_part}\n(–ü–†–û–î–û–õ–ñ–ï–ù–ò–ï –†–ê–ó–ì–û–í–û–†–ê - —á–∞—Å—Ç—å {i+1}/{len(conversation_chunks)})\n{chunk}"
            prompts.append(chunk_prompt)

        logger.info(f"üìù –ü—Ä–æ–º–ø—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(prompts)} —á–∞—Å—Ç–µ–π")
        return prompts

    async def generate_summary(
        self,
        prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 900,
        top_p: float = 0.93,
        presence_penalty: float = 0.05,
        max_prompt_tokens: int = 30000,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤

        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            top_p: Top-p –ø–∞—Ä–∞–º–µ—Ç—Ä
            presence_penalty: Presence penalty
            max_prompt_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30000, –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è 32768 –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        prompt_parts = self._split_prompt(prompt, max_prompt_tokens)

        if len(prompt_parts) == 1:
            # –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            return await self._generate_single_summary(
                prompt_parts[0], temperature, max_tokens, top_p, presence_penalty
            )
        else:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ —á–∞—Å—Ç—è–º –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            logger.info(
                f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ —á–∞—Å—Ç—è–º ({len(prompt_parts)} —á–∞—Å—Ç–µ–π)"
            )
            summaries = []

            for i, part_prompt in enumerate(prompt_parts):
                logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i+1}/{len(prompt_parts)}")
                part_summary = await self._generate_single_summary(
                    part_prompt, temperature, max_tokens, top_p, presence_penalty
                )
                summaries.append(part_summary)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined = "\n\n".join(summaries)
            logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã {len(summaries)} —á–∞—Å—Ç–µ–π —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
            return combined

    async def _generate_single_summary(
        self,
        prompt: str,
        temperature: float,
        max_tokens: int,
        top_p: float,
        presence_penalty: float,
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        payload = {
            "model": self.llm_model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": top_p,
                "presence_penalty": presence_penalty,
                "num_ctx": 8192,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è Gemma3n
            },
        }

        if self.llm_thinking_level:
            payload["thinking"] = {"level": self.llm_thinking_level}

        try:
            timeout = aiohttp.ClientTimeout(total=300)  # 5 –º–∏–Ω—É—Ç –¥–ª—è LLM
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{self.base_url}/api/generate", json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("response", "").strip()
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ API Ollama: {response.status}")
                        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: HTTP {response.status}"
        except aiohttp.InvalidURL as e:
            logger.error(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –≤ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama: {e}")
            return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"


# –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
class OllamaEmbeddingClientSync:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Ollama"""

    def __init__(
        self,
        model_name: str = "hf.co/lmstudio-community/Magistral-Small-2509-GGUF:Q4_K_M",
        base_url: str = "http://localhost:11434",
        max_text_length: int = 16384,  # 4096 —Ç–æ–∫–µ–Ω–æ–≤ * 4 —Å–∏–º–≤–æ–ª–∞/—Ç–æ–∫–µ–Ω –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞
    ):
        self.model_name = model_name
        self.base_url = base_url
        self.max_text_length = max_text_length

    def _truncate_text(self, text: str, max_length: int = None) -> str:
        """–û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        if max_length is None:
            max_length = self.max_text_length

        if len(text) <= max_length:
            return text

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ–±–µ–ª–∞, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞
        truncated = text[:max_length]
        last_space = truncated.rfind(" ")
        if last_space > max_length * 0.8:  # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ
            result = truncated[:last_space]
        else:
            result = truncated

        logger.warning(
            f"–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω —Å {len(text)} –¥–æ {len(result)} —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤"
        )
        return result

    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""

        embeddings = []

        for text in texts:
            try:
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞—Å—Ç–∏
                text_chunks = self._split_text_into_chunks(text)

                if len(text_chunks) == 1:
                    # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
                    embedding = self._generate_single_embedding_sync(text)
                    if embedding:
                        embeddings.append(embedding)
                    else:
                        logger.warning(
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {text[:50]}..."
                        )
                        embeddings.append([0.0] * 5120)
                else:
                    # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º
                    chunk_embeddings = []
                    for j, chunk in enumerate(text_chunks):
                        logger.debug(
                            f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {j+1}/{len(text_chunks)} (–¥–ª–∏–Ω–∞: {len(chunk)})"
                        )
                        chunk_embedding = self._generate_single_embedding_sync(chunk)
                        if chunk_embedding:
                            chunk_embeddings.append(chunk_embedding)
                        else:
                            logger.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞—Å—Ç–∏ {j+1}"
                            )
                            chunk_embeddings.append([0.0] * 5120)

                    # –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
                    if chunk_embeddings:
                        averaged_embedding = self._average_embeddings(chunk_embeddings)
                        embeddings.append(averaged_embedding)
                        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ª–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä—É–ø–ø)
                    if len(chunk_embeddings) > 3:
                        logger.debug(
                            f"–£—Å—Ä–µ–¥–Ω–µ–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ {len(chunk_embeddings)} —á–∞—Å—Ç–µ–π"
                        )
                    else:
                        embeddings.append([0.0] * 5120)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
                embeddings.append([0.0] * 5120)

        return embeddings

    def _split_text_into_chunks(self, text: str, max_length: int = None) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        if max_length is None:
            max_length = self.max_text_length

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º, –∞ –Ω–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        # –î–ª—è Magistral-Small-2509 –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ü–µ–Ω–∫—É: ~1.2 —Å–∏–º–≤–æ–ª–∞/—Ç–æ–∫–µ–Ω
        estimated_tokens = len(text) // 1.2
        max_tokens = max_length // 3.5

        if estimated_tokens <= max_tokens:
            return [text]

        chunks = []
        start = 0
        chunk_size_chars = max_tokens * 4  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö

        while start < len(text):
            end = start + chunk_size_chars

            if end >= len(text):
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∫—É—Å–æ–∫
                chunks.append(text[start:])
                break

            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö chunk_size_chars
            last_space = text.rfind(" ", start, end)
            if (
                last_space > start + chunk_size_chars * 0.7
            ):  # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ
                chunks.append(text[start:last_space])
                start = last_space + 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª
            else:
                # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ, –æ–±—Ä–µ–∑–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
                chunks.append(text[start:end])
                start = end

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–∞—Ö —á–∞–Ω–∫–æ–≤
        if len(chunks) > 1:
            chunk_tokens = [len(chunk) // 3.5 for chunk in chunks]
            logger.debug(
                f"–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π "
                f"(~{estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤ -> ~{sum(chunk_tokens)} —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–∞—Ö)"
            )

        return chunks

    def _average_embeddings(self, embeddings: List[List[float]]) -> List[float]:
        """–£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –æ–¥–∏–Ω"""
        if not embeddings:
            return [0.0] * 5120

        if len(embeddings) == 1:
            return embeddings[0]

        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –∫–∞–∂–¥–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é
        dimension = len(embeddings[0])
        averaged = []

        for i in range(dimension):
            sum_val = sum(emb[i] for emb in embeddings)
            averaged.append(sum_val / len(embeddings))

        return averaged

    def _generate_single_embedding_sync(self, text: str) -> Optional[List[float]]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        import requests

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        if len(text) > self.max_text_length:
            logger.warning(
                f"–¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({len(text)} > {self.max_text_length}), –æ–±—Ä–µ–∑–∞–µ–º"
            )
            text = text[: self.max_text_length]

        payload = {"model": self.model_name, "prompt": text}

        try:
            response = requests.post(
                f"{self.base_url}/api/embeddings", json=payload, timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                return data.get("embedding")
            else:
                logger.error(f"–û—à–∏–±–∫–∞ API Ollama: {response.status_code}")
                return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None

    def test_connection(self) -> Dict[str, Any]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        import requests

        result = {
            "ollama_available": False,
            "model_available": False,
            "model_name": self.model_name,
            "base_url": self.base_url,
        }

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            response = requests.get(f"{self.base_url}/api/version", timeout=10)
            if response.status_code == 200:
                result["ollama_available"] = True
                version_data = response.json()
                result["ollama_version"] = version_data.get("version", "unknown")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            if result["ollama_available"]:
                response = requests.get(f"{self.base_url}/api/tags", timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    models = [model["name"] for model in data.get("models", [])]
                    result["model_available"] = self.model_name in models

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            result["error"] = str(e)

        return result
