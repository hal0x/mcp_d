#!/usr/bin/env python3
"""–§–∞—Å–∞–¥–Ω—ã–π –∫–ª–∞—Å—Å TwoLevelIndexer, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤—Å–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã."""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional
from zoneinfo import ZoneInfo

from ...analysis.segmentation import (
    AdaptiveMessageGrouper,
    DayGroupingSegmenter,
    SemanticRegrouper,
    SessionClusterer,
    SessionSegmenter,
)
from ...analysis.entities import EntityExtractor, EntityDictionary
from ...analysis.entities.entity_dictionary import get_entity_dictionary
from ...analysis.utils import InstructionManager, TimeProcessor
from ...analysis.rendering import MarkdownRenderer
from ...analysis.summarization import ClusterSummarizer
from ...analysis.summarization.session.summarizer import SessionSummarizer
from ...memory.storage.vector.qdrant_collections import QdrantCollectionsManager
from ...utils.system.naming import slugify
from ..adapters.langchain_adapters import LangChainLLMAdapter, get_llm_client_factory

from .collections_manager import CollectionsManager
from .data_loader import DataLoader
from .l1_indexer import L1Indexer
from .l2_indexer import L2Indexer
from .tasks_indexer import TasksIndexer
from .clustering_manager import ClusteringManager
from .smart_aggregation import SmartAggregationManager
from .entities_indexer import EntitiesIndexer
from .progress_manager import ProgressManager

logger = logging.getLogger(__name__)


class TwoLevelIndexer:
    """–î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: L1 (sessions —Å —Å–∞–º–º–∞—Ä–∏) –∏ L2 (messages —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)."""

    def __init__(
        self,
        artifacts_path: str = "./artifacts",
        embedding_client: Optional[LangChainLLMAdapter] = None,
        enable_quality_check: bool = True,
        enable_iterative_refinement: bool = True,
        min_quality_score: float = 80.0,
        enable_clustering: bool = True,
        clustering_threshold: float = 0.8,
        min_cluster_size: int = 2,
        max_messages_per_group: int = 100,
        max_session_hours: int = 6,
        gap_minutes: int = 60,
        enable_smart_aggregation: bool = False,
        aggregation_strategy: str = "smart",
        now_window_hours: int = 24,
        fresh_window_days: int = 14,
        recent_window_days: int = 30,
        strategy_threshold: int = 1000,
        force: bool = False,
        enable_entity_learning: bool = True,
        enable_time_analysis: bool = True,
        graph: Optional[Any] = None,
        progress_callback: Optional[Callable[[str, str, Dict[str, Any]], None]] = None,
        enable_message_grouping: bool = True,
        message_grouping_strategy: str = "session",
        min_group_size: int = 3,
        max_group_size: int = 50,
        max_group_tokens: int = 8000,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        self.progress_callback = progress_callback
        
        self.artifacts_path = Path(artifacts_path).expanduser()
        self.artifacts_path.mkdir(parents=True, exist_ok=True)
        self.reports_path = self.artifacts_path / "reports"
        self.reports_path.mkdir(parents=True, exist_ok=True)
        
        if embedding_client is None:
            embedding_client = get_llm_client_factory()
            if embedding_client is None:
                raise ValueError(
                    "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LangChain LLM –∫–ª–∏–µ–Ω—Ç. "
                    "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LangChain —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ MEMORY_MCP_LMSTUDIO_LLM_MODEL –Ω–∞—Å—Ç—Ä–æ–µ–Ω."
                )
        self.embedding_client = embedding_client
        
        from ...config import get_settings
        settings = get_settings()
        qdrant_url = settings.get_qdrant_url()
        if qdrant_url:
            embedding_dimension = self.embedding_client.dimension if self.embedding_client else 1024
            self.qdrant_manager = QdrantCollectionsManager(url=qdrant_url, vector_size=embedding_dimension)
            if not self.qdrant_manager.available():
                logger.warning("Qdrant –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω–µ –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã")
                self.qdrant_manager = None
        else:
            logger.warning("QDRANT_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω–µ –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã")
            self.qdrant_manager = None
        
        self.enable_clustering = enable_clustering
        self.clustering_threshold = clustering_threshold
        self.min_cluster_size = min_cluster_size
        self.max_messages_per_group = max_messages_per_group
        self.max_session_hours = max_session_hours
        self.gap_minutes = gap_minutes
        self.enable_smart_aggregation = enable_smart_aggregation
        self.aggregation_strategy = aggregation_strategy
        self.now_window_hours = now_window_hours
        self.fresh_window_days = fresh_window_days
        self.recent_window_days = recent_window_days
        self.strategy_threshold = strategy_threshold
        self.force = force
        self.enable_entity_learning = enable_entity_learning
        self.enable_time_analysis = enable_time_analysis
        self.enable_message_grouping = enable_message_grouping
        self.message_grouping_strategy = message_grouping_strategy
        self.min_group_size = min_group_size
        self.max_group_size = max_group_size
        self.max_group_tokens = max_group_tokens
        
        self.session_segmenter = SessionSegmenter(
            gap_minutes=gap_minutes,
            max_session_hours=max_session_hours,
            enable_time_analysis=enable_time_analysis,
        )
        self.day_grouping_segmenter = DayGroupingSegmenter(
            max_messages_per_group=max_messages_per_group,
        )
        
        self.instruction_manager = InstructionManager()
        self.session_summarizer = SessionSummarizer(
            self.embedding_client,
            self.reports_path,
            instruction_manager=self.instruction_manager,
            enable_quality_check=enable_quality_check,
            enable_iterative_refinement=enable_iterative_refinement,
            min_quality_score=min_quality_score,
        )
        
        self.entity_extractor = EntityExtractor(
            enable_learning=enable_entity_learning,
            enable_natasha=True,
            enable_llm_validation=True,
        )
        
        self.time_processor = TimeProcessor() if enable_time_analysis else None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å —Å—É—â–Ω–æ—Å—Ç–µ–π
        if enable_entity_learning:
            self.entity_dictionary = get_entity_dictionary(
                enable_llm_validation=True,
                enable_description_generation=settings.entity_description_enabled,
                graph=graph,
            )
        else:
            self.entity_dictionary = None
        
        self.markdown_renderer = MarkdownRenderer(self.reports_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏
        self.graph = graph
        if self.graph:
            from ...memory.ingest import MemoryIngestor
            self.ingestor = MemoryIngestor(self.graph)
            logger.info("TwoLevelIndexer: –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω, –∑–∞–ø–∏—Å–∏ –±—É–¥—É—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è")
        else:
            self.ingestor = None
            logger.debug("TwoLevelIndexer: –≥—Ä–∞—Ñ –ø–∞–º—è—Ç–∏ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω, –∑–∞–ø–∏—Å–∏ –±—É–¥—É—Ç —Ç–æ–ª—å–∫–æ –≤ Qdrant")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º VectorStore
        from ...memory.storage.vector.vector_store import build_vector_store_from_env
        self.vector_store = build_vector_store_from_env()
        if self.vector_store and self.vector_store.available():
            logger.info("VectorStore (Qdrant) –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
            if self.embedding_client:
                try:
                    dimension = self.embedding_client.dimension
                    if dimension:
                        self.vector_store.ensure_collection(dimension)
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant: {e}")
        else:
            logger.warning("VectorStore (Qdrant) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å")
            self.vector_store = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º EntityVectorStore
        if enable_entity_learning:
            from ...memory.storage.vector.vector_store import build_entity_vector_store_from_env
            self.entity_vector_store = build_entity_vector_store_from_env()
            if self.entity_vector_store:
                logger.info("EntityVectorStore –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π")
            else:
                logger.debug("EntityVectorStore –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (Qdrant –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω)")
        else:
            self.entity_vector_store = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä—ã
        self.collections_manager = CollectionsManager(
            self.qdrant_manager,
            self.embedding_client,
            force_recreate=force,
        )
        
        self.data_loader = DataLoader(
            max_messages_per_group=max_messages_per_group,
            max_session_hours=max_session_hours,
            gap_minutes=gap_minutes,
            enable_time_analysis=enable_time_analysis,
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        self.semantic_regrouper = None
        self.adaptive_grouper = None
        if self.enable_message_grouping:
            if self.message_grouping_strategy == "semantic":
                self.semantic_regrouper = SemanticRegrouper(embedding_client=self.embedding_client)
            elif self.message_grouping_strategy == "adaptive":
                self.adaptive_grouper = AdaptiveMessageGrouper(
                    max_tokens=max_group_tokens,
                )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä—ã
        self.l1_indexer = L1Indexer(
            embedding_client=self.embedding_client,
            session_summarizer=self.session_summarizer,
            qdrant_manager=self.qdrant_manager,
            sessions_collection=self.collections_manager.sessions_collection,
            ingestor=self.ingestor,
            graph=self.graph,
            vector_store=self.vector_store,
            force_recreate=force,
            collections_manager=self.collections_manager,
        )
        
        self.l2_indexer = L2Indexer(
            qdrant_manager=self.qdrant_manager,
            messages_collection=self.collections_manager.messages_collection,
            embedding_client=self.embedding_client,
            vector_store=self.vector_store,
            ingestor=self.ingestor,
            graph=self.graph,
            entity_extractor=self.entity_extractor,
            enable_message_grouping=enable_message_grouping,
            message_grouping_strategy=message_grouping_strategy,
            min_group_size=min_group_size,
            max_group_size=max_group_size,
            max_group_tokens=max_group_tokens,
            semantic_regrouper=self.semantic_regrouper,
            adaptive_grouper=self.adaptive_grouper,
            force=force,
            collections_manager=self.collections_manager,
        )
        
        self.tasks_indexer = TasksIndexer(
            embedding_client=self.embedding_client,
            qdrant_manager=self.qdrant_manager,
            tasks_collection=self.collections_manager.tasks_collection,
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        if self.enable_clustering:
            self.session_clusterer = SessionClusterer(
                similarity_threshold=self.clustering_threshold,
                min_cluster_size=self.min_cluster_size,
                use_hdbscan=False,
            )
            self.cluster_summarizer = ClusterSummarizer(
                embedding_client=self.embedding_client
            )
        else:
            self.session_clusterer = None
            self.cluster_summarizer = None
        
        self.clustering_manager = ClusteringManager(
            enable_clustering=enable_clustering,
            clustering_threshold=clustering_threshold,
            min_cluster_size=min_cluster_size,
            session_clusterer=self.session_clusterer,
            cluster_summarizer=self.cluster_summarizer,
            qdrant_manager=self.qdrant_manager,
            sessions_collection=self.collections_manager.sessions_collection,
            clusters_collection=self.collections_manager.clusters_collection,
            collections_manager=self.collections_manager,
        )
        
        self.smart_aggregation_manager = SmartAggregationManager(
            qdrant_manager=self.qdrant_manager,
            sessions_collection=self.collections_manager.sessions_collection,
            day_grouping_segmenter=self.day_grouping_segmenter,
            strategy_threshold=strategy_threshold,
            now_window_hours=now_window_hours,
            fresh_window_days=fresh_window_days,
            recent_window_days=recent_window_days,
        )
        
        self.entities_indexer = EntitiesIndexer(
            entity_dictionary=self.entity_dictionary,
            entity_vector_store=self.entity_vector_store,
            graph=self.graph,
            embedding_client=self.embedding_client,
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        # –ü—Ä–æ–≥—Ä–µ—Å—Å —Ç–µ–ø–µ—Ä—å —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ SQLite —á–µ—Ä–µ–∑ IndexingJobTracker, –Ω–µ –≤ Qdrant
        self.progress_manager = ProgressManager(
            qdrant_manager=None,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º Qdrant –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            progress_collection=None,
            progress_callback=progress_callback,
        )
        
        logger.info("TwoLevelIndexer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å–æ –≤—Å–µ–º–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏")

    async def build_index(
        self,
        scope: str = "all",
        chat: Optional[str] = None,
        force_full: bool = False,
        recent_days: int = 7,
        adapter: Optional[Any] = None,
        job_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞

        Args:
            scope: "all" –∏–ª–∏ "chat"
            chat: –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞ (–¥–ª—è scope="chat")
            force_full: –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞
            recent_days: –ü–µ—Ä–µ—Å–∞–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
            adapter: –ê–¥–∞–ø—Ç–µ—Ä –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
            job_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–¥–∞—á–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        """
        logger.info(
            f"–ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: scope={scope}, chat={chat}, force_full={force_full}"
        )
        
        stats = {
            "indexed_chats": [],
            "sessions_indexed": 0,
            "messages_indexed": 0,
            "tasks_indexed": 0,
        }
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∞—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        chats_path = Path("chats")
        if scope == "chat" and chat:
            chat_dir = chats_path / chat
            if not chat_dir.exists() or not chat_dir.is_dir():
                logger.error(f"‚ùå –ß–∞—Ç '{chat}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {chats_path}")
                return {
                    "indexed_chats": [],
                    "sessions_indexed": 0,
                    "messages_indexed": 0,
                    "tasks_indexed": 0,
                    "error": f"–ß–∞—Ç '{chat}' –Ω–µ –Ω–∞–π–¥–µ–Ω",
                }
            chat_dirs = [chat_dir]
        else:
            chat_dirs = [d for d in chats_path.iterdir() if d.is_dir()]
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞—Ç
        total_chats = len(chat_dirs)
        for chat_idx, chat_dir in enumerate(chat_dirs, 1):
            try:
                chat_name = chat_dir.name
                logger.info(f"üìÅ –ß–∞—Ç {chat_idx}/{total_chats}: {chat_name}")
                
                # Callback: –Ω–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Ç–∞
                self.progress_manager.call_progress_callback(
                    job_id,
                    "chat_started",
                    {
                        "chat": chat_name,
                        "chat_index": chat_idx,
                        "total_chats": total_chats,
                    },
                )
                
                # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                if force_full and adapter is not None:
                    logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–∞—Ç–∞ {chat_name} –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π...")
                    try:
                        cleanup_stats = adapter.clear_chat_data(chat_name)
                        logger.info(
                            f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: "
                            f"—É–∑–ª–æ–≤={cleanup_stats.get('nodes_deleted', 0)}, "
                            f"–≤–µ–∫—Ç–æ—Ä–æ–≤={cleanup_stats.get('vectors_deleted', 0)}, "
                            f"Qdrant={cleanup_stats.get('qdrant_deleted', 0)}"
                        )
                    except Exception as e:
                        logger.warning(
                            f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö —á–∞—Ç–∞ {chat_name}: {e}. "
                            f"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é...",
                            exc_info=True,
                        )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
                messages = await self.data_loader.load_messages_from_chat(chat_dir)
                
                if not messages:
                    logger.warning(f"–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —á–∞—Ç–µ {chat_name}")
                    continue
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
                if not force_full:
                    last_indexed_date = self.progress_manager.get_last_indexed_date(chat_name)
                    
                    if last_indexed_date:
                        messages_to_index = [
                            m
                            for m in messages
                            if self.data_loader.parse_message_time(m) > last_indexed_date
                        ]
                        logger.info(
                            f"üìä –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ "
                            f"—Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç {last_indexed_date.strftime('%Y-%m-%d %H:%M:%S')}"
                        )
                    else:
                        if recent_days > 0:
                            recent_cutoff = datetime.now(ZoneInfo("UTC")) - timedelta(
                                days=recent_days
                            )
                            messages_to_index = [
                                m
                                for m in messages
                                if self.data_loader.parse_message_time(m) >= recent_cutoff
                            ]
                            logger.info(
                                f"üìä –ü–µ—Ä–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ {recent_days} –¥–Ω–µ–π"
                            )
                        else:
                            messages_to_index = messages
                            logger.info(
                                "üìä –ü–µ—Ä–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è"
                            )
                else:
                    messages_to_index = messages
                    logger.info("üìä –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è")
                
                logger.info(
                    f"–°–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {len(messages_to_index)} –∏–∑ {len(messages)}"
                )
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
                if self.enable_smart_aggregation:
                    logger.info("üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É —Å —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏")
                    sessions = self.smart_aggregation_manager.group_messages_by_smart_strategy(
                        messages_to_index, chat_name
                    )
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É —á–µ—Ä–µ–∑ DataLoader
                    day_groups = self.day_grouping_segmenter.group_messages_by_days(
                        messages_to_index, chat_name
                    )
                    sessions = self.data_loader.expand_day_groups(day_groups, chat_name)
                
                logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(sessions)} —Å–µ—Å—Å–∏–π")
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Å–µ—Å—Å–∏–∏
                chat_stats = await self._index_chat_sessions(
                    chat_name, sessions, force_full, job_id
                )
                
                stats["indexed_chats"].append(chat_name)
                stats["sessions_indexed"] += chat_stats.get("sessions_indexed", 0)
                stats["messages_indexed"] += chat_stats.get("messages_indexed", 0)
                stats["tasks_indexed"] += chat_stats.get("tasks_indexed", 0)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if messages_to_index:
                    last_message_date = self.data_loader.parse_message_time(messages_to_index[-1])
                    self.progress_manager.save_indexing_progress(
                        chat_name,
                        last_message_date,
                        len(messages_to_index),
                        len(sessions),
                    )
                
                # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
                if self.enable_clustering:
                    summaries = await self._load_existing_summaries(chat_name, sessions)
                    if summaries:
                        clustering_stats = await self.clustering_manager.cluster_chat_sessions(
                            chat_name, summaries
                        )
                        logger.info(
                            f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {clustering_stats.get('clusters_count', 0)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, "
                            f"{clustering_stats.get('sessions_clustered', 0)} —Å–µ—Å—Å–∏–π"
                        )
                
                # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
                if self.enable_entity_learning:
                    await self.entities_indexer.build_and_index_entities(chat_name)
                
                # Callback: –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Ç–∞
                self.progress_manager.call_progress_callback(
                    job_id,
                    "chat_completed",
                    {
                        "chat": chat_name,
                        "stats": chat_stats,
                    },
                )
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —á–∞—Ç–∞ {chat_dir.name}: {e}", exc_info=True)
                continue
        
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {stats}")
        return stats

    async def _index_chat_sessions(
        self,
        chat_name: str,
        sessions: List[Dict[str, Any]],
        force_full: bool,
        job_id: Optional[str],
    ) -> Dict[str, Any]:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Å–µ—Å—Å–∏–∏ —á–∞—Ç–∞."""
        stats = {
            "sessions_indexed": 0,
            "messages_indexed": 0,
            "tasks_indexed": 0,
        }
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ—Å—Å–∏–∏
        existing_session_ids = set()
        if not force_full and self.qdrant_manager and self.collections_manager.sessions_collection:
            try:
                result = self.qdrant_manager.get(
                    collection_name=self.collections_manager.sessions_collection,
                    where={"chat": chat_name}
                )
                if result and result.get("ids"):
                    existing_session_ids = set(result["ids"])
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ—Å—Å–∏–∏: {e}")
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Å–µ—Å—Å–∏—é
        for session_idx, session in enumerate(sessions, 1):
            try:
                session_id = session.get("session_id")
                if not session_id:
                    logger.warning(f"–°–µ—Å—Å–∏—è –±–µ–∑ ID, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                if not force_full and session_id in existing_session_ids:
                    logger.debug(f"–°–µ—Å—Å–∏—è {session_id} —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ {session_idx}/{len(sessions)}: {session_id}")
                
                # Callback: –Ω–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ—Å—Å–∏–∏
                self.progress_manager.call_progress_callback(
                    job_id,
                    "session_started",
                    {
                        "chat": chat_name,
                        "session_id": session_id,
                        "session_index": session_idx,
                        "total_sessions": len(sessions),
                    },
                )
                
                # –°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ (L1)
                summary = await self.session_summarizer.summarize_session(
                    session, chat_name
                )
                
                if not summary:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}")
                    continue
                
                # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è L1
                await self.l1_indexer.index_session_l1(summary)
                stats["sessions_indexed"] += 1
                
                # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è L2 (—Å–æ–æ–±—â–µ–Ω–∏—è)
                messages_count = await self.l2_indexer.index_messages_l2(session)
                stats["messages_indexed"] += messages_count
                
                # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–¥–∞—á
                tasks_count = await self.tasks_indexer.index_tasks(summary)
                stats["tasks_indexed"] += tasks_count
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –≤ JSON
                await self._save_summary(summary, chat_name)
                
                # Callback: –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ—Å—Å–∏–∏
                self.progress_manager.call_progress_callback(
                    job_id,
                    "session_completed",
                    {
                        "chat": chat_name,
                        "session_id": session_id,
                        "messages_indexed": messages_count,
                        "tasks_indexed": tasks_count,
                    },
                )
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏ {session.get('session_id', 'unknown')}: {e}", exc_info=True)
                continue
        
        return stats

    async def _save_summary(self, summary: Dict[str, Any], chat_name: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –≤ JSON —Ñ–∞–π–ª."""
        try:
            chat_slug = slugify(chat_name)
            sessions_dir = self.reports_path / chat_slug / "sessions"
            sessions_dir.mkdir(parents=True, exist_ok=True)
            
            session_id = summary.get("session_id")
            if not session_id:
                logger.warning("–°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ session_id, –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º")
                return
            
            json_file = sessions_dir / f"{session_id}.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"–°–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {json_file}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")

    async def _load_existing_summaries(
        self, chat_name: str, sessions: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–∑ JSON —Ñ–∞–π–ª–æ–≤."""
        summaries = []
        try:
            chat_slug = slugify(chat_name)
            sessions_dir = self.reports_path / chat_slug / "sessions"
            
            if not sessions_dir.exists():
                return summaries
            
            session_ids = {s.get("session_id") for s in sessions if s.get("session_id")}
            
            for session_id in session_ids:
                json_file = sessions_dir / f"{session_id}.json"
                if json_file.exists():
                    try:
                        with open(json_file, encoding="utf-8") as f:
                            summary = json.load(f)
                            summaries.append(summary)
                    except Exception as e:
                        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—é {session_id}: {e}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–π: {e}")
        
        return summaries

    def get_clusters(
        self, chat: Optional[str] = None, limit: int = 20
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

        Args:
            chat: –§–∏–ª—å—Ç—Ä –ø–æ —á–∞—Ç—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        return self.clustering_manager.get_clusters(chat=chat, limit=limit)

    def get_cluster_sessions(
        self, cluster_id: str, limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–µ—Å—Å–∏–∏, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—É

        Args:
            cluster_id: ID –∫–ª–∞—Å—Ç–µ—Ä–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Å—Å–∏–π

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ—Å—Å–∏–π
        """
        return self.clustering_manager.get_cluster_sessions(cluster_id, limit=limit)

