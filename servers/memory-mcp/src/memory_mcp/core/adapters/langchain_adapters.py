"""LangChain –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ LLM."""

from __future__ import annotations

import asyncio
import logging
from typing import Any, List, Optional

try:
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    from langchain_core.embeddings import Embeddings as LangChainEmbeddings
    from langchain_core.language_models.chat_models import BaseChatModel
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError:
    ChatOpenAI = None  # type: ignore
    OpenAIEmbeddings = None  # type: ignore
    LangChainEmbeddings = None  # type: ignore
    BaseChatModel = None  # type: ignore
    HumanMessage = None  # type: ignore
    SystemMessage = None  # type: ignore

from ...config import get_settings

logger = logging.getLogger(__name__)


class LangChainEmbeddingAdapter:
    """–ê–¥–∞–ø—Ç–µ—Ä LangChain Embeddings.
    
    –û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ LangChain Embeddings, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —á–µ—Ä–µ–∑ LangChain.
    """

    def __init__(
        self,
        embeddings: LangChainEmbeddings,
        *,
        timeout: float = 10.0,
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞.
        
        Args:
            embeddings: –≠–∫–∑–µ–º–ø–ª—è—Ä LangChain Embeddings
            timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é)
        """
        if LangChainEmbeddings is None:
            raise ImportError(
                "LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install langchain langchain-community langchain-openai"
            )
        self.embeddings = embeddings
        self.timeout = timeout
        self._dimension: Optional[int] = None
        self._base_url: Optional[str] = None
        self._model_name: Optional[str] = None

    def available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
        return self.embeddings is not None

    @property
    def dimension(self) -> Optional[int]:
        """–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
        if self._dimension is None:
            # –ü–æ–ø—ã—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            try:
                test_embedding = self.embed("test")
                if test_embedding:
                    self._dimension = len(test_embedding)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return self._dimension

    def embed(self, text: str) -> Optional[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª (–≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not text or not text.strip():
            return None
        
        try:
            # LangChain embeddings.embed_query —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π
            vector = self.embeddings.embed_query(text.strip())
            if self._dimension is None and vector:
                self._dimension = len(vector)
            return vector
        except Exception as exc:
            logger.warning(f"LangChain embedding error: {exc}")
            return None

    def embed_batch(self, texts: List[str]) -> List[Optional[List[float]]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å None –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö)
        """
        if not texts:
            return []
        
        # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã
        processed_texts = [text.strip() for text in texts if text.strip()]
        if not processed_texts:
            return [None] * len(texts)
        
        try:
            # LangChain embeddings.embed_documents –¥–ª—è –±–∞—Ç—á–∞
            embeddings_list = self.embeddings.embed_documents(processed_texts)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
            if self._dimension is None and embeddings_list and embeddings_list[0]:
                self._dimension = len(embeddings_list[0])
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å None –¥–ª—è –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            result = []
            processed_idx = 0
            for text in texts:
                if text.strip():
                    emb = embeddings_list[processed_idx] if processed_idx < len(embeddings_list) else None
                    result.append(emb)
                    processed_idx += 1
                else:
                    result.append(None)
            
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len([e for e in result if e])} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
            return result
        except Exception as exc:
            logger.warning(f"LangChain embedding batch error: {exc}")
            return [None] * len(texts)

    def close(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π."""
        # LangChain embeddings –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É—é—Ç —è–≤–Ω–æ–≥–æ –∑–∞–∫—Ä—ã—Ç–∏—è
        pass


class LangChainLLMAdapter:
    """–ê–¥–∞–ø—Ç–µ—Ä LangChain LLM.
    
    –û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ LangChain ChatModel, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥—ã generate_summary
    –∏ –¥—Ä—É–≥–∏–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ LangChain.
    """

    def __init__(
        self,
        llm: BaseChatModel,
        *,
        model_name: Optional[str] = None,
        llm_model_name: Optional[str] = None,
        base_url: Optional[str] = None,
    ) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞.
        
        Args:
            llm: –≠–∫–∑–µ–º–ø–ª—è—Ä LangChain ChatModel
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            llm_model_name: –ò–º—è LLM –º–æ–¥–µ–ª–∏
            base_url: –ë–∞–∑–æ–≤—ã–π URL
        """
        if BaseChatModel is None:
            raise ImportError(
                "LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install langchain langchain-community langchain-openai"
            )
        self.llm = llm
        self.model_name = model_name
        self.llm_model_name = llm_model_name
        self.base_url = base_url
        self.session = None  # –î–ª—è async context manager

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥."""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥."""
        pass

    async def generate_summary(
        self,
        prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 131072,
        top_p: float = 0.93,
        presence_penalty: float = 0.05,
        max_prompt_tokens: int = 100000,
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ LLM.
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            top_p: Top-p –ø–∞—Ä–∞–º–µ—Ç—Ä
            presence_penalty: Presence penalty (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ LangChain)
            max_prompt_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            if hasattr(self.llm, "temperature"):
                self.llm.temperature = temperature
            if hasattr(self.llm, "max_tokens"):
                self.llm.max_tokens = max_tokens
            if hasattr(self.llm, "top_p"):
                self.llm.top_p = top_p
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            prompt_parts = self._split_prompt(prompt, max_prompt_tokens)
            
            if len(prompt_parts) == 1:
                return await self._generate_single_summary(prompt_parts[0])
            else:
                logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ —á–∞—Å—Ç—è–º ({len(prompt_parts)} —á–∞—Å—Ç–µ–π)")
                summaries = []
                for i, part_prompt in enumerate(prompt_parts):
                    logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i+1}/{len(prompt_parts)}")
                    part_summary = await self._generate_single_summary(part_prompt)
                    summaries.append(part_summary)
                combined = "\n\n".join(summaries)
                logger.info(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã {len(summaries)} —á–∞—Å—Ç–µ–π —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
                return combined
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ LangChain: {e}")
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

    async def _generate_single_summary(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞."""
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è LangChain
            messages = [
                SystemMessage(content="You are a helpful AI assistant. Follow the user's instructions carefully and provide accurate responses."),
                HumanMessage(content=prompt),
            ]
            
            # –í—ã–∑—ã–≤–∞–µ–º LLM (LangChain –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç async)
            response = await self.llm.ainvoke(messages)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            if hasattr(response, "content"):
                return response.content
            elif isinstance(response, str):
                return response
            else:
                return str(response)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

    def _estimate_tokens(self, text: str) -> int:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
        return len(text) // 1.2

    def _split_prompt(self, prompt: str, max_prompt_tokens: int = 31000) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
        estimated_tokens = self._estimate_tokens(prompt)
        
        if estimated_tokens <= max_prompt_tokens:
            return [prompt]
        
        logger.warning(
            f"‚ö†Ô∏è  –ü—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤), —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ (–ª–∏–º–∏—Ç {max_prompt_tokens})"
        )
        
        conversation_markers = [
            "–ò–°–•–û–î–ù–´–ô –†–ê–ó–ì–û–í–û–†:",
            "–†–∞–∑–≥–æ–≤–æ—Ä:",
            "–°–æ–æ–±—â–µ–Ω–∏—è:",
            "conversation_text:",
            "conversation:",
        ]
        
        system_part = ""
        conversation_part = ""
        
        for marker in conversation_markers:
            if marker in prompt:
                parts = prompt.split(marker, 1)
                if len(parts) == 2:
                    system_part = parts[0] + marker
                    conversation_part = parts[1]
                    break
        
        if not conversation_part:
            max_chars = int(max_prompt_tokens * 1.2)
            chunks = []
            for i in range(0, len(prompt), max_chars):
                chunks.append(prompt[i : i + max_chars])
            return chunks
        
        max_conversation_tokens = max_prompt_tokens - self._estimate_tokens(system_part) - 100
        max_conversation_chars = int(max_conversation_tokens * 1.2)
        
        conversation_chunks = []
        for i in range(0, len(conversation_part), max_conversation_chars):
            conversation_chunks.append(conversation_part[i : i + max_conversation_chars])
        
        prompts = []
        for i, chunk in enumerate(conversation_chunks):
            if i == 0:
                chunk_prompt = system_part + chunk
            else:
                chunk_prompt = f"{system_part}\n(–ü–†–û–î–û–õ–ñ–ï–ù–ò–ï –†–ê–ó–ì–û–í–û–†–ê - —á–∞—Å—Ç—å {i+1}/{len(conversation_chunks)})\n{chunk}"
            prompts.append(chunk_prompt)
        
        logger.info(f"üìù –ü—Ä–æ–º–ø—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(prompts)} —á–∞—Å—Ç–µ–π")
        return prompts


def build_langchain_embeddings_from_env() -> Optional[LangChainEmbeddingAdapter]:
    """–°–æ–∑–¥–∞–Ω–∏–µ LangChain Embeddings –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –æ–∫—Ä—É–∂–µ–Ω–∏—è.
    
    Returns:
        LangChainEmbeddingAdapter –∏–ª–∏ None, –µ—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
    """
    if OpenAIEmbeddings is None:
        logger.warning("LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å embeddings –∞–¥–∞–ø—Ç–µ—Ä")
        return None
    
    settings = get_settings()
    
    # Priority 1: embeddings_url
    url = settings.get_embeddings_url()
    model_name = None
    
    if url:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAIEmbeddings –¥–ª—è OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö API
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ URL
        if "/v1" in url or ":1234" in url:
            # LM Studio –∏–ª–∏ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
            model_name = settings.lmstudio_model
            embeddings = OpenAIEmbeddings(
                model=model_name or "text-embedding-ada-002",
                openai_api_base=url,
                openai_api_key="not-needed",  # LM Studio –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∫–ª—é—á
            )
        else:
            # text-embeddings-inference –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç
            # –î–ª—è TEI –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∫–∞—Å—Ç–æ–º–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
            logger.warning(f"TEI —Ñ–æ—Ä–º–∞—Ç ({url}) —Ç—Ä–µ–±—É–µ—Ç –∫–∞—Å—Ç–æ–º–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAIEmbeddings")
            embeddings = OpenAIEmbeddings(
                openai_api_base=url,
                openai_api_key="not-needed",
            )
    else:
        # Priority 2: LM Studio variables
        host = settings.lmstudio_host
        port = str(settings.lmstudio_port)
        model_name = settings.lmstudio_model
        
        base_url = f"http://{host}:{port}/v1"
        embeddings = OpenAIEmbeddings(
            model=model_name,
            openai_api_base=base_url,
            openai_api_key="not-needed",
        )
    
    adapter = LangChainEmbeddingAdapter(embeddings)
    adapter._base_url = url or f"http://{settings.lmstudio_host}:{settings.lmstudio_port}"
    adapter._model_name = model_name or settings.lmstudio_model
    
    # Warm-up –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    test_vector = adapter.embed("warmup sentence for embeddings")
    if test_vector is None:
        logger.warning("LangChain embedding service –Ω–µ –≤–µ—Ä–Ω—É–ª –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏ warm-up")
        return None
    
    logger.info(
        f"LangChain embedding service initialized: URL={adapter._base_url}, "
        f"Model={adapter._model_name}, Dimension={adapter.dimension}"
    )
    return adapter


def build_langchain_llm_from_env() -> Optional[LangChainLLMAdapter]:
    """–°–æ–∑–¥–∞–Ω–∏–µ LangChain LLM –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –æ–∫—Ä—É–∂–µ–Ω–∏—è.
    
    Returns:
        LangChainLLMAdapter –∏–ª–∏ None, –µ—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
    """
    if ChatOpenAI is None:
        logger.warning("LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å LLM –∞–¥–∞–ø—Ç–µ—Ä")
        return None
    
    settings = get_settings()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ LM Studio —á–µ—Ä–µ–∑ LangChain
    if not settings.lmstudio_llm_model:
        logger.debug("LM Studio LLM –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–¥–∞–Ω–∞, LangChain –∞–¥–∞–ø—Ç–µ—Ä –Ω–µ —Å–æ–∑–¥–∞–Ω")
        return None
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º LM Studio —á–µ—Ä–µ–∑ ChatOpenAI
    base_url = f"http://{settings.lmstudio_host}:{settings.lmstudio_port}/v1"
    llm = ChatOpenAI(
        model=settings.lmstudio_llm_model,
        base_url=base_url,
        api_key="not-needed",
        temperature=0.3,
    )
    adapter = LangChainLLMAdapter(
        llm,
        model_name=settings.lmstudio_model,
        llm_model_name=settings.lmstudio_llm_model,
        base_url=base_url,
    )
    
    logger.info(f"LangChain LLM adapter initialized: Model={adapter.llm_model_name}")
    return adapter


def get_llm_client_factory() -> Optional[LangChainLLMAdapter]:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è LangChain LLM –∫–ª–∏–µ–Ω—Ç–∞.
    
    Returns:
        LangChainLLMAdapter –∏–ª–∏ None
    """
    try:
        adapter = build_langchain_llm_from_env()
        if adapter:
            logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LangChain LLM –∞–¥–∞–ø—Ç–µ—Ä")
            return adapter
        else:
            logger.error("LangChain LLM failed to initialize")
            return None
    except ImportError as e:
        logger.error(f"LangChain not available: {e}. Install: pip install langchain langchain-community langchain-openai")
        return None
    except Exception as e:
        logger.error(f"Error initializing LangChain LLM: {e}")
        return None

