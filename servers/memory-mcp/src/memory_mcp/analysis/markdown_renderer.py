#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ Markdown –æ—Ç—á—ë—Ç–æ–≤
–°–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ TelegramDumpManager_Spec.md
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..utils.naming import slugify

logger = logging.getLogger(__name__)


class MarkdownRenderer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ Markdown –æ—Ç—á—ë—Ç–æ–≤"""

    def __init__(self, output_dir: Path = Path("artifacts/reports")):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–Ω–¥–µ—Ä–µ—Ä–∞

        Args:
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤
        """
        self.output_dir = output_dir
        self.output_dir.mkdir(exist_ok=True)

    def render_session_summary(
        self,
        summary: Dict[str, Any],
        chat_links: Optional[Dict[str, Any]] = None,
        force: bool = False,
    ) -> Dict[str, Path]:
        """–°–æ–∑–¥–∞—ë—Ç Markdown –∏ JSON –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–µ—Å—Å–∏–∏ –ø–æ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–π —Å—Ö–µ–º–µ."""

        meta = summary.get("meta", {})
        chat_name = meta.get("chat_name", "Unknown chat")
        chat_id = summary.get("chat_id") or self._safe_name(chat_name)
        session_id = summary.get("session_id", "session")
        profile = meta.get("profile", "group-project")
        quality = summary.get("quality", {})
        quality_status = quality.get("status", "accepted")

        sessions_dir = self.output_dir / chat_id / "sessions"
        sessions_dir.mkdir(parents=True, exist_ok=True)

        json_path = sessions_dir / f"{session_id}.json"
        md_filename = (
            f"{session_id}-needs-review.md"
            if quality_status == "needs_review"
            else f"{session_id}.md"
        )
        md_path = sessions_dir / md_filename

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        if not force and json_path.exists() and md_path.exists():
            logger.info(f"–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–µ—Å—Å–∏–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç: {md_path}, {json_path}")
            return {"markdown": md_path, "json": json_path}

        with open(json_path, "w", encoding="utf-8") as fp:
            json.dump(summary, fp, ensure_ascii=False, indent=2)

        if profile == "broadcast":
            content = self._render_broadcast_markdown(summary)
        else:
            content = self._render_group_markdown(summary, chat_links)

        if quality_status == "needs_review":
            banner = "> ‚ö†Ô∏è **–≠—Ç–æ—Ç –æ—Ç—á—ë—Ç –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ needs_review.** –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤—Ä—É—á–Ω—É—é –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º.\n\n"
            content = banner + content

        with open(md_path, "w", encoding="utf-8") as fp:
            fp.write(content)

        logger.info(f"–°–æ–∑–¥–∞–Ω—ã –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–µ—Å—Å–∏–∏: {md_path}, {json_path}")
        return {"markdown": md_path, "json": json_path}

    def render_chat_index(
        self, chat: str, sessions: List[Dict[str, Any]], force: bool = False
    ) -> Path:
        """–°–æ–∑–¥–∞—ë—Ç JSON –∏–Ω–¥–µ–∫—Å —Å–µ—Å—Å–∏–π –¥–ª—è —á–∞—Ç–∞."""

        chat_id = self._safe_name(chat)
        chat_dir = self.output_dir / chat_id
        chat_dir.mkdir(parents=True, exist_ok=True)
        index_path = chat_dir / "index.json"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        if not force and index_path.exists():
            logger.info(f"–ò–Ω–¥–µ–∫—Å —á–∞—Ç–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {index_path}")
            return index_path

        entries: List[Dict[str, Any]] = []
        for session in sessions:
            session_id = session.get("session_id", "")
            meta = session.get("meta", {})
            quality = session.get("quality", {})
            quality_status = quality.get("status", "accepted")
            kpi = quality.get("kpi", {})
            flags = quality.get("flags", {})
            md_filename = (
                f"{session_id}-needs-review.md"
                if quality_status == "needs_review"
                else f"{session_id}.md"
            )
            entry = {
                "session_id": session_id,
                "time_span": meta.get("time_span", ""),
                "messages_total": meta.get("messages_total", 0),
                "profile": meta.get("profile", ""),
                "addons": meta.get("addons", []),
                "policy_flags": meta.get("policy_flags", []),
                "quality": {
                    "score": quality.get("score", 0),
                    "status": quality_status,
                    "kpi": {
                        "coverage": kpi.get("coverage"),
                        "claims_coverage": kpi.get("claims_coverage"),
                        "topics": kpi.get("topics"),
                        "actions": kpi.get("actions"),
                        "risks": kpi.get("risks"),
                        "threads": kpi.get("threads"),
                    },
                    "flags": flags,
                },
                "counts": {
                    "topics": len(session.get("topics", [])),
                    "claims": len(session.get("claims", [])),
                    "discussion": len(session.get("discussion", [])),
                    "actions": len(session.get("actions", [])),
                    "risks": len(session.get("risks", [])),
                },
                "paths": {
                    "markdown": f"sessions/{md_filename}",
                    "json": f"sessions/{session_id}.json",
                },
            }
            entries.append(entry)

        payload = {
            "chat_id": chat_id,
            "chat_name": chat,
            "updated_at": datetime.now().isoformat(),
            "sessions_total": len(entries),
            "sessions": entries,
        }

        with open(index_path, "w", encoding="utf-8") as fp:
            json.dump(payload, fp, ensure_ascii=False, indent=2)

        logger.info(f"–°–æ–∑–¥–∞–Ω –∏–Ω–¥–µ–∫—Å —á–∞—Ç–∞: {index_path}")
        return index_path

    def _render_broadcast_markdown(self, summary: Dict[str, Any]) -> str:
        meta = summary.get("meta", {})
        quality = summary.get("quality", {})
        topics = summary.get("topics", [])
        claims = summary.get("claims", [])
        discussion = summary.get("discussion", [])
        uncertainties = summary.get("uncertainties", [])
        attachments = summary.get("attachments", [])

        lines = []
        lines.append(
            f"# {meta.get('chat_name', '–ß–∞—Ç')} ‚Äî {summary.get('session_id', '')}"
        )
        lines.append("")
        lines.append(
            f"–ü–µ—Ä–∏–æ–¥: {meta.get('time_span', 'N/A')} ¬∑ –ü—Ä–æ—Ñ–∏–ª—å: {meta.get('profile', 'broadcast')} ¬∑ "
            f"–°–æ–æ–±—â–µ–Ω–∏–π: {meta.get('messages_total', 0)} ¬∑ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {self._format_float(meta.get('confidence'))} ¬∑ "
            f"Quality: {quality.get('score', 0)}"
        )
        lines.append("")

        lines.append("## Topics")
        if topics:
            for topic in topics:
                lines.append(
                    f"- **{topic.get('title', '–¢–µ–º–∞')}** ‚Ä¢ {topic.get('time_span', '')}"
                )
                lines.append(f"  {topic.get('summary', '')}")
        else:
            lines.append("- (–¢–µ–º—ã –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã)")
        lines.append("")

        lines.append("## Claims")
        if claims:
            lines.append(self._format_claims_table(claims))
        else:
            lines.append("- (–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ)")
        lines.append("")

        lines.append("## Timeline")
        if discussion:
            for item in discussion:
                lines.append(self._format_timeline_item(item))
        else:
            lines.append("- (–¶–∏—Ç–∞—Ç—ã –Ω–µ –ø–æ–¥–æ–±—Ä–∞–Ω—ã)")
        lines.append("")

        lines.append("## Uncertainties")
        if uncertainties:
            for entry in uncertainties:
                lines.append(f"- {entry}")
        else:
            lines.append("- (–ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –Ω–µ –≤—ã–¥–µ–ª–µ–Ω—ã)")
        lines.append("")

        lines.append("## Attachments")
        lines.extend(self._render_attachments_section(attachments))
        lines.append("")

        lines.append(f"## Rationale\n{summary.get('rationale', 'no_risks_detected')}")

        return "\n".join(lines)

    def _render_group_markdown(
        self, summary: Dict[str, Any], chat_links: Optional[Dict[str, Any]] = None
    ) -> str:
        meta = summary.get("meta", {})
        quality = summary.get("quality", {})
        topics = summary.get("topics", [])
        discussion = summary.get("discussion", [])
        actions = summary.get("actions", [])
        risks = summary.get("risks", [])
        uncertainties = summary.get("uncertainties", [])
        attachments = summary.get("attachments", [])

        lines = []
        lines.append(
            f"# {meta.get('chat_name', '–ß–∞—Ç')} ‚Äî {summary.get('session_id', '')}"
        )
        lines.append("")
        participant_str = ", ".join(meta.get("participants", []))
        lines.append(
            f"–ü–µ—Ä–∏–æ–¥: {meta.get('time_span', 'N/A')} ¬∑ –ü—Ä–æ—Ñ–∏–ª—å: {meta.get('profile', 'group-project')} ¬∑ "
            f"–°–æ–æ–±—â–µ–Ω–∏–π: {meta.get('messages_total', 0)} ¬∑ –£—á–∞—Å—Ç–Ω–∏–∫–∏: {participant_str or '‚Äî'} ¬∑ "
            f"Quality: {quality.get('score', 0)}"
        )
        lines.append("")

        lines.append("## Topics")
        if topics:
            for topic in topics:
                lines.append(
                    f"- **{topic.get('title', '–¢–µ–º–∞')}** ‚Ä¢ {topic.get('time_span', '')}"
                )
                lines.append(f"  {topic.get('summary', '')}")
        else:
            lines.append("- (–¢–µ–º—ã –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã)")
        lines.append("")

        lines.append("## Discussion")
        if discussion:
            for item in discussion:
                lines.append(self._format_timeline_item(item))
        else:
            lines.append("- (–ö–ª—é—á–µ–≤—ã–µ —Ü–∏—Ç–∞—Ç—ã –Ω–µ –≤—ã–¥–µ–ª–µ–Ω—ã)")
        lines.append("")

        lines.append("## Actions")
        if actions:
            for action in actions:
                lines.append(
                    self._format_action_item(
                        action, meta.get("chat_name", ""), chat_links
                    )
                )
        else:
            lines.append("- (–î–µ–π—Å—Ç–≤–∏—è –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã)")
        lines.append("")

        lines.append("## Risks")
        if risks:
            for risk in risks:
                lines.append(self._format_risk_item(risk))
        else:
            lines.append("- (–†–∏—Å–∫–æ–≤ –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ)")
        lines.append("")

        lines.append("## Uncertainties")
        if uncertainties:
            for entry in uncertainties:
                lines.append(f"- {entry}")
        else:
            lines.append("- (–û—Ç–∫—Ä—ã—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ—Ç)")
        lines.append("")

        lines.append("## Attachments")
        lines.extend(self._render_attachments_section(attachments))
        lines.append("")

        lines.append(
            f"## Rationale\n{summary.get('rationale', 'project_session_with_actions')}"
        )

        return "\n".join(lines)

    def _format_claims_table(self, claims: List[Dict[str, Any]]) -> str:
        header = "| Time | Source | Credibility | Entities | Summary |\n| --- | --- | --- | --- | --- |"
        rows = []
        for claim in claims:
            ts = self._format_time(claim.get("ts"))
            source = claim.get("source", "")
            credibility = claim.get("credibility", "")
            entities = ", ".join(claim.get("entities", []))
            summary = claim.get("summary", "")
            rows.append(f"| {ts} | {source} | {credibility} | {entities} | {summary} |")
        return "\n".join([header] + rows)

    def _format_timeline_item(self, item: Dict[str, Any]) -> str:
        ts = self._format_time(item.get("ts"))
        author = item.get("author", "")
        quote = item.get("quote", "")
        msg_id = item.get("msg_id")
        suffix = f" (msg: {msg_id})" if msg_id else ""
        return f"- [{ts}] {author} ¬∑ ¬´{quote}¬ª{suffix}"

    def _format_action_item(
        self, action: Dict[str, Any], chat: str, chat_links: Optional[Dict[str, Any]]
    ) -> str:
        checkbox = "- [ ]"
        text = action.get("text", "")
        owner = action.get("owner") or ""
        due_raw = action.get("due_raw") or action.get("due") or ""
        priority = action.get("priority", "normal")
        msg_id = action.get("msg_id")

        owner_part = f" ‚Äî owner: {owner}" if owner else ""
        due_part = f" ‚Äî due: {due_raw}" if due_raw else ""
        priority_part = f" ‚Äî pri: {priority}"

        deeplink = (
            self._generate_deeplink(chat, {"msg_id": msg_id}, chat_links)
            if msg_id
            else None
        )
        link_part = f" ‚Üó {deeplink}" if deeplink else ""

        fallback = self._generate_fallback(chat, {"msg_id": msg_id}) if msg_id else ""
        fallback_part = f" ({fallback})" if fallback else ""

        return f"{checkbox} {text}{owner_part}{due_part}{priority_part}{link_part}{fallback_part}"

    def _format_risk_item(self, risk: Dict[str, Any]) -> str:
        text = risk.get("text", "")
        likelihood = risk.get("likelihood") or "‚Äî"
        impact = risk.get("impact") or "‚Äî"
        mitigation = risk.get("mitigation")
        msg_id = risk.get("msg_id")
        detail = f" (L:{likelihood}, I:{impact})"
        if mitigation:
            detail += f" ¬∑ Mitigation: {mitigation}"
        if msg_id:
            detail += f" ¬∑ msg: {msg_id}"
        return f"- {text}{detail}"

    def _render_attachments_section(self, attachments: List[str]) -> List[str]:
        if not attachments:
            return ["- (–ê—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)"]
        lines = []
        for attachment in attachments[:20]:
            if ":" in attachment:
                kind, value = attachment.split(":", 1)
                lines.append(f"- **{kind}**: {value}")
            else:
                lines.append(f"- {attachment}")
        return lines

    def _format_time(self, iso: Optional[str]) -> str:
        if not iso:
            return "‚Äî"
        try:
            dt = datetime.fromisoformat(iso)
            return dt.strftime("%Y-%m-%d %H:%M")
        except Exception:
            return iso

    def _format_float(self, value: Optional[float]) -> str:
        if value is None:
            return "‚Äî"
        try:
            return f"{float(value):.2f}"
        except Exception:
            return str(value)

    def render_chat_summary(
        self,
        chat: str,
        sessions: List[Dict[str, Any]],
        top_sessions: Optional[List[Dict[str, Any]]] = None,
        force: bool = False,
    ) -> Path:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ —á–∞—Ç—É –≤ Markdown."""
        chat_id = self._safe_name(chat)
        file_path = self.output_dir / f"{chat_id}.md"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏
        if not force and file_path.exists():
            logger.info(f"–°–≤–æ–¥–∫–∞ —á–∞—Ç–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
            return file_path

        participants = set()
        start_times = []
        end_times = []
        for session in sessions:
            meta = session.get("meta", {})
            participants.update(meta.get("participants", []))
            if meta.get("start_time_utc"):
                start_times.append(meta["start_time_utc"])
            if meta.get("end_time_utc"):
                end_times.append(meta["end_time_utc"])

        if start_times and end_times:
            time_range = f"{min(start_times)[:10]} ‚Äî {max(end_times)[:10]}"
        else:
            time_range = "N/A"

        lines = []
        lines.append(f"# –°–≤–æ–¥–∫–∞ —á–∞—Ç–∞ {chat}")
        lines.append("")
        lines.append(
            f"–í—Å–µ–≥–æ —Å–µ—Å—Å–∏–π: {len(sessions)} ¬∑ –ü–µ—Ä–∏–æ–¥: {time_range} ¬∑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {datetime.now().strftime('%Y-%m-%d')}"
        )
        lines.append(
            f"–£—á–∞—Å—Ç–Ω–∏–∫–∏: {', '.join(sorted(participants)) if participants else '‚Äî'}"
        )
        lines.append("")

        lines.append("## üìå –ê–∫—Ç—É–∞–ª—å–Ω–æ –∑–∞ 30 –¥–Ω–µ–π")
        if top_sessions:
            for i, session in enumerate(top_sessions[:10], 1):
                meta = session.get("meta", {})
                session_id = session.get("session_id", "N/A")
                span = meta.get("time_span", "")
                score = session.get("quality", {}).get("score", 0)
                context = " ".join(
                    topic.get("summary", "") for topic in session.get("topics", [])[:1]
                )
                lines.append(
                    f"{i}. **[{session_id}](sessions/{session_id}.md)** ({span}) ¬∑ Score: {score} ¬∑ {context}"
                )
        else:
            lines.append("_(–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö)_")
        lines.append("")

        lines.append("## üìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–µ—Å—Å–∏–∏")
        for session in reversed(sessions[-10:]):
            meta = session.get("meta", {})
            session_id = session.get("session_id", "N/A")
            span = meta.get("time_span", "N/A")
            score = session.get("quality", {}).get("score", 0)
            topic = session.get("topics", [{}])[0]
            summary = topic.get("summary", "")
            lines.append(
                f"- **[{session_id}](sessions/{session_id}.md)** ¬∑ {span} ¬∑ Score: {score} ¬∑ {summary}"
            )
        lines.append("")

        with open(file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —Å–≤–æ–¥–∫–∏ —á–∞—Ç–∞: {file_path}")
        return file_path

    def render_snippets(self, session: Dict[str, Any], force: bool = False) -> Path:
        """
        –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∫–ª—é—á–µ–≤—ã—Ö —Å–Ω–∏–ø–ø–µ—Ç–æ–≤ —Å–µ—Å—Å–∏–∏

        Args:
            session: –°–µ—Å—Å–∏—è
            force: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª

        Returns:
            –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–Ω–∏–ø–ø–µ—Ç–∞–º–∏
        """
        session_id = session["session_id"]
        chat = session["chat"]

        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–Ω–∏–ø–ø–µ—Ç–æ–≤
        snippets_dir = self.output_dir / self._safe_name(chat) / "snippets"
        snippets_dir.mkdir(parents=True, exist_ok=True)

        # –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª
        file_path = snippets_dir / f"{session_id}.jsonl"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å–Ω–∏–ø–ø–µ—Ç–æ–≤
        if not force and file_path.exists():
            logger.info(f"–§–∞–π–ª —Å–Ω–∏–ø–ø–µ—Ç–æ–≤ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
            return file_path

        # –û—Ç–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = session.get("messages", [])
        key_messages = self._select_key_messages(messages)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ JSONL
        with open(file_path, "w", encoding="utf-8") as f:
            for msg in key_messages:
                snippet = {
                    "msg_id": msg.get("id", ""),
                    "text": msg.get("text", "")[:220],  # –ú–∞–∫—Å–∏–º—É–º 220 —Å–∏–º–≤–æ–ª–æ–≤
                    "date": msg.get("date_utc", ""),
                    "from": msg.get("from", {}),
                }
                f.write(json.dumps(snippet, ensure_ascii=False) + "\n")

        logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —Å–Ω–∏–ø–ø–µ—Ç–æ–≤: {file_path}")
        return file_path

    def _select_key_messages(
        self, messages: List[Dict[str, Any]], max_count: int = 5
    ) -> List[Dict[str, Any]]:
        """
        –û—Ç–±–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —Å–µ—Å—Å–∏–∏

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            max_count: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–Ω–∏–ø–ø–µ—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ, –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏ —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ
        if len(messages) <= max_count:
            return messages

        key_messages = []

        # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        key_messages.append(messages[0])

        # –°–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ)
        sorted_by_length = sorted(
            messages[1:-1], key=lambda x: len(x.get("text", "")), reverse=True
        )
        key_messages.extend(sorted_by_length[: max_count - 2])

        # –ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        key_messages.append(messages[-1])

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        key_messages.sort(key=lambda x: x.get("date_utc", ""))

        return key_messages[:max_count]

    def _group_sessions_by_month(
        self, sessions: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        grouped: Dict[str, List[Dict[str, Any]]] = {}
        for session in sessions:
            start_time = session.get("meta", {}).get("start_time_utc")
            if not start_time:
                continue
            month_key = start_time[:7]
            grouped.setdefault(month_key, []).append(session)
        return grouped

    def _generate_deeplink(
        self,
        chat: str,
        decision: Dict[str, Any],
        chat_links: Optional[Dict[str, Any]] = None,
    ) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Telegram deeplink

        Args:
            chat: –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞
            decision: –†–µ—à–µ–Ω–∏–µ
            chat_links: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫

        Returns:
            Deeplink –∏–ª–∏ None
        """
        if not chat_links:
            return None

        chats_config = chat_links.get("chats", {})
        chat_config = chats_config.get(chat, {})

        if chat_config.get("type") == "public":
            domain = chat_config.get("domain")
            msg_id = decision.get("msg_id")

            if domain and msg_id:
                return f"tg://resolve?domain={domain}&message_id={msg_id}"

        return None

    def _generate_fallback(self, chat: str, decision: Dict[str, Any]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è fallback —Å—Å—ã–ª–∫–∏

        Args:
            chat: –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞
            decision: –†–µ—à–µ–Ω–∏–µ

        Returns:
            Fallback —Å—Å—ã–ª–∫–∞
        """
        msg_id = decision.get("msg_id", "")
        date = decision.get("date", "")[:10] if decision.get("date") else "unknown"

        return f"/chats/{chat}/{date}.json#msg={msg_id}"

    def _safe_name(self, name: str) -> str:
        """–°–æ–∑–¥–∞—ë—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π slug –¥–ª—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π/—Ñ–∞–π–ª–æ–≤."""
        return slugify(name)

    def render_cumulative_context(
        self, chat: str, sessions: List[Dict[str, Any]], force: bool = False
    ) -> Path:
        """–°–æ–∑–¥–∞—ë—Ç —Ñ–∞–π–ª —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —á–∞—Ç–∞."""
        chat_id = self._safe_name(chat)
        chat_dir = self.output_dir / chat_id
        chat_dir.mkdir(parents=True, exist_ok=True)

        file_path = chat_dir / f"{chat_id}_context.md"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if not force and file_path.exists():
            logger.info(f"–§–∞–π–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
            return file_path

        lines = []
        lines.append(f"# –ù–∞–∫–∞–ø–ª–∏–≤–∞—é—â–∏–π—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Ç–∞ {chat}")
        lines.append("")
        lines.append(f"**–û–±–Ω–æ–≤–ª–µ–Ω–æ:** {datetime.now().strftime('%Y-%m-%d %H:%M')} BKK")
        lines.append(f"**–í—Å–µ–≥–æ —Å–µ—Å—Å–∏–π:** {len(sessions)}")
        lines.append("")

        for index, session in enumerate(sessions, 1):
            meta = session.get("meta", {})
            session_id = session.get("session_id", "unknown")
            span = meta.get("time_span", "")
            topics = session.get("topics", [])
            summary = " ".join(topic.get("summary", "") for topic in topics[:2])
            if not summary:
                continue
            lines.append(f"## {index}. {session_id}")
            if span:
                lines.append(f"**–ü–µ—Ä–∏–æ–¥:** {span}")
            lines.append("")
            lines.append(summary)
            lines.append("---")

        with open(file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {file_path}")
        return file_path


if __name__ == "__main__":
    test_summary = {
        "version": "1.0.0",
        "chat_id": "TestChat",
        "session_id": "TestChat-D0001",
        "meta": {
            "chat_name": "TestChat",
            "profile": "group-project",
            "time_span": "2025-10-01 10:00 ‚Äì 12:00 BKK",
            "messages_total": 24,
            "confidence": 0.92,
            "participants": ["alice", "bob"],
            "dominant_language": "ru",
            "chat_mode": "group",
            "start_time_utc": "2025-10-01T03:00:00+00:00",
            "end_time_utc": "2025-10-01T05:00:00+00:00",
        },
        "topics": [
            {
                "title": "–ü–ª–∞–Ω —Ä–∞–±–æ—Ç –Ω–∞ –Ω–µ–¥–µ–ª—é",
                "time_span": "2025-10-01 10:00 ‚Äì 11:00 BKK",
                "message_ids": ["1", "2"],
                "summary": "–û–±—Å—É–¥–∏–ª–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞ –∑–∞–¥–∞—á–∏.",
            }
        ],
        "claims": [
            {
                "ts": "2025-10-01T10:05:00+07:00",
                "source": "internal",
                "modality": "internal",
                "credibility": "medium",
                "entities": ["alice"],
                "summary": "–ö–æ–º–∞–Ω–¥–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–ª–∞ —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –Ω–∞ –Ω–µ–¥–µ–ª—é.",
                "msg_id": "1",
                "topic_title": "–ü–ª–∞–Ω —Ä–∞–±–æ—Ç –Ω–∞ –Ω–µ–¥–µ–ª—é",
            }
        ],
        "discussion": [
            {
                "ts": "2025-10-01T10:15:00+07:00",
                "author": "alice",
                "msg_id": "1",
                "quote": "–ê–∫—Ü–µ–Ω—Ç –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É.",
            }
        ],
        "actions": [
            {
                "text": "–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –æ—Ç—á—ë—Ç –∫ –ø—è—Ç–Ω–∏—Ü–µ",
                "owner": "@alice",
                "due_raw": "2025-10-03",
                "due": "2025-10-03",
                "priority": "high",
                "status": "open",
                "msg_id": "1",
                "topic_title": "–ü–ª–∞–Ω —Ä–∞–±–æ—Ç –Ω–∞ –Ω–µ–¥–µ–ª—é",
            }
        ],
        "risks": [
            {
                "text": "–í–æ–∑–º–æ–∂–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥–∞–Ω–Ω—ã—Ö",
                "likelihood": "medium",
                "impact": "medium",
                "mitigation": None,
                "msg_id": "2",
                "topic_title": "–ü–ª–∞–Ω —Ä–∞–±–æ—Ç –Ω–∞ –Ω–µ–¥–µ–ª—é",
            }
        ],
        "uncertainties": [],
        "entities": ["alice", "bob"],
        "attachments": ["link:https://example.com/doc"],
        "rationale": "project_session_with_actions",
        "quality": {
            "score": 90,
            "status": "accepted",
            "kpi": {
                "coverage": 0.8,
                "claims_coverage": 0.6,
                "topics": 1,
                "actions": 1,
                "risks": 1,
                "threads": 0,
            },
            "details": {},
        },
        "raw_summary": "Draft text",
        "fallback_used": False,
        "_legacy": {},
    }

    renderer = MarkdownRenderer()
    paths = renderer.render_session_summary(test_summary, force=True)
    print(f"Markdown: {paths['markdown']}")
    print(f"JSON: {paths['json']}")
