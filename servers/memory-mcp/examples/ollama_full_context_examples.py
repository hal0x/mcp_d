#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä—ã –æ—Ç–ø—Ä–∞–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Ollama —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 32,768 —Ç–æ–∫–µ–Ω–æ–≤
"""

import asyncio
from typing import Any, Dict, Optional

import aiohttp
import requests


class OllamaFullContextClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.model_name = "gemma3n:e4b-it-q8_0"
        self.max_context = 32768  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏

    async def generate_with_full_context_async(
        self,
        prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 8000,
        top_p: float = 0.93,
        presence_penalty: float = 0.05,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

        Args:
            prompt: –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-1.0)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: Top-p –ø–∞—Ä–∞–º–µ—Ç—Ä
            presence_penalty: Presence penalty
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞
        estimated_tokens = len(full_prompt) // 4
        if estimated_tokens > self.max_context - max_tokens:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π: {estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
            print(f"   –ú–∞–∫—Å–∏–º—É–º –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: {self.max_context - max_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
            # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–º–ø—Ç
            max_prompt_chars = (self.max_context - max_tokens) * 4
            full_prompt = full_prompt[:max_prompt_chars] + "..."

        payload = {
            "model": self.model_name,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": top_p,
                "presence_penalty": presence_penalty,
                "num_ctx": self.max_context,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                "num_thread": 8,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
                "repeat_penalty": 1.1,  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                "stop": ["</s>", "Human:", "Assistant:"],  # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
            },
        }

        try:
            timeout = aiohttp.ClientTimeout(total=600)  # 10 –º–∏–Ω—É—Ç
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{self.base_url}/api/generate", json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("response", "").strip()
                    else:
                        error_text = await response.text()
                        raise Exception(f"HTTP {response.status}: {error_text}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"–û—à–∏–±–∫–∞: {str(e)}"

    def generate_with_full_context_sync(
        self,
        prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 8000,
        top_p: float = 0.93,
        presence_penalty: float = 0.05,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞
        estimated_tokens = len(full_prompt) // 4
        if estimated_tokens > self.max_context - max_tokens:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π: {estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
            max_prompt_chars = (self.max_context - max_tokens) * 4
            full_prompt = full_prompt[:max_prompt_chars] + "..."

        payload = {
            "model": self.model_name,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": top_p,
                "presence_penalty": presence_penalty,
                "num_ctx": self.max_context,
                "num_thread": 8,
                "repeat_penalty": 1.1,
                "stop": ["</s>", "Human:", "Assistant:"],
            },
        }

        try:
            response = requests.post(
                f"{self.base_url}/api/generate", json=payload, timeout=600
            )

            if response.status_code == 200:
                data = response.json()
                return data.get("response", "").strip()
            else:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"–û—à–∏–±–∫–∞: {str(e)}"

    async def chat_with_context_async(
        self,
        messages: list,
        temperature: float = 0.3,
        max_tokens: int = 8000,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        –ß–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π)

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π [{"role": "user", "content": "..."}, ...]
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç

        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        prompt_parts = []

        if system_prompt:
            prompt_parts.append(f"System: {system_prompt}")

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            prompt_parts.append(f"{role.title()}: {content}")

        full_prompt = "\n\n".join(prompt_parts)

        return await self.generate_with_full_context_async(
            prompt=full_prompt, temperature=temperature, max_tokens=max_tokens
        )

    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                for model in models:
                    if model["name"] == self.model_name:
                        return model
            return {}
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏: {e}")
            return {}


# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def example_usage():
    """–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞"""

    client = OllamaFullContextClient()

    print("ü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Ollama —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 32,768 —Ç–æ–∫–µ–Ω–æ–≤\n")

    # 1. –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    print("1Ô∏è‚É£ –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è:")
    simple_prompt = (
        "–†–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–æ –æ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –∏ –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≤ –∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏–∏."
    )
    result = await client.generate_with_full_context_async(
        prompt=simple_prompt, max_tokens=2000, temperature=0.7
    )
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:200]}...\n")

    # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
    print("2Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º:")
    system_prompt = (
        "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–ª–æ–∫—á–µ–π–Ω—É –∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏."
    )
    user_prompt = "–û–±—ä—è—Å–Ω–∏ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É Proof of Work –∏ Proof of Stake –∫–æ–Ω—Å–µ–Ω—Å—É—Å–æ–º."
    result = await client.generate_with_full_context_async(
        prompt=user_prompt,
        system_prompt=system_prompt,
        max_tokens=3000,
        temperature=0.5,
    )
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:200]}...\n")

    # 3. –ß–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    print("3Ô∏è‚É£ –ß–∞—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º:")
    messages = [
        {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π."},
        {
            "role": "assistant",
            "content": "–ü—Ä–∏–≤–µ—Ç, –ê–ª–µ–∫—Å–µ–π! –†–∞–¥ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è. –ö–∞–∫ –¥–µ–ª–∞?",
        },
        {"role": "user", "content": "–û—Ç–ª–∏—á–Ω–æ! –†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ –æ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏."},
    ]
    result = await client.chat_with_context_async(
        messages=messages, max_tokens=1500, temperature=0.6
    )
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:200]}...\n")

    # 4. –î–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç (—Ç–µ—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
    print("4Ô∏è‚É£ –¢–µ—Å—Ç –¥–ª–∏–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞:")
    long_text = (
        "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç: "
        + "–≠—Ç–æ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. " * 1000
    )
    result = await client.generate_with_full_context_async(
        prompt=long_text, max_tokens=1000, temperature=0.3
    )
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:200]}...\n")

    # 5. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    print("5Ô∏è‚É£ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
    model_info = client.get_model_info()
    if model_info:
        print(f"–ú–æ–¥–µ–ª—å: {model_info.get('name', 'Unknown')}")
        print(f"–†–∞–∑–º–µ—Ä: {model_info.get('size', 'Unknown')} –±–∞–π—Ç")
        print(f"–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω: {model_info.get('modified_at', 'Unknown')}")
    else:
        print("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")


def curl_examples():
    """–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ curl"""

    print("üåê –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ curl:\n")

    # 1. –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
    print("1Ô∏è‚É£ –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å:")
    curl_simple = """
curl -X POST http://localhost:11434/api/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "gemma3n:e4b-it-q8_0",
    "prompt": "–†–∞—Å—Å–∫–∞–∂–∏ –æ –±–ª–æ–∫—á–µ–π–Ω–µ",
    "stream": false,
    "options": {
      "num_ctx": 32768,
      "num_predict": 1000,
      "temperature": 0.7
    }
  }'
"""
    print(curl_simple)

    # 2. –ó–∞–ø—Ä–æ—Å —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
    print("2Ô∏è‚É£ –ó–∞–ø—Ä–æ—Å —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º:")
    curl_system = """
curl -X POST http://localhost:11434/api/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "gemma3n:e4b-it-q8_0",
    "prompt": "System: –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞–º.\\n\\nUser: –û–±—ä—è—Å–Ω–∏ DeFi",
    "stream": false,
    "options": {
      "num_ctx": 32768,
      "num_predict": 2000,
      "temperature": 0.5,
      "top_p": 0.9,
      "repeat_penalty": 1.1
    }
  }'
"""
    print(curl_system)

    # 3. –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å
    print("3Ô∏è‚É£ –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å:")
    curl_stream = """
curl -X POST http://localhost:11434/api/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "gemma3n:e4b-it-q8_0",
    "prompt": "–ù–∞–ø–∏—à–∏ —ç—Å—Å–µ –æ –±—É–¥—É—â–µ–º –ò–ò",
    "stream": true,
    "options": {
      "num_ctx": 32768,
      "num_predict": 3000,
      "temperature": 0.8
    }
  }'
"""
    print(curl_stream)


if __name__ == "__main__":
    print("üöÄ Ollama Full Context Client")
    print("=" * 50)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã curl
    curl_examples()

    print("\n" + "=" * 50)
    print("üêç Python –ø—Ä–∏–º–µ—Ä—ã:")

    # –ó–∞–ø—É—Å–∫–∞–µ–º Python –ø—Ä–∏–º–µ—Ä—ã
    asyncio.run(example_usage())
